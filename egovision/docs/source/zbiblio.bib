@inproceedings{Abbeel2005,
abstract = {Kalman filters are a workhorse of robotics and are routinely used in state-estimation problems. However, their performance critically depends on a large number of modeling parameters which can be very difficult to obtain, and are often set via significant manual tweaking and at a great cost of engineering time. In this paper, we propose a method for automatically learning the noise parameters of a Kalman filter. We also demonstrate on a commercial wheeled rover that our Kalman filter's learned noise covariance parameters—obtained quickly and fully automatically—significantly outperform an earlier, carefully and laboriously hand-designed one.},
address = {Cambridge, MA, USA},
author = {Abbeel, Pieter and Coates, Adam and Montemerlo, Michael and Ng, Andrew Y and Thrun, Sebastian},
booktitle = {Proceedings of Robotics: Science and Systems I},
doi = {10.1.1.76.759},
file = {:home/abetan16/Dropbox/MendeleyV3/Abbeel et al. - 2005 - Discriminative Training of Kalman Filters.pdf:pdf},
pages = {289--296},
title = {{Discriminative Training of Kalman Filters}},
url = {http://www-cs.stanford.edu/people/ang/papers/rss05-discriminativeKF.pdf},
year = {2005}
}
@article{Abdullah2009,
abstract = {Recent research in image recognition has shown that combining multiple descriptors is a very useful way to improve classification performance. Furthermore, the use of spatial pyramids that compute descriptors at multiple spatial resolution levels generally increases the discriminative power of the descriptors. In this paper we focus on combination methods that combine multiple descriptors at multiple spatial resolution levels. A possible problem of the naive solution to create one large input vector for a machine learning classifier such as a support vector machine, is that the input vector becomes of very large dimensionality, which can increase problems of overfitting and hinder generalization performance. Therefore we propose the use of stacking support vector machines where at the first layer each support vector machine receives the input constructed by each single descriptor and is trained to compute the right output class. A second layer support vector machine is then used to combine the class probabilities of all trained first layer support vector models to learn the right output class given these reduced input vectors. We have performed experiments on 20 classes from the Caltech object database with 10 different single descriptors at 3 different resolutions. The results show that our 2-layer stacking approach outperforms the naive approach that combines all descriptors directly in a very large single input vector.},
author = {Abdullah, Azizi and Veltkamp, Remco C. and Wiering, Marco a.},
doi = {10.1109/IJCNN.2009.5178743},
file = {:home/abetan16/Dropbox/MendeleyV3/Abdullah, Veltkamp, Wiering - 2009 - Spatial Pyramids and Two-layer Stacking Svm Classifiers for Image Categorization a Comparative Stud.pdf:pdf},
isbn = {9781424435531},
issn = {1098-7576},
journal = {Proceedings of the International Joint Conference on Neural Networks},
month = {jun},
number = {2},
pages = {5--12},
publisher = {Ieee},
title = {{Spatial pyramids and two-layer stacking SVM classifiers for image categorization: A comparative study}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5178743},
year = {2009}
}
@article{Achanta2012,
abstract = {Computer vision applications have come to rely increasingly on superpixels in recent years, but it is not always clear what constitutes a good superpixel algorithm. In an effort to understand the benefits and drawbacks of existing methods, we empirically compare five state-of-the-art superpixel algorithms for their ability to adhere to image boundaries, speed, memory efficiency, and their impact on segmentation performance. We then introduce a new superpixel algorithm, simple linear iterative clustering (SLIC), which adapts a k-means clustering approach to efficiently generate superpixels. Despite its simplicity, SLIC adheres to boundaries as well as or better than previous methods. At the same time, it is faster and more memory efficient, improves segmentation performance, and is straightforward to extend to supervoxel generation.},
author = {Achanta, R. and Shaji, A. and Smith, K. and Lucchi, A. and Fua, P. and Süsstrunk, Sabine},
doi = {10.1109/TPAMI.2012.120},
file = {:home/abetan16/Dropbox/MendeleyV3/Achanta et al. - 2012 - SLIC Superpixels Compared to State-of-the-art Superpixel Methods.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Algorithms,Automated,Automated: methods,Computer-Assisted,Computer-Assisted: methods,Image Enhancement,Image Enhancement: methods,Image Interpretation,Pattern Recognition,Reproducibility of Results,Sensitivity and Specificity,Signal Processing},
month = {nov},
number = {11},
pages = {2274--2282},
pmid = {22641706},
title = {{SLIC Superpixels Compared to State-of-the-Art Superpixel Methods}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6205760},
volume = {34},
year = {2012}
}
@article{Achanta2012a,
author = {Achanta, Radhakrishna and Shaji, Appu and Smith, Kevin},
doi = {10.1109/tpami.2012.120},
file = {:home/abetan16/Dropbox/MendeleyV3/Achanta, Shaji, Smith - 2012 - SLIC superpixels compared to state-of-the-art superpixel methods.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {Pattern Analysis and {\ldots}},
number = {11},
pages = {2274--2281},
pmid = {22641706},
title = {{SLIC Superpixels Compared to State-of-the-Art Superpixel Methods}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6205760},
volume = {34},
year = {2012}
}
@misc{Adams2007,
abstract = {Changepoints are abrupt variations in the generative parameters of a data sequence. Online detection of changepoints is useful in modelling and prediction of time series in application areas such as finance, biometrics, and robotics. While frequentist methods have yielded online filtering and prediction techniques, most Bayesian papers have focused on the retrospective segmentation problem. Here we examine the case where the model parameters before and after the changepoint are independent and we derive an online algorithm for exact inference of the most recent changepoint. We compute the probability distribution of the length of the current ``run,'' or time since the last changepoint, using a simple message-passing algorithm. Our implementation is highly modular so that the algorithm may be applied to a variety of types of data. We illustrate this modularity by demonstrating the algorithm on three different real-world data sets.},
archivePrefix = {arXiv},
arxivId = {0710.3742},
author = {Adams, Ryan Prescott and MacKay, David J. C.},
booktitle = {arXiv preprint arXiv:0710.3742},
doi = {arXiv:0710.3742v1},
eprint = {0710.3742},
file = {:home/abetan16/Dropbox/MendeleyV3/Adams, MacKay - 2007 - Bayesian Online Changepoint Detection.pdf:pdf},
pages = {7},
title = {{Bayesian Online Changepoint Detection}},
url = {http://arxiv.org/abs/0710.3742},
year = {2007}
}
@inproceedings{Afolabi2007,
abstract = {A challenging and daunting task for financial investors is determining stock market timing - when to buy, sell and the future price of a stock. This challenge is due to the complexity of the stock market. New methods have emerged that increase the accuracy of stock prediction. Examples of these methods are fuzzy logic, neural network and hybridized methods such as hybrid Kohonen self organizing map (SOM), adaptive neuro-fuzzy inference system (ANFIS) etc. This paper presents a number of methods used to predict the stock price of the day. These methods are backpropagation, Kohonen SOM, and a hybrid Kohonen SOM. The results show that the difference in error of the hybrid Kohonen SOM is significantly reduced compared to the other methods used. Hence, the results suggest that the hybrid Kohonen SOM is a better predictor compared to Kohonen SOM and backpropagation},
author = {Afolabi, Mark O. and Olude, Olatoyosi},
booktitle = {Proceedings of the Annual Hawaii International Conference on System Sciences},
doi = {10.1109/HICSS.2007.441},
file = {:home/abetan16/Dropbox/MendeleyV3/Afolabi, Olude - 2007 - Predicting stock prices using a hybrid Kohonen Self Organizing Map (SOM).pdf:pdf},
isbn = {0769527558},
issn = {15301605},
keywords = {Backpropagation,Hybrid kohonen SOM and stock price prediction,Kohonen SOM},
pages = {1--8},
title = {{Predicting stock prices using a hybrid Kohonen Self Organizing Map (SOM)}},
year = {2007}
}
@inproceedings{Aghazadeh2011,
abstract = {This paper demonstrates a system for the automatic extraction of novelty in images captured from a small video camera attached to a subject's chest, replicating his visual perspective, while performing activities which are repeated daily. Novelty is detected when a (sub)sequence cannot be registered to previously stored sequences captured while performing the same daily activity. Sequence registration is performed by measuring appearance and geometric similarity of individual frames and exploiting the invariant temporal order of the activity. Experimental results demonstrate that this is a robust way to detect novelties induced by variations in the wearer's ego-motion such as stopping and talking to a person. This is an essentially new and generic way of automatically extracting information of interest to the camera wearer and can be used as input to a system for life logging or memory support.},
address = {Pittsburgh, PA},
author = {Aghazadeh, Omid and Sullivan, Josephine and Carlsson, Stefan},
booktitle = {Cvpr},
doi = {10.1109/CVPR.2011.5995731},
file = {:home/abetan16/Dropbox/MendeleyV3/Aghazadeh, Sullivan, Carlsson - 2011 - Novelty Detection from an Ego-centric Perspective.pdf:pdf},
isbn = {9781457703942},
issn = {10636919},
month = {jun},
pages = {3297--3304},
publisher = {Ieee},
title = {{Novelty detection from an ego-centric perspective}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5995731},
year = {2011}
}
@inproceedings{Aizawa2001,
abstract = {"We want to record our entire life by video" is the motivation of this research. Developing wearable devices and huge storage devices will make it possible to keep entire life by video. We could capture 70 years of our life, however, the problem is how to handle such a huge amount of data. Automatic summarization based on personal interest should be required. In this paper we propose an approach to the automatic structuring and summarization of wearable video. (Wearable video is our abbreviation of "video captured by a wearable camera".) In our approach, we make use of a wearable camera and a sensor of brain waves. The video is firstly structured by objective features of video, and the shots are rated by subjective measures based on brain waves. The approach is very successful for real world experiments and it automatically extracted all the events that the subjects reported they had felt interesting},
author = {Aizawa, K. and Ishijima, K. and Shiina, M.},
booktitle = {International Conference on Image Processing (ICIP)},
doi = {10.1109/ICIP.2001.958135},
file = {:home/abetan16/Dropbox/MendeleyV3/Aizawa, Ishijima, Shiina - 2001 - Summarizing Wearable Video.pdf:pdf},
isbn = {0-7803-6725-1},
pages = {398--401},
publisher = {IEEE Signal Processing},
title = {{Summarizing wearable video}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=958135},
volume = {3},
year = {2001}
}
@inproceedings{Aizawa2005,
abstract = {In wearable computing environments, digitization of personal experiences will be made possible by continuous recordings using a wearable video camera. This could lead to the ``automatic life-log application''. However, it is evident that the resulting amount of video content will be enormous. Accordingly, to retrieve and browse desired scenes, a vast quantity of video data must be organized using structural information. We are developing a ``context-based video retrieval system for life-log applications''. Our life log system captures video, audio, acceleration sensor, gyro, GPS, annotations, documents, web pages, and emails, and provides functions that make efficient video browsing and retrieval possible by using data from these sensors, some databases and various document data.},
author = {Aizawa, Kiyoharu},
booktitle = {International Conference on Multimedia Modeling},
doi = {10.1109/MMMC.2005.34},
file = {:home/abetan16/Dropbox/MendeleyV3/Aizawa - 2005 - Digitizing Personal Experiences Capture and Retrieval of Life Log.pdf:pdf},
isbn = {0-7695-2164-9},
issn = {1550-5502},
pages = {10--15},
publisher = {Ieee},
title = {{Digitizing Personal Experiences: Capture and Retrieval of Life Log}},
url = {http://portal.acm.org/citation.cfm?id=1043519$\backslash$nhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1385968},
year = {2005}
}
@article{Alata2009,
abstract = {The aim of this work is the evaluation of Multivariate Gaussian Mixture Model (MGMM) in different color spaces (RGB, L*a*b* and YC1C2 issued from IHLS). We have analysed the approach for the description of the color information in an image on the one hand and for the representation of a color image on the other hand. To achieve this goal we need an accurate estimation method of MGMM parameter set firstly and distances for evaluating MGMMs secondly. A SEMmean-EM procedure and two distances between color images and synthesized images are then proposed. First distance is based on normalized histograms and second distance is based on a psychovisual distance between two colors. To compute these distances, two methods for synthezising images are proposed from a direct procedure of site classification: for each site, sample a color from its associated multivariate Gaussian law or attribute the mean color. The conclusion of our study is that estimation accuracy is better using L*a*b* color space than using RGB or YC1C2 and discrimination performances based on the two distances are also better in L*a*b* color space. A comparison between different criteria for choosing the number of components in the mixture is also done. Integrated Completed Likelihood criterion may be used if one only needs to characterize color information and Bayesian Information Criterion or ??{\{}symbol{\}}??min criterion may be used if one needs to characterize color information along with color spatial localization. ?? 2009 Elsevier Inc. All rights reserved.},
author = {Alata, Olivier and Quintard, Ludovic},
doi = {10.1016/j.cviu.2009.03.001},
file = {:home/abetan16/Dropbox/MendeleyV3/Alata, Quintard - 2009 - Is there a best color space for color image characterization or representation based on Multivariate Gaussian M.pdf:pdf},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {Color spaces,EM algorithm,Histogram,Information Criteria,Model selection,Multivariate Gaussian Mixture Model,Psychovisual distance,SEM algorithm},
number = {8},
pages = {867--877},
publisher = {Elsevier Inc.},
title = {{Is there a best color space for color image characterization or representation based on Multivariate Gaussian Mixture Model?}},
url = {http://dx.doi.org/10.1016/j.cviu.2009.03.001},
volume = {113},
year = {2009}
}
@inproceedings{Alexe2010,
abstract = {We present a generic objectness measure,quantifying how likely it is for an image window to contain an object of any class. We explicitly train it to distinguish objects with a well-defined boundary in space, such as cows and telephones, from amorphous background elements, such as grass and road. The measure combines in a Bayesian framework several image cues measuring characteristics of objects, such as appearing different from their surroundings and having a closed boundary. This includes an innovative cue measuring the closed boundary characteristic. In experiments on the challenging PASCAL VOC 07 dataset, we show this new cue to outperform a state-of-the-art saliency measure [17], and the combined measure to perform better than any cue alone. Finally, we show how to sample windows from an image according to their objectness distribution and give an algorithm to employ them as location priors for modern class-specific object detectors. In experiments on PASCAL VOC 07 we show this greatly reduces the number of windows evaluated by class-specific object detectors.},
address = {San Francisco, CA, USA},
author = {Alexe, Bogdan and Deselaers, Thomas and Ferrari, Vittorio},
booktitle = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2010.5540226},
file = {:home/abetan16/Dropbox/MendeleyV3/Alexe, Deselaers, Ferrari - 2010 - What Is an Object.pdf:pdf},
isbn = {978-1-4244-6984-0},
month = {jun},
pages = {73--80},
publisher = {Ieee},
title = {{What is an object?}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5540226},
year = {2010}
}
@inproceedings{Alletto,
abstract = {In this paper we present a new method for head pose real-time estimation in ego-vision scenarios that is a key step in the understanding of social interactions. In order to robustly detect head under changing aspect ratio, scale and orientation we use and extend the Hough-Based Tracker which allows to follow simultaneously each subject in the scene. In an ego-vision scenario where a group interacts in a discussion, each subject's head orientation will be more likely to remain focused for a while on the person who has the floor. In order to encode this behavior we include a stateful Hidden Markov Model technique that enforces the predicted pose with the temporal coherence from a video sequence. We extensively test our approach on several indoor and outdoor ego-vision videos with high illumination variations showing its validity and outperforming other recent related state of the art approaches.},
author = {Alletto, Stefano and Serra, Giuseppe and Calderara, Simone and Cucchiara, Rita},
booktitle = {2014 22nd International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2014.718},
file = {:home/abetan16/Dropbox/MendeleyV3/Alletto et al. - 2014 - Head Pose Estimation in First-Person Camera Views.pdf:pdf},
isbn = {978-1-4799-5209-0},
pages = {4188--4193},
publisher = {IEEE Computer Society},
title = {{Head Pose Estimation in First-Person Camera Views}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6977430},
year = {2014}
}
@inproceedings{Alletto2014,
abstract = {In this paper we present a novel approach to detect groups in ego-vision scenarios. People in the scene are tracked through the video sequence and their head pose and 3D location are estimated. Based on the concept of f-formation, we define with the orientation and distance an inherently social pairwise feature that describes the affinity of a pair of people in the scene. We apply a correlation clus- tering algorithm that merges pairs of people into socially related groups. Due to the very shifting nature of social in- teractions and the different meanings that orientations and distances can assume in different contexts, we learn the weight vector of the correlation clustering using Structural SVMs. We extensively test our approach on two publicly available datasets showing encouraging results when de- tecting},
annote = {- Interaction Detection
Structured SVM
Correlation Clustering
- People detection
Hough-Based Tracker
Viola Jones
- Head Pose Estimation
HoG
SVM
HMM
- Location
3D perspective},
author = {Alletto, Stefano and Serra, Giuseppe and Calderara, Simone and Solera, Francesco and Cucchiara, Rita},
booktitle = {CVPR 2014 Workshop},
doi = {10.1109/CVPRW.2014.91},
file = {:home/abetan16/Dropbox/MendeleyV3/Alletto et al. - 2014 - From Ego to Nos-Vision Detecting Social Relationships in First-Person Views.pdf:pdf},
isbn = {9781479943081},
issn = {21607516},
month = {jun},
pages = {580--585},
publisher = {Ieee},
title = {{From Ego to Nos-vision: Detecting Social Relationships in First-Person Views}},
url = {http://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}workshops{\_}2014/W16/html/Alletto{\_}From{\_}Ego{\_}to{\_}2014{\_}CVPR{\_}paper.html},
year = {2014}
}
@inproceedings{Anbu2002,
abstract = {We present a fast algorithm for achieving motion-based object detection in an image sequence. While in most existing object-detection algorithms, segmentation of the image is done as the first step followed by grouping of segments, the proposed algorithm first uses motion information to identify what we call a region of interest. Segmentation (which is computationally very expensive) is done only within a square of interest (whose area is smaller than that of the entire image), which ensures a speed up. The segments are then combined to obtain the final segment, which closely matches the shape of the object to be detected. Since the square of interest is always smaller than the image, the proposed algorithm is 2 to 4 times faster than every existing algorithm for object detection. In terms of the accuracy with which a desired object is detected, the performance of our algorithm is comparable to existing algorithms.},
author = {Anbu, Aiwin Alwin and Agarwal, G and Srivastava, Gaurav},
booktitle = {Digital Signal Processing, {\ldots}},
doi = {10.1109/ICDSP.2002.1028285},
file = {:home/abetan16/Dropbox/MendeleyV3/Anbu, Aganval, Srivastava - 2002 - A Fast Object Detection Algorithm Using Motion-based Region of Interest Determination.pdf:pdf},
isbn = {0780375033},
keywords = {confidence,object detection,region of interest,watershed segmentation},
number = {June},
pages = {16--19},
title = {{A fast object detection algorithm using motion-based region-of-interest determination}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1028285$\backslash$nA fast object detection algorithm using motion-based region-of-interest determination},
year = {2002}
}
@inproceedings{Andrews2002,
author = {Andrews, S and Tsochantaridis, I and Hofmann, T},
booktitle = {Advances in Neural Information Processing Systems 15},
file = {:home/abetan16/Dropbox/MendeleyV3/Andrews, Tsochantaridis, Hofmann - 2003 - Support Vector Machines for Multiple-Instance Learning.pdf:pdf},
isbn = {1049-5258},
pages = {561--568},
title = {{Support Vector Machines for Multiple-Instance Learning}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AA10.pdf},
year = {2003}
}
@inproceedings{Aner2006,
abstract = {We present an approach for compact video summaries that allows fast and direct access to video data. The video is segmented into shots and, in appropriate video genres, into scenes, using previously proposed methods. A new concept that supports the hierarchical representation of video is presented, and is based on physical setting and camera locations. We use mosaics to represent and cluster shots, and detect appropriate mosaics to represent scenes. In contrast to approaches to video indexing which are based on key-frames, our efficient mosaic-based scene representation allows fast clustering of scenes into physical settings, as well as further comparison of physical settings across videos. This enables us to detect plots of different episodes in situation comedies and serves as a basis for indexing whole video sequences. In sports videos where settings are not as well defined, our approach allows classifying shots for characteristic event detection. We use a novel method for mosaic comparison and create a highly compact non-temporal representation of video. This representation allows accurate comparison of scenes across different videos and serves as a basis for indexing video libraries.},
address = {Copenhagen, Denmark},
author = {Aner, A and Kender, J},
booktitle = {European Conference on Computer Vision},
file = {:home/abetan16/Dropbox/Articles/Mendeley/Aner, Kender - 2006 - Video Summaries through Mosaic-Based Shot and Scene Clustering.pdf:pdf},
isbn = {3-540-43748-7},
pages = {388--402},
publisher = {Springer Berlin Heidelberg},
title = {{Video Summaries through Mosaic-Based Shot and Scene Clustering}},
url = {http://link.springer.com/chapter/10.1007/3-540-47979-1{\_}26},
year = {2006}
}
@inproceedings{Nguyem2012,
abstract = {The development of Affective Computing has witnessed tremendous number of studies about facial and vocal expression, while bodily expression only comprises the minority. However, with the emerging of social signal processing, people have paid more attention to the significant role of body language in social communication and emotional expression. One situation when body language shows its importance is public speaking. This paper presents an online feedback system for public speakers, in which emotion recognised from body language of speakers is regarded as the primary component for analysis. The system captures input through a Microsoft Kinect, thus offer users with a convenience way to interact. In order to recognise bodily expression, a posture and gesture representation method based on Laban Movement Analysis was adopted. The system could be used as a support tool for people who learn presentation skills, as well as for public speakers who need immediate feedbacks. This paper will present the architecture of the system, along with details about the employed methods and a collected database of presentations.},
address = {Kuala Lumpur},
author = {Anh-Tuan, Nguyen and Chen, Wei and Rauterberg, Matthias and Ieee},
booktitle = {2012 Ieee Symposium on E-Learning, E-Management and E-Services (Is3e 2012)},
file = {:home/abetan16/Dropbox/MendeleyV3/Anh-Tuan et al. - 2012 - Online Feedback System for Public Speakers.pdf:pdf},
keywords = {-emotion detection,technology enhanced learning},
pages = {45--49},
publisher = {IEEE Comput. Soc},
title = {{Online Feedback System for Public Speakers}},
url = {{\textless}Go to ISI{\textgreater}://WOS:000335758800009},
year = {2012}
}
@article{Anselin2003,
abstract = {In spatial econometrics, the typical alternative of spatial autocorrelation is expressed in the form of a spatial autorregressive process. While the bulk of the literature is devoted to specification tests and estimation methods for this model, alternatives have been suggested as well. In this paper, we consider an alternative that takes the form of the spatial error components formulation proposed by [Kelejian and Robinson (1995)]. We consider a number of specification tests against this alternative, based on both a maximum likelihood framework as well as on a general method of moments estimation approach. We compare the performance of these tests in a series of Monte Carlo simulation experiments for a range of different spatial layouts and under a number of different error distributions. {\textcopyright} 2003 Elsevier B.V. All rights reserved.},
author = {Anselin, Luc and Moreno, Rosina},
doi = {10.1016/S0166-0462(03)00008-5},
file = {:home/abetan16/Dropbox/MendeleyV3/Anselin, Moreno - 2003 - Properties of Tests for Spatial Error Components.pdf:pdf},
isbn = {01660462},
issn = {01660462},
journal = {Regional Science and Urban Economics},
keywords = {Spatial econometrics,Spatial error components,Specification tests},
number = {5},
pages = {595--618},
title = {{Properties of tests for spatial error components}},
volume = {33},
year = {2003}
}
@inproceedings{Aoki1999,
abstract = {Context awareness is an important functionality for wearable computers. In particular, the computer should know where the person is in the environment. This paper proposes an image sequence matching technique for the recognition of locations and previously visited places. As in single word recognition in speech recognition, a dynamic programming algorithm is proposed for the calculation of the similarity of different locations. The system runs on a standalone wearable computer, such as a Libretto PC. Using a training sequence, a dictionary of locations is created automatically. These locations are then recognized by the system in real time using a hat-mounted camera.},
address = {San Francisco, CA, USA},
author = {Aoki, H. and Schiele, B. and Pentland, A.},
booktitle = {Digest of Papers. Third International Symposium on Wearable Computers},
doi = {10.1109/ISWC.1999.806642},
file = {:home/abetan16/Dropbox/MendeleyV3/Aoki, Schiele, Pentland - 1999 - Realtime Personal Positioning System for a Wearable Computer.pdf:pdf},
isbn = {0-7695-0428-0},
pages = {37--43},
publisher = {IEEE Comput. Soc},
title = {{Realtime personal positioning system for a wearable computer}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=806642},
year = {1999}
}
@article{Arev2014,
abstract = {We present an approach that takes multiple videos captured by social cameras---cameras that are carried or worn by members of the group involved in an activity---and produces a coherent "cut" video of the activity. Footage from social cameras contains an intimate, personalized view that reflects the part of an event that was of importance to the camera operator (or wearer). We leverage the insight that social cameras share the focus of attention of the people carrying them. We use this insight to determine where the important "content" in a scene is taking place, and use it in conjunction with cinematographic guidelines to select which cameras to cut to and to determine the timing of those cuts. A trellis graph representation is used to optimize an objective function that maximizes coverage of the important content in the scene, while respecting cinematographic guidelines such as the 180-degree rule and avoiding jump cuts. We demonstrate cuts of the videos in various styles and lengths for a number of scenarios, including sports games, street performances, family activities, and social get-togethers. We evaluate our results through an in-depth analysis of the cuts in the resulting videos and through comparison with videos produced by a professional editor and existing commercial solutions.},
author = {Arev, Ido and Park, Hyun Soo and Sheikh, Yaser and Hodgins, Jessica and Shamir, Ariel},
doi = {10.1145/2601097.2601198},
file = {:home/abetan16/Dropbox/MendeleyV3/Arev et al. - 2014 - Automatic Editing of Footage from Multiple Social Cameras.pdf:pdf},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {multiple cameras,video editing},
month = {jul},
number = {4},
pages = {1--11},
title = {{Automatic Editing of Footage from Multiple Social Cameras}},
url = {http://dl.acm.org/citation.cfm?doid=2601097.2601198},
volume = {33},
year = {2014}
}
@article{Armour2014,
abstract = {Handedness is a human behavioural phenotype that appears to be congenital, and is often assumed to be inherited, but for which the developmental origin and underlying causation(s) have been elusive. Models of the genetic basis of variation in handedness have been proposed that fit different features of the observed resemblance between relatives, but none has been decisively tested or a corresponding causative locus identified. In this study, we applied data from well-characterised individuals studied at the London Twin Research Unit. Analysis of genome-wide SNP data from 3940 twins failed to identify any locus associated with handedness at a genome-wide level of significance. The most straightforward interpretation of our analyses is that they exclude the simplest formulations of the 'right-shift' model of Annett and the 'dextral/chance' model of McManus, although more complex modifications of those models are still compatible with our observations. For polygenic effects, our study is inadequately powered to reliably detect alleles with effect sizes corresponding to an odds ratio of 1.2, but should have good power to detect effects at an odds ratio of 2 or more.},
author = {Armour, J a L and Davison, a and McManus, I C},
doi = {10.1038/hdy.2013.93},
file = {:home/abetan16/Dropbox/MendeleyV3/Armour, Davison, McManus - 2014 - Genome-wide association study of handedness excludes simple genetic models.pdf:pdf},
issn = {1365-2540},
journal = {Heredity},
keywords = {chirality,gwas,handedness},
number = {3},
pages = {221--5},
pmid = {24065183},
publisher = {Nature Publishing Group},
title = {{Genome-wide association study of handedness excludes simple genetic models.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24065183},
volume = {112},
year = {2014}
}
@article{Arous2010a,
author = {Arous, Najet and Ellouze, Noureddine},
doi = {10.4156/jdcta.vol4.issue3.7},
file = {:home/abetan16/Dropbox/MendeleyV3/Arous, Ellouze - 2010 - On the Search of Organization Measures for a Kohonen Map Case Study Speech Signal Recognition(2).pdf:pdf},
issn = {19759339},
journal = {International Journal of Digital Content Technology and its Applications},
number = {3},
pages = {75--84},
title = {{On the Search of Organization Measures for a Kohonen Map Case Study: Speech Signal Recognition}},
url = {http://www.aicit.org/jdcta/ppl/Binder9{\_}Part7.pdf},
volume = {4},
year = {2010}
}
@incollection{Chiappino2014,
abstract = {The objective of this book series is to highlight cutting edge advances and state-of-the-art work being made in the exponentially growing field of Augmented Vision {\&} Reality along its three main axis: Algorithms, Sensors and Applications. This field involves deep theoretical research in sub-areas of machine vision, pattern recognition, robotics, augmented and mixed reality within and beyond the visible spectrum. It presents also a suitable framework for building solid advanced vision based systems.},
address = {Berlin, Heidelberg},
author = {Asari, Vk},
booktitle = {Wide Area Surveillance (Augmented Vision and Reality)},
doi = {10.1007/8612_2012_1},
editor = {Asari, Vijayan K.},
isbn = {9783642378409},
pages = {93--122},
publisher = {Springer Berlin Heidelberg},
series = {Augmented Vision and Reality},
title = {{Wide Area Surveillance}},
url = {http://link.springer.com/content/pdf/10.1007/978-3-642-37841-6.pdf},
volume = {6},
year = {2013}
}
@article{Aviezer2008,
abstract = {Current theories of emotion perception posit that basic facial expressions signal categorically discrete emotions or affective dimensions of valence and arousal. In both cases, the information is thought to be directly "read out" from the face in a way that is largely immune to context. In contrast, the three studies reported here demonstrated that identical facial configurations convey strikingly different emotions and dimensional values depending on the affective context in which they are embedded. This effect is modulated by the similarity between the target facial expression and the facial expression typically associated with the context. Moreover, by monitoring eye movements, we demonstrated that characteristic fixation patterns previously thought to be determined solely by the facial expression are systematically modulated by emotional context already at very early stages of visual processing, even by the first time the face is fixated. Our results indicate that the perception of basic facial expressions is not context invariant and can be categorically altered by context at early perceptual levels.},
author = {Aviezer, Hillel and Hassin, Ran R. and Ryan, Jennifer and Grady, Cheryl and Susskind, Josh and Anderson, Adam and Moscovitch, Morris and Bentin, Shlomo},
doi = {10.1111/j.1467-9280.2008.02148.x},
file = {:home/abetan16/Dropbox/MendeleyV3/Aviezer et al. - 2008 - Angry, Disgusted, or Afraid Studies on the Malleability of Emotion Perception.pdf:pdf},
isbn = {0956-7976, 1467-9280},
issn = {09567976},
journal = {Psychological Science},
keywords = {Adolescent,Affect,Anger,Facial Expression,Fear,Female,Humans,Male,Social Perception,Visual Perception,Young Adult},
month = {jul},
number = {7},
pages = {724--732},
pmid = {18727789},
title = {{Angry, disgusted, or afraid? Studies on the malleability of emotion perception: Research article}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18727789},
volume = {19},
year = {2008}
}
@inproceedings{Bagdanov2012,
abstract = {One of the most critical limitations of KinectTM-based interfaces is the need for persistence in order to interact with virtual objects. Indeed, a user must keep her arm still for a not-so-short span of time while pointing at an object with which she wishes to interact. The most natural way to overcome this limitation and improve interface reactivity is to employ a vision module able to recognize simple hand poses (e.g. open/closed) in order to add a state to the virtual pointer represented by the user hand. In this paper we propose a method to robustly predict the status of the user hand in real-time. We jointly exploit depth and RGB imagery to produce a robust feature for hand representation. Finally, we use temporal filtering to reduce spurious prediction errors. We have also prepared a dataset of more than 30K depth-RGB image pairs of hands that is being made publicly available. The proposed method achieves more than 98{\%} accuracy and is highly responsive.},
author = {Bagdanov, a},
booktitle = {Pattern Recognition (ICPR), 2012 21st International Conference},
file = {:home/abetan16/Dropbox/MendeleyV3/Bagdanov - 2012 - Real-time Hand Status Recognition from Rgb-d Imagery.pdf:pdf},
isbn = {9784990644109},
issn = {10514651},
keywords = {Gesture and Behavior Analysis,Human Computer Interaction},
number = {Icpr},
pages = {2456--2459},
title = {{Real-time Hand Status Recognition from Rgb-d Imagery}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6460664},
year = {2012}
}
@article{Balakrishnan2007,
author = {Balakrishnan, G. and Sainarayanan, G. and Nagarajan, R. and Yaacob, Sazali},
file = {:home/abetan16/Dropbox/MendeleyV3/Balakrishnan et al. - 2007 - Wearable Real-Time Stereo Vision for the Visually Impaired.pdf:pdf},
isbn = {6088320000355},
journal = {Engineering Letters},
number = {2},
pages = {6--14},
title = {{Wearable Real-Time Stereo Vision for the Visually Impaired.}},
volume = {14},
year = {2007}
}
@incollection{Puliti2003,
abstract = {The paper proposes a method for visual based self-localisation of a mobile agent in indoor environment. The images acquired by the camera constitute an implicit topological representation of the environment. The environment is a priori unknown and so the implemented architecture is entirely unsupervised. To compare the performance of some self-organising neural networks, a similar neural network architecture of both Self-Organizing Map (SOM) and Growing Neural Gas (GNG) has been realized. Extensive simulations are provided to characterise the effectiveness of the GNG model in recognition speed, classification tasks and in particular topology preserving as compared to the SOM model. This behaviour depends on the following fact: a network (GNG) that adds nodes into map space can approximate the input space more accurately than a network with a predefined structure and size (SOM). The work shows that the GNG network is able to correctly reconstruct the environment topological map.},
author = {Baldassarri, P and Puliti, Paolo},
booktitle = {Artificial Neural Nets Problem Solving Methods},
doi = {10.1007/3-540-44869-1_26},
file = {:home/abetan16/Dropbox/MendeleyV3/Baldassarri, Puliti - 2003 - Self-organizing maps versus growing neural gas in a robotic application.pdf:pdf},
isbn = {978-3-540-40211-4},
pages = {201--208},
title = {{Self-organizing maps versus growing neural gas in a robotic application}},
url = {http://link.springer.com/chapter/10.1007/3-540-44869-1{\_}26},
year = {2003}
}
@incollection{Ball2000,
address = {Cambridge, MA, USA},
author = {Ball, G. and Breese, J.},
booktitle = {Embodied Conversational Agents},
editor = {Cassell, Justine and Sullivan, Joseph and Prevost, Scott and Churchill, Elizabeth F.},
file = {:home/abetan16/Dropbox/Articles/Mendeley/Ball, Breese - 2000 - Emotion and Personality in a Conversational Agent.pdf:pdf},
isbn = {9780262032780},
pages = {189--219},
publisher = {MIT Press},
title = {{Emotion and personality in a conversational agent}},
year = {2000}
}
@inproceedings{Baltrusaitis2011,
abstract = {We present a real-time system for detecting facial action units and inferring emotional states from head and shoulder gestures and facial expressions. The dynamic system uses three levels of inference on progressively longer time scales. Firstly, facial action units and head orientation are identified from 22 feature points and Gabor filters. Secondly, Hidden Markov Models are used to classify sequences of actions into head and shoulder gestures. Finally, a multi level Dynamic Bayesian Network is used to model the unfolding emotional state based on probabilities of different gestures. The most probable state over a given video clip is chosen as the label for that clip. The average F1 score for 12 action units (AUs 1, 2, 4, 6, 7, 10, 12, 15, 17, 18, 25, 26), labelled on a frame by frame basis, was 0.461. The average classification rate for five emotional states (anger, fear, joy, relief, sadness) was 0.440. Sadness had the greatest rate, 0.64, anger the smallest, 0.11.},
address = {Santa Barbara, CA},
author = {Baltru{\v{s}}aitis, Tadas and McDuff, Daniel and Banda, Ntombikayise and Mahmoud, Marwa and Kaliouby, Rana El and Robinson, Peter and Picard, Rosalind},
booktitle = {2011 IEEE International Conference on Automatic Face and Gesture Recognition and Workshops, FG 2011},
doi = {10.1109/FG.2011.5771372},
file = {:home/abetan16/Dropbox/Articles/Mendeley/Baltrusaitis, McDuff - 2011 - Real-time inference of mental states from facial expressions and upper body gestures.pdf:pdf},
isbn = {9781424491407},
pages = {909--914},
publisher = {IEEE},
title = {{Real-time inference of mental states from facial expressions and upper body gestures}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5771372},
year = {2011}
}
@article{Bane2006,
abstract = {An augmented reality system enhances a mobile user's situational awareness and provides new visualization functionality. The custom-built multimodal interface provides access to information encountered in urban environments. In this article, we detail our experiences with various input devices and modalities and discuss their advantages and drawbacks in the context of interaction tasks in mobile computing. We show how we integrated the input channels to use the modalities beneficially and how this enhances the interface's overall usability},
author = {Bane, Ryan},
file = {:home/abetan16/Dropbox/MendeleyV3/Bane - 2006 - Interaction with a Wearable Augmented.pdf:pdf},
journal = {IEEE Computer Graphics And Applications},
number = {June},
pages = {62--71},
title = {{Interaction with a Wearable Augmented}},
volume = {26},
year = {2006}
}
@inproceedings{Bane2004,
abstract = { This paper presents a set of interactive tools designed to give users virtual x-ray vision. These tools address a common problem in depicting occluded infrastructure: either too much information is displayed, confusing users, or too little information is displayed, depriving users of important depth cues. Four tools are presented: the tunnel tool and room selector tool directly augment the user's view of the environment, allowing them to explore the scene in direct, first person view. The room in miniature tool allows the user to select and interact with a room from a third person perspective, allowing users to view the contents of the room from points of view that would normally be difficult or impossible to achieve. The room slicer tool aids users in exploring volumetric data displayed within the room in miniature tool. Used together, the tools presented in this paper can be used to achieve the virtual x-ray vision effect. We test our prototype system in a far-field mobile augmented reality setup, visualizing the interiors of a small set of buildings on the UCSB campus.},
author = {Bane, Ryan and H{\"{o}}llerer, Tobias},
booktitle = {ISMAR 2004: Proceedings of the Third IEEE and ACM International Symposium on Mixed and Augmented Reality},
doi = {10.1109/ISMAR.2004.36},
file = {:home/abetan16/Dropbox/MendeleyV3/Bane, H{\"{o}}llerer - 2004 - Interactive Tools for Virtual X-Ray Vision in Mobile Augmented Reality.pdf:pdf},
isbn = {0769521916},
number = {Ismar},
pages = {231--239},
publisher = {Ieee},
title = {{Interactive tools for virtual X-ray vision in mobile augmented reality}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1383060},
year = {2004}
}
@article{Barakova2005a,
author = {Barakova, E and Lourens, T},
doi = {10.1080/00207720500382209},
file = {:home/abetan16/Dropbox/MendeleyV3/Barakova, Lourens - 2005 - Efficient episode encoding for spatial navigation.pdf:pdf},
issn = {0020-7721},
journal = {International Journal of Systems Science},
number = {October 2014},
pages = {887--895},
title = {{Efficient episode encoding for spatial navigation}},
volume = {36},
year = {2005}
}
@article{Barakova2005,
abstract = {A method for synergistic integration of multimodal sensor data is proposed in this paper. This method is based on two aspects of the integration process: (1) achieving synergistic integration of two or more sensory modalities, and (2) fusing the various information streams at particular moments during processing. Inspired by psychophysical experiments, we propose a self-supervised learning method for achieving synergy with combined representations. Evidence from temporal registration and binding experiments indicates that different cues are processed individually at specific time intervals. Therefore, an event-based temporal co-occurrence principle is proposed for the integration process. This integration method was applied to a mobile robot exploring unfamiliar environments. Simulations showed that integration enhanced route recognition with many perceptual similarities; moreover, they indicate that a perceptual hierarchy of knowledge about instant movement contributes significantly to short-term navigation, but that visual perceptions have bigger impact over longer intervals.},
author = {Barakova, E and Lourens, T},
doi = {10.1142/S021963520500077X},
file = {:home/abetan16/Dropbox/MendeleyV3/Barakova, Lourens - 2005 - Event Based Self-Supervised Temporal Integration for Multimodal Sensor Data.pdf:pdf},
isbn = {0219-6352 (Print)$\backslash$r0219-6352 (Linking)},
issn = {0219-6352},
journal = {Journal of Integrative Neuroscience},
number = {02},
pages = {265--282},
pmid = {15988800},
title = {{Event Based Self-Supervised Temporal Integration for Multimodal Sensor Data}},
url = {http://www.worldscientific.com/doi/abs/10.1142/S021963520500077X},
volume = {04},
year = {2005}
}
@inproceedings{Baraldi2014,
abstract = {We present a novel method for monocular hand gesture recognition in ego-vision scenarios that deals with static and dynamic gestures and can achieve high accuracy results using a few positive samples. Specifically, we use and extend the dense trajectories approach that has been successfully introduced for action recognition. Dense features are extracted around regions selected by a new hand segmentation technique that integrates superpixel classification, temporal and spatial coherence. We extensively testour gesture recognition and segmentation algorithms on public datasets and propose a new dataset shot with a wearable camera. In addition, we demonstrate that our solution can work in near real-time on a wearable device.},
address = {Columbus, Ohio},
author = {Baraldi, Lorenzo and Paci, Francesco and Serra, Giuseppe},
booktitle = {Computer Vision and Pattern Recognition},
doi = {10.1109/CVPRW.2014.107},
file = {:home/abetan16/Dropbox/MendeleyV3/Baraldi, Paci, Serra - 2014 - Gesture Recognition in Ego-Centric Videos using Dense Trajectories and Hand Segmentation.pdf:pdf},
isbn = {9781479943081},
pages = {688--693},
publisher = {IEEE Computer Society},
title = {{Gesture Recognition in Ego-Centric Videos using Dense Trajectories and Hand Segmentation}},
url = {http://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}workshops{\_}2014/W17/html/Baraldi{\_}Gesture{\_}Recognition{\_}in{\_}2014{\_}CVPR{\_}paper.html},
year = {2014}
}
@article{Baraldi2015,
abstract = {We introduce a novel approach to cultural heritage experience: by means of ego-vision embedded devices we develop a system, which offers a more natural and entertaining way of accessing museum knowledge. Our method is based on distributed self-gesture and artwork recognition, and does not need fixed cameras nor radio-frequency identifications sensors. We propose the use of dense trajectories sampled around the hand region to perform self-gesture recognition, understanding the way a user naturally interacts with an artwork, and demonstrate that our approach can benefit from distributed training. We test our algorithms on publicly available data sets and we extend our experiments to both virtual and real museum scenarios, where our method shows robustness when challenged with real-world data. Furthermore, we run an extensive performance analysis on our ARM-based wearable device.},
author = {Baraldi, Lorenzo and Paci, Francesco and Serra, Giuseppe and Benini, Luca and Cucchiara, Rita},
doi = {10.1109/JSEN.2015.2411994},
file = {:home/abetan16/Dropbox/MendeleyV3/Baraldi et al. - 2015 - Gesture Recognition Using Wearable Vision Sensors to Enhance Visitors' Museum Experiences.pdf:pdf},
issn = {1530-437X},
journal = {IEEE Sensors Journal},
keywords = {ARM-based wearable device,Cameras,Cultural differences,Gesture recognition,Histograms,Sensors,Training,Trajectory,Wearable vision,artwork recognition,cultural heritage experience,dense trajectories,distributed self-gesture recognition,distributed training,ego-vision embedded devices,embedded systems,gesture recognition,history,human computer interaction,image sensors,interactive museum,museum knowledge access,museums,natural interfaces,real museum scenario,virtual museum scenario,virtual reality,visitors museum experience enhancement,wearable vision sensors},
number = {5},
pages = {1--1},
title = {{Gesture Recognition using Wearable Vision Sensors to Enhance Visitors' Museum Experiences}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7058423},
volume = {15},
year = {2015}
}
@inproceedings{Barker2014,
abstract = {In this paper, we present a novel approach for segment- ing video into large regions of generally similar activity. Based on the Dirichlet Process Multinomial Mixture model, we introduce temporal dependency into the inference algo- rithm, allowing our method to automatically create long segments with high saliency while ignoring small, inconse- quential interruptions. We evaluate our algorithm and other topic models with both synthetic datasets and real-world video. Additionally, applicability to image segmentation is shown. Results show that our method outperforms related methods with respect to accuracy and noise removal.},
author = {Barker, Joseph W. and Davis, James W.},
booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2014.88},
file = {:home/abetan16/Dropbox/MendeleyV3/Barker, Davis - 2014 - Temporally-Dependent Dirichlet Process Mixtures for Egocentric Video Segmentation.pdf:pdf},
isbn = {978-1-4799-4308-1},
issn = {21607516},
month = {jun},
pages = {571--578},
publisher = {Ieee},
title = {{Temporally-Dependent Dirichlet Process Mixtures for Egocentric Video Segmentation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6910037},
year = {2014}
}
@inproceedings{Barker2014a,
abstract = {In this paper, we present a novel approach for segment- ing video into large regions of generally similar activity. Based on the Dirichlet Process Multinomial Mixture model, we introduce temporal dependency into the inference algo- rithm, allowing our method to automatically create long segments with high saliency while ignoring small, inconse- quential interruptions. We evaluate our algorithm and other topic models with both synthetic datasets and real-world video. Additionally, applicability to image segmentation is shown. Results show that our method outperforms related methods with respect to accuracy and noise removal.},
author = {Barker, Joseph W. and Davis, James W.},
booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2014.88},
file = {:home/abetan16/Dropbox/MendeleyV3/Barker, Davis - 2014 - Temporally-Dependent Dirichlet Process Mixtures for Egocentric Video Segmentation(2).pdf:pdf},
isbn = {978-1-4799-4308-1},
issn = {21607516},
month = {jun},
pages = {571--578},
publisher = {Ieee},
title = {{Temporally-Dependent Dirichlet Process Mixtures for Egocentric Video Segmentation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6910037},
year = {2014}
}
@inproceedings{Bartneck2002,
abstract = {The OCC (Ortony, Clore, {\&} Collins, 1988) model has established itself as the standard model for emotion syn- thesis. A large number of studies employed the OCC model to generate emotions for their embodied characters. Many developers of such characters believe that the OCC model will be all they ever need to equip their character with emotions. This paper points out what the OCC model is able to do for an embodied emotional character and what it does not. Missing features include a history function, a personality designer and the interaction of the emotional categories.},
address = {Melbourne, Australia.},
author = {Bartneck, Christoph},
booktitle = {Workshop on Virtual Conversational Characters},
doi = {10.1016/j.corsci.2009.11.016},
file = {:home/abetan16/Dropbox/Articles/Mendeley/Bartneck - 2002 - Integrating the OCC Model of Emotions in Embodied Character.pdf:pdf},
pages = {1--5},
publisher = {VCC},
title = {{Integrating the OCC Model of Emotions in Embodied Characters}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.4323{\&}rep=rep1{\&}type=pdf},
year = {2002}
}
@article{Ben-Artzi2014,
abstract = {We introduce a method to determine if two videos are of the same event even when they are taken from significantly different viewing directions. Based on this, a video can be retrieved from a database of events, and video collections can be clustered into sets of videos each viewing the same event. Our method works well even in cases where the videos are so different that appearance based methods (e.g. SIFT) are not sufficient. This viewpoint invariance is based on a new pixel-based feature, "motion barcode", which records the existence/non-existence of motion as a function of time. While appearance, motion magnitude, and motion direction can vary between viewpoints, the existence of motion is viewpoint invariant. Based on the motion barcode, a similarity measure is developed for videos from very different viewpoints. This measure is robust to occlusions common under different viewpoints and can be computed efficiently We show the utility of motion barcodes using challenging sequences from stationary and hand held cameras.},
archivePrefix = {arXiv},
arxivId = {1412.1455},
author = {Ben-Artzi, G and Werman, M and Peleg, S},
eprint = {1412.1455},
file = {:home/abetan16/Dropbox/MendeleyV3/Ben-Artzi, Werman, Peleg - 2015 - Event Matching from Significantly Different Views using Motion Barcodes.pdf:pdf},
journal = {arXiv preprint},
month = {dec},
title = {{Event Matching from Significantly Different Views using Motion Barcodes}},
url = {http://arxiv.org/abs/1412.1455},
year = {2015}
}
@article{Bengio2004,
abstract = {estimation-methods paper},
author = {Bengio, Yoshua and Grandvalet, Yves},
file = {:home/abetan16/Dropbox/MendeleyV3/Bengio, Grandvalet - 2004 - No Unbiased Estimator of the Variance of k-fold Cross-Validation.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {cross-validation,k-fold cross-validation,statistical comparisons,variance estimators},
pages = {1089--1105},
title = {{No Unbiased Estimator of the Variance of K-Fold Cross-Validation}},
url = {http://dl.acm.org/citation.cfm?id=1044695},
volume = {5},
year = {2004}
}
@inproceedings{Berg2006,
abstract = { We demonstrate a method for identifying images containing categories of animals. The images we classify depict animals in a wide range of aspects, configurations and appearances. In addition, the images typically portray multiple species that differ in appearance (e.g. ukari{\&}{\#}146;s, vervet monkeys, spider monkeys, rhesus monkeys, etc.). Our method is accurate despite this variation and relies on four simple cues: text, color, shape and texture. Visual cues are evaluated by a voting method that compares local image phenomena with a number of visual exemplars for the category. The visual exemplars are obtained using a clustering method applied to text on web pages. The only supervision required involves identifying which clusters of exemplars refer to which sense of a term (for example, "monkey" can refer to an animal or a bandmember). Because our method is applied to web pages with free text, the word cue is extremely noisy. We show unequivocal evidence that visual information improves performance for our task. Our method allows us to produce large, accurate and challenging visual datasets mostly automatically.},
author = {Berg, Tamara L. and Forsyth, David a.},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2006.57},
file = {:home/abetan16/Dropbox/Articles/Mendeley/Berg, Forsyth - 2006 - Animals on the Web.pdf:pdf},
isbn = {0769525970},
issn = {10636919},
number = {1},
pages = {1463--1470},
publisher = {IEEE},
title = {{Animals on the web}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1640929},
volume = {2},
year = {2006}
}
@inproceedings{Betancourt2014a,
abstract = {Hand detection is one of the most explored areas in Egocentric Vision Video Analysis for wearable devices. Current methods are focused on pixel-by-pixel hand segmentation, with the implicit assumption of hand presence in almost all activities. However, this assumption is false in many applications for wearable cameras. Ignoring this fact could affect the whole performance of the device since hand measurements are usually the starting point for higher level inference, or could lead to inefficient use of computational resources and battery power. In this paper we propose a two-level sequential classifier, in which the first level, a hand-detector, deals with the possible presence of hands from a global perspective, and the second level, a hand-segmentator, delineates the hand regions at pixel level in the cases indicated by the first block. The performance of the sequential classifier is stated in probabilistic notation as a combination of both, classifiers allowing to test new hand- detectors independently of the type of segmentation and the dataset used in the training stage. Experimental results show a considerable improvement in the detection of true negatives, without compromising the performance of the true positives.},
address = {Columbus, Ohio},
author = {Betancourt, A and Lopez, M and Regazzoni, C.S and Rauterberg, M},
booktitle = {Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPRW.2014.92},
file = {:home/abetan16/Dropbox/MendeleyV3/Betancourt et al. - 2014 - A Sequential Classifier for Hand Detection in the Framework of Egocentric Vision.pdf:pdf},
isbn = {9781479943081},
issn = {21607516},
keywords = {-hand detection,egocentric,first person vision,video processing,vision},
month = {jun},
pages = {600--605},
publisher = {IEEE},
title = {{A Sequential Classifier for Hand Detection in the Framework of Egocentric Vision}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6910041},
volume = {1},
year = {2014}
}
@inproceedings{Betancourt2015a,
abstract = {Hand detection and segmentation methods stand as two of the most most prominent objectives in First Person Vision. Their popularity is mainly ex- plained by the importance of a reliable detection and location of the hands to develop human-machine interfaces for emergent wearable cameras. Current de- velopments have been focused on hand segmentation problems, implicitly as- suming that hands are always in the field of view of the user. Existing methods are commonly presented with new datasets. However, given their implicit as- sumption, none of them ensure a proper composition of frames with and without hands, as the hand-detection problem requires. This paper presents a new dataset for hand-detection, carefully designed to guarantee a good balance between pos- itive and negative frames, as well as challenging conditions such as illumination changes, hand occlusions and realistic locations. Additionally, this paper extends a state-of-the-art method using a dynamic filter to improve its detection rate. The improved performance is proposed as a baseline to be used with the datase},
address = {Malta},
author = {Betancourt, A and Morerio, P and Barakova, E and Marcenaro, L and Rauterberg, M and Regazzoni, C.S},
booktitle = {International Conference on Computer Analysis of Images and Patterns},
doi = {10.1007/978-3-319-23192-1_23},
file = {:home/abetan16/Dropbox/MendeleyV3/Betancourt et al. - 2015 - A Dynamic Approach and a New Dataset for Hand-Detection in First Person Vision.pdf:pdf},
isbn = {9783319231914},
issn = {16113349},
title = {{A Dynamic Approach and a New Dataset for Hand-Detection in First Person Vision.}},
year = {2015}
}
@inproceedings{Betancourt2015b,
abstract = {First Person Vision (Egocentric) video analysis stands nowadays as one of the emerging fields in computer vision. The availability of wearable devices recording exactly what the user is looking at is ineluctable and the opportunities and challenges carried by this kind of devices are broad. Particularly, for the first time a device is so intimate with the user to be able to record the movements of his hands, making hand-based applications for First Person Vision one the most explored area in the field. This paper explores the more popular processing steps to develop hand-based applications, and proposes a hierarchical structure that optimally switches between each of the levels to reduce the computational cost of the system and improve its performance.},
address = {Turin},
author = {Betancourt, A and Morerio, P and Marcenaro, L and Barakova, E and Rauterberg, M and Regazzoni, C.S.},
booktitle = {IEEE International Conference on Multimedia and Expo (Workshops)},
doi = {10.1109/ICMEW.2015.7169784},
file = {:home/abetan16/Dropbox/MendeleyV3/Betancourt et al. - 2015 - Towards a Unified Framework for Hand-based Methods in First Person Vision.pdf:pdf},
isbn = {9781479970797},
publisher = {IEEE},
title = {{Towards a Unified Framework for Hand-based Methods in First Person Vision}},
year = {2015}
}
@inproceedings{Betancourt2015,
abstract = {Classifying frames, or parts of them, is a common way of carrying out detection tasks in computer vision. However, frame by frame classification suffers from sudden significant variations in image texture, colour and luminosity, resulting in noise in the extracted features and consequently in the decisions taken. Support Vector Machines have been widely validated as powerful tools for frame by frame detection of non-separable datasets, but are extremely sensitive to these variations between adjacent frames, creating as consequence sudden flickering in the classification results. This work proposes a Dynamic Bayesian Network to smooth the classification results of Support Vector Machines (SVM) in detection tasks. The method is evaluated in First Person Vision (FPV) videos, where a SVM is used to decide whether or not the user's hands are in his field of view.},
address = {Quebec, Canada},
author = {Betancourt, A and Morerio, P and Marcenaro, L. and Rauterberg, M and Regazzoni, C.S.},
booktitle = {International Conference on Image Processing},
file = {:home/abetan16/Dropbox/MendeleyV3/Betancourt et al. - 2015 - Filtering SVM frame-by-frame binary classification in a detection framework.pdf:pdf},
publisher = {IEEE},
title = {{Filtering SVM frame-by-frame binary classification in a detection framework}},
year = {2015}
}
@article{Betancourt2014,
abstract = {The emergence of new wearable technologies such as action cameras and smart glasses has increased the interest of the computer vision scientists in the First Person perspective. Nowadays, this field is attracting attention and investments of companies aiming to develop commercial devices with First Person Vision recording capabili- ties. Due to this interest, it is expected to have an increasing demand of methods to exploit these videos. The current methods to process these videos present particular combinations of different image features and quantitative methods to accomplish specific objectives like object detection, activity recognition, user machine interaction and so on. This paper summarizes the evolution of the state of the art in First Person Vision video analysis between 1997 and 2013, highlighting the most commonly used features, methods, challenges and opportunities of the field.},
author = {Betancourt, A and Morerio, P and Regazzoni, C.S. and Rauterberg, M},
doi = {10.1109/TCSVT.2015.2409731},
file = {:home/abetan16/Dropbox/MendeleyV3/Betancourt et al. - 2015 - The Evolution of First Person Vision Methods A Survey.pdf:pdf},
issn = {1051-8215},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
keywords = {Computer Vision,Egocentric Vision,First Person Vision,Human- machine Interaction,Smart-Glasses,Video Analytics,Wearable De- vices},
number = {5},
pages = {744--760},
title = {{The Evolution of First Person Vision Methods: A Survey}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7055926},
volume = {25},
year = {2015}
}
@article{Betancourt,
abstract = {This article presents the methodological conceptualization and the main results of a System Dynamics model, which main objective is to support the housing policies in the city of Envigado. The used methodology developed a scenario-based model to emulate the approximate evolution of the housing demand and supply for the city, using a scenario of the Gross Domestic Product (GDP) and a housing authorization strategy as input. Diverse results were obtained, for instance it was found that due to the soil availability, the housing supply reaches the saturation point between 2040 and 2046. Finally this article could be considered as an example of how academic tools such as System Dynamics can be used by decisions makers in the government},
address = {Bucaramanga},
author = {Betancourt, A and Quintero, L},
file = {:home/abetan16/Dropbox/MendeleyV3/Betancourt, Quintero - 2012 - Urban Dynamics Evaluation of Envigado City.pdf:pdf},
journal = {Ecos de Econom{\'{i}}a},
keywords = {Construction,Demography,Inmigration,Migration,Territorial Ordering Plan.,Urban Dynamics},
number = {34},
pages = {30--48},
title = {{Urban Dynamics Evaluation of Envigado City}},
volume = {16},
year = {2012}
}
@inproceedings{Bettadapura,
abstract = {We present a technique that uses images, videos and sensor data taken from first-person point-of-view devices to perform egocentric field-of-view (FOV) localization. We define egocentric FOV localization as capturing the visual information from a person's field-of-view in a given environment and transferring this information onto a reference corpus of images and videos of the same space, hence determining what a person is attending to. Our method matches images and video taken from the first-person perspective with the reference corpus and refines the results using the first-person's head orientation information obtained using the device sensors. We demonstrate single and multi-user egocentric FOV localization in different indoor and outdoor environments with applications in augmented reality, event understanding and studying social interactions.},
address = {Waikoloa, HI},
author = {Bettadapura, Vinay and Essa, Irfan and Pantofaru, Caroline},
booktitle = {2015 IEEE Winter Conference on Applications of Computer Vision},
doi = {10.1109/WACV.2015.89},
file = {:home/abetan16/Dropbox/MendeleyV3/Bettadapura, Essa, Pantofaru - 2015 - Egocentric Field-of-View Localization Using First-Person Point-of-View Devices.pdf:pdf},
isbn = {978-1-4799-6683-7},
number = {1},
pages = {626--633},
title = {{Egocentric Field-of-View Localization Using First-Person Point-of-View Devices}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7045943},
volume = {Jan},
year = {2015}
}
@inproceedings{Bhuvana2013a,
abstract = {In this work, we propose the cubature Kalman filter (CKF) based distributed object tracking algorithm in a visual sensor network (VSN). A VSN consists of several distributed smart cameras having the ability to process and analyze the retrieved data locally. The first objective is to optimize the tracking process within the VSN through the CKF. Under the conditions of non-linear motion and observation model, the CKF based method features a considerably better tracking accuracy than the extended Kalman filter (EKF) based method in terms of the mean square error (MSE). Although, the particle filter (PF) based method shows better performance than the CKF, it is computationally very complex. The second objective is to optimize the object tracking by aggregating the tracking results from multiple cameras. Assuming the VSN is a multi-camera network with overlapping field of views (FOVs), cameras having the same object in their FOV exchange their local estimates of the object's position and velocity. During the estimation process, each of the participating cameras aggregates the received states via a consensus algorithm. Thus, the object's real state is more accurately predicted by the resulting joint state than it would be by processing only a single camera's observation. I.},
author = {Bhuvana, Venkata P. and Schranz, Melanie and Huemer, Mario and Rinner, Bernhard},
booktitle = {Asilomar Conference on Signals, Systems, and Computers},
file = {:home/abetan16/Dropbox/MendeleyV3/Bhuvana et al. - 2013 - Distributed Object Tracking based on Cubature Kalman Filter.pdf:pdf},
pages = {1--5},
title = {{Distributed Object Tracking based on Cubature Kalman Filter}},
year = {2013}
}
@inproceedings{Bhuvana2013,
abstract = {The tracking of the internal states of a battery such as the state-of-charge (SoC) is a substantive task in battery management systems. In general, batteries are represented as linear or non-linear mathematical models. The extended Kalman filter (EKF) and the unscented Kalman filter (UKF) are widely used for the non-linear battery state estimation but their efficiency is limited. Recently, more efficient non-linear state estimation methods such as the cubature Kalman filter (CKF) and the particle filters (PF) have been developed. In this paper, we compare the efficiency and the complexity of different non-linear battery internal state estimation methods based on the EKF, the UKF, the CKF, and the PF. In addition to the SoC, the transient response of the battery is also estimated. The experimental results show that the PF- and the CKF-based methods perform best. Under the chosen conditions, the PF-based method achieves the root mean square error of approximately 3{\%} of the SoC. Although, the efficiency of the PF is slightly better than the CKF, it is computationally more complex.},
address = {Beijing},
author = {Bhuvana, Venkata Pathuri and Unterrieder, Christoph and Christophunterriederaauat, Email and Huemer, Mario},
booktitle = {Vehicle Power and Propulsion Conference},
file = {:home/abetan16/Dropbox/MendeleyV3/Bhuvana et al. - 2013 - Battery Internal State Estimation A Comparative Study of Non-Linear State Estimation Algorithms.pdf:pdf},
isbn = {9781479907205},
number = {1},
pages = {1 -- 6},
publisher = {IEEE Comput. Soc},
title = {{Battery Internal State Estimation : A Comparative Study of Non-Linear State Estimation Algorithms}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6671666},
year = {2013}
}
@inproceedings{Bhuvana2014,
abstract = {Several non-linear state estimation methods such as extended Kalman filter, cubature Kalman filter, and unscented Kalman filter are used to track objects in visual sensor networks. These conventional non-linear state estimation methods require the accurate knowledge of the object's initial conditions, process and measurement models, and corresponding noise characteristics. Often, the object trackers used in a visual sensor networks may not be provided with this knowledge. In this work, we propose a square root cubature H¿ information Kalman filter (SCHIF) based distributed object tracking algorithm. The H¿ method requires neither the exact knowledge of noise characteristic nor accurate process model. The information filters can be used without the knowledge of accurate initial conditions and it also makes the measurement update step computationally less complex in the distributed process. Finally, the square root version makes the filter numerically stable. Furthermore, the cameras in the network exchange their local estimates with other cameras. In the last step, the cameras fuse the received local estimates to obtain a global estimate of the object. Hence, the proposed method constitutes a more robust and efficient solution for the targeted application compared to the traditional methods.},
address = {Salamanca},
author = {Bhuvana, VP},
booktitle = {Information Fusion},
file = {:home/abetan16/Dropbox/MendeleyV3/Bhuvana - 2014 - Distributed object tracking based on square root cubature H-infinity information filter.pdf:pdf},
pages = {1 -- 6},
publisher = {IEEE Signal Processing},
title = {{Distributed object tracking based on square root cubature H-infinity information filter}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6915994},
year = {2014}
}
@article{Blei,
abstract = {Probabilistic topic models are a suite of algorithms whose aim is to discover the hidden thematic structure in large archives of documents. In this article, we review the main ideas of this field, survey the current state-of-the-art, and describe some promising future directions. We first describe latent Dirichlet allocation (LDA) [8], which is the simplest kind of topic model. We discuss its connections to probabilistic modeling, and describe two kinds of algorithms for topic discovery. We then survey the growing body of research that extends and applies topic models in interesting ways. These extensions have been developed by relaxing some of the statistical assumptions of LDA, incorporating meta-data into the analysis of the documents, and using similar kinds of models on a diversity of data types such as social networks, images and genetics. Finally, we give our thoughts as to some of the important unexplored directions for topic modeling. These include rigorous methods for checking models built for data exploration, new approaches to visualizing text and other high dimensional data, and moving beyond traditional information engineering applications towards using topic models for more scientific ends.},
archivePrefix = {arXiv},
arxivId = {1003.4916},
author = {Blei, David M.},
doi = {10.1145/2133806.2133826},
eprint = {1003.4916},
file = {:home/abetan16/Dropbox/MendeleyV3/Blei - Unknown - Introduction to Probabilistic Topic Models.pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
pages = {77--84},
pmid = {7789277},
title = {{Introduction to Probabilistic Topic Modeling}},
url = {ftp://182.71.223.9/old{\_}sharepoint/Labs/Papers/Probabilistic{\_}topicModeling{\_}V2.0(new).pdf},
volume = {55},
year = {2012}
}
@article{Blum2006,
abstract = {Our wearable data collection system lets users collect their experiences into a continually growing and adapting multimedia diary. The system - called inSense - uses the patterns in sensor readings from a camera, microphone, and accelerometers to classify the user's activities and automatically collect multimedia clips when the user is in an "interesting" situation},
author = {Blum, Mark and Pentland, Alex and Tr{\"{o}}ster, Gehrard},
doi = {10.1109/MMUL.2006.87},
file = {:home/abetan16/Dropbox/MendeleyV3/Blum, Pentland, Tr{\"{o}}ster - 2006 - InSense Life Logging.pdf:pdf},
issn = {1070986X},
journal = {IEEE Multimedia},
number = {4},
pages = {40--48},
title = {{InSense: Interest-based life logging}},
volume = {13},
year = {2006}
}
@inproceedings{Bolanos2014,
abstract = {Life-logging devices are characterized by easily collecting huge amount of images. One of the challenges of lifelogging is how to organize the big amount of image data acquired in semantically meaningful segments. In this paper, we propose an energy-based approach for motion-based event segmentation of life-logging sequences of low temporal resolution. The segmentation is reached integrating different kind of image features and classifiers into a graph-cut framework to assure consistent sequence treatment. The results show that the proposed method is promising to create summaries of everyday person's life.},
address = {Palma de Mallorca, Spain},
annote = {Segmentation
BLUR
COLOUR
SIFT
HOG




Graph cut classifier
SVM
KNN},
author = {Bolanos, M and Garolera, Maite and Radeva, Petia},
booktitle = {Articulated Motion and Deformable Objects},
doi = {10.1007/978-3-319-08849-5_1},
file = {:home/abetan16/Dropbox/MendeleyV3/Bolanos, Garolera, Radeva - 2014 - Video Segmentation of Life-Logging Videos.pdf:pdf},
keywords = {life-logging,video segmentation},
pages = {1--9},
publisher = {Springer Verlag},
title = {{Video Segmentation of Life-Logging Videos}},
url = {http://link.springer.com/chapter/10.1007/978-3-319-08849-5{\_}1},
year = {2014}
}
@article{Bolanos2015,
abstract = {Recent technology of visual lifelogging consists in acquiring images that capture our everyday experience by wearing a camera over long periods of time. The collected data offer a large potential of mining and inferring knowledge about how people live their lives and for analyzing the story behind these data. This opens new opportunities for a large amount of potential applications ranging from health-care, security or leisure, to quantified self. However, extracting and locating relevant content from a huge collection of personal data triggers major challenges that strongly limit their utility and usability in practice. Furthermore, due to the first-person view nature of lifelogging data and to the technical specifications of the imaging sensor, many available Computer Vision techniques become inadequate, demanding full or partial re-formulation. This review aims to provide a comprehensive summary of what is currently available and what would be needed from a Computer Vision perspective to complete the building blocks for automatic personal storytelling, being a suitable reference for all Computer Vision scientists interested in Egocentric vision in general, and in Storytelling through visual lifelogging.},
archivePrefix = {arXiv},
arxivId = {1507.06120},
author = {Bola{\~{n}}os, Marc and Dimiccoli, Mariella and Radeva, Petia},
eprint = {1507.06120},
file = {:home/abetan16/Dropbox/MendeleyV3/Bola{\~{n}}os, Dimiccoli, Radeva - 2015 - Towards Storytelling from Visual Lifelogging An Overview.pdf:pdf},
journal = {Submitted to IEEE Transactions on Human-Machine Systems},
number = {July},
pages = {1--17},
title = {{Towards Storytelling from Visual Lifelogging: An Overview}},
url = {http://arxiv.org/abs/1507.06120},
year = {2015}
}
@inproceedings{Borji2012,
abstract = {Despite a considerable amount of previous work on bottom-up saliency modeling for predicting human fixations over static and dynamic stimuli, few studies have thus far attempted to model top-down and task-driven influences of visual attention. Here, taking advantage of the sequential nature of real-world tasks, we propose a unified Bayesian approach for modeling task-driven visual attention. Sev- eral sources of information, including global context of a scene, previous attended locations, and previous motor ac- tions, are integrated over time to predict the next attended location. Recording eye movements while subjects engage in 5 contemporary 2D and 3D video games, as modest coun- terparts of everyday tasks, we show that our approach is able to predict human attention and gaze better than the state-of-the-art, with a large margin (about 15{\%} increase in prediction accuracy). The advantage of our approach is that it is automatic and applicable to arbitrary visual tasks.},
address = {Providence, RI},
author = {Borji, Ali and Sihite, Dicky N. and Itti, Laurent},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2012.6247710},
file = {:home/abetan16/Dropbox/Mendeley/Borji, Sihite, Itti - 2012 - Probabilistic Learning of Task-Specific Visual Attention.pdf:pdf},
isbn = {9781467312264},
issn = {10636919},
month = {jun},
pages = {470--477},
publisher = {Ieee},
title = {{Probabilistic learning of task-specific visual attention}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6247710},
year = {2012}
}
@inproceedings{Bosch2007,
address = {New York, New York, USA},
author = {Bosch, Anna and Zisserman, Andrew and Munoz, Xavier},
booktitle = {Proceedings of the 6th ACM international conference on Image and video retrieval - CIVR '07},
doi = {10.1145/1282280.1282340},
file = {:home/abetan16/Dropbox/MendeleyV3/Bosch, Zisserman, Munoz - 2007 - Representing Shape with a Spatial Pyramid Kernel.pdf:pdf},
isbn = {9781595937339},
keywords = {object and video,shape features,spatial pyramid kernel},
pages = {401--408},
publisher = {ACM Press},
title = {{Representing shape with a spatial pyramid kernel}},
url = {http://dl.acm.org/citation.cfm?doid=1282280.1282340},
year = {2007}
}
@article{Boujut2012,
abstract = {In this paper we are interested in the saliency of visual content from wearable cameras. The subjective saliency in wearable video is studied first due to the psycho-visual experience on this content. Then the method for objective saliency map computation with a specific contribution based on geometrical saliency is proposed. Fusion of spatial, temporal and geometric cues in an objective saliency map is realized by the multiplicative operator. Resulting objective saliency maps are evaluated against the subjective maps with promising results, highlighting interesting performance of proposed geometric saliency model.},
author = {Boujut, Hugo and Benois-Pineau, Jenny and Megret, Remi},
doi = {10.1007/978-3-642-33885-4_44},
file = {:home/abetan16/Dropbox/MendeleyV3/Boujut, Benois-Pineau, Megret - 2012 - Fusion of Multiple Visual Cues for Visual Saliency Extraction from Wearable Camera Settings with.pdf:pdf},
isbn = {9783642338847},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 3},
pages = {436--445},
title = {{Fusion of multiple visual cues for visual saliency extraction from wearable camera settings with strong motion}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-33885-4{\_}44},
volume = {7585 LNCS},
year = {2012}
}
@inproceedings{Bradski1998,
abstract = {As a step towards a perceptual user interface, an object tracking$\backslash$nalgorithm is developed and demonstrated tracking human faces. Computer$\backslash$nvision algorithms that are intended to form part of a perceptual user$\backslash$ninterface must be fast and efficient. They must be able to track in real$\backslash$ntime and yet not absorb a major share of computational resources. An$\backslash$nefficient, new algorithm is described here based on the mean shift$\backslash$nalgorithm. The mean shift algorithm robustly finds the mode (peak) of$\backslash$nprobability distributions. We first describe histogram based methods of$\backslash$nproducing object probability distributions. In our case, we want to$\backslash$ntrack the mode of an object's probability distribution within a video$\backslash$nscene. Since the probability distribution of the object can change and$\backslash$nmove dynamically in time, the mean shift algorithm is modified to deal$\backslash$nwith dynamically changing probability distributions. The modified$\backslash$nalgorithm is called the Continuously Adaptive Mean Shift (CAMSHIFT)$\backslash$nalgorithm. CAMSHIFT is then used as an interface for games and graphics$\backslash$n},
author = {Bradski, G.R.},
booktitle = {Proceedings Fourth IEEE Workshop on Applications of Computer Vision. WACV'98 (Cat. No.98EX201)},
doi = {10.1109/ACV.1998.732882},
file = {:home/abetan16/Dropbox/MendeleyV3/Bradski - 1998 - Real Time Face and Object Tracking as a Component of a Perceptual User Interface.pdf:pdf},
isbn = {0-8186-8606-5},
issn = {09600760},
pages = {14--19},
pmid = {10418979},
publisher = {IEEE},
title = {{Real time face and object tracking as a component of a perceptual$\backslash$nuser interface}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=732882},
year = {1998}
}
@article{Bramley2009,
author = {Bramley, Glen and Power, Sin{\'{e}}ad},
doi = {10.1068/b33129},
file = {:home/abetan16/Dropbox/MendeleyV3/Bramley, Power - 2009 - Urban form and social sustainability the role of density and housing type.pdf:pdf},
issn = {0265-8135},
journal = {Environment and Planning B: Planning and Design},
number = {1},
pages = {30--48},
title = {{Urban form and social sustainability: the role of density and housing type}},
url = {http://epb.sagepub.com/lookup/doi/10.1068/b33129},
volume = {36},
year = {2009}
}
@article{Brandt2010,
abstract = {Fictitious play is a simple learning algorithm for strategic games that proceeds in rounds. In each round, the players play a best response to a mixed strategy that is given by the empirical frequencies of actions played in previous rounds. There is a close relationship between fictitious play and the Nash equilibria of a game: if the empirical frequencies of fictitious play converge to a strategy profile, this strategy profile is a Nash equilibrium. While fictitious play does not converge in general, it is known to do so for certain restricted classes of games, such as constant-sum games, non-degenerate 2×n games, and potential games. We study the rate of convergence of fictitious play and show that, in all the classes of games mentioned above, fictitious play may require an exponential number of rounds (in the size of the representation of the game) before some equilibrium action is eventually played. In particular, we show the above statement for symmetric constant-sum win-lose-tie games.},
author = {Brandt, Felix and Fischer, Felix and Harrenstein, Paul},
doi = {10.1007/s00224-013-9460-5},
file = {:home/abetan16/Dropbox/MendeleyV3/Brandt, Fischer, Harrenstein - 2013 - On the Rate of Convergence of Fictitious Play.pdf:pdf},
issn = {1432-4350},
journal = {Theory of Computing Systems},
month = {apr},
number = {1},
pages = {41--52},
title = {{On the Rate of Convergence of Fictitious Play}},
url = {http://link.springer.com/10.1007/s00224-013-9460-5},
volume = {53},
year = {2013}
}
@article{Breiman2001,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning : Proceedings of the Thirteenth International conference , ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
archivePrefix = {arXiv},
arxivId = {http://dx.doi.org/10.1023{\%}2FA{\%}3A1010933404324},
author = {Breiman, Leo},
doi = {10.1023/A:1010933404324},
eprint = {/dx.doi.org/10.1023{\%}2FA{\%}3A1010933404324},
file = {:home/abetan16/Dropbox/MendeleyV3/Breiman - 2001 - Random forests.pdf:pdf},
isbn = {9781424444427},
issn = {08856125},
journal = {Machine Learning},
keywords = {classiﬁcation,ensemble,regression},
number = {1},
pages = {5--32},
pmid = {20142443},
primaryClass = {http:},
title = {{Random Forests}},
url = {http://portal.acm.org/citation.cfm?id=570182$\backslash$nhttp://www.springerlink.com/content/u0p06167n6173512},
volume = {45},
year = {2001}
}
@article{Brox2004,
abstract = {We study an energy functional for computing optical flow that combines three assumptions: a brightness constancy assumption, a gradient constancy assumption, and a discontinuity-preserving spatio-temporal smoothness constraint. In order to allow for large displacements, linearisations in the two data terms are strictly avoided. We present a consistent numerical scheme based on two nested fixed point iterations. By proving that this scheme implements a coarse-to-fine warping strategy, we give a theoretical foundation for warping which has been used on a mainly experimental basis so far. Our evaluation demonstrates that the novel method gives significantly smaller angular errors than previous techniques for optical flow estimation.We show that it is fairly insensitive to parameter variations, and we demonstrate its excellent robustness under noise.},
author = {Brox, Thomas and Papenberg, Nils and Weickert, Joachim},
doi = {10.1007/978-3-540-24673-2_3},
file = {:home/abetan16/Dropbox/MendeleyV3/Brox, Papenberg, Weickert - 2004 - High Accuracy Optical Flow Estimation Based on a Theory for Warping.pdf:pdf},
isbn = {9783540219811},
journal = {Computer Vision - ECCV 2004},
number = {May},
pages = {25--36},
title = {{High Accuracy Optical Flow Estimation Based on a Theory for Warping}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-24673-2{\_}3},
volume = {4},
year = {2004}
}
@inproceedings{Buehler2009,
abstract = {The goal of this work is to automatically learn a large number of British sign language (BSL) signs from TV broadcasts. We achieve this by using the supervisory information available from subtitles broadcast simultaneously with the signing. This supervision is both weak and noisy: it is weak due to the correspondence problem since temporal distance between sign and subtitle is unknown and signing does not follow the text order; it is noisy because subtitles can be signed in different ways, and because the occurrence of a subtitle word does not imply the presence of the corresponding sign. The contributions are: (i) we propose a distance function to match signing sequences which includes the trajectory of both hands, the hand shape and orientation, and properly models the case of hands touching; (ii) we show that by optimizing a scoring function based on multiple instance learning, we are able to extract the sign of interest from hours of signing footage, despite the very weak and noisy supervision. The method is automatic given the English target word of the sign to be learnt. Results are presented for 210 words including nouns, verbs and adjectives.},
author = {Buehler, P. and Zisserman, A. and Everingham, M.},
booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2009.5206523},
file = {:home/abetan16/Dropbox/MendeleyV3/Buehler, Zisserman, Everingham - 2009 - Learning Sign Language by Watching Tv (Using Weakly Aligned Subtitles).pdf:pdf},
isbn = {978-1-4244-3992-8},
issn = {1063-6919},
keywords = {British sign language signs,Cranes,Deafness,Handicapped aids,History,Multi-stage noise shaping,Noise shaping,Shape,TV broadcasting,TV broadcasts,Training data,Video sequences,computer aided instruction,feature extraction,image sequences,learning sign language,match signing sequences,natural language processing,sign of interest extraction,television broadcasting,television programs,video signal processing},
month = {jun},
pages = {2961--2968},
publisher = {Ieee},
title = {{Learning sign language by watching TV (using weakly aligned subtitles)}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5206523},
year = {2009}
}
@article{Bull2014,
abstract = {Image segmentation seeks to partition the pixels in images into distinct regions to assist other image processing functions such as object recognition. Over the last few years dictionary learning methods have become very popular for image processing tasks such as denoising, and recently structured low rank dictionary learning has been shown to be capable of promising results for recognition tasks. This paper investigates the suitability of dictionary learning for image segmentation. A structured low rank dictionary learning algorithm is developed to segment images using compressed sensed features from image patches. To enable a supervised learning approach, classes of pixels in images are designated using training scribbles. A classifier is then learned from these training pixels and subsequently used to classify all other pixels in the images to form the segmentations. A number of dictionary learning models are compared together with K-means/nearest neighbour and support vector machine classifiers. {\textcopyright} 2014 IEEE.},
author = {Bull, G and Gao, J and Antolovich, M},
doi = {10.1109/DICTA.2014.7008112},
file = {:home/abetan16/Dropbox/MendeleyV3/Bull, Gao, Antolovich - 2014 - Image segmentation using random features.pdf:pdf},
isbn = {9781479954094},
journal = {2014 International Conference on Digital Image Computing: Techniques and Applications, DICTA 2014},
keywords = {Classification (of information); Compressed sensin,Dictionary learning; Dictionary learning algorith,Image segmentation},
number = {Icgip 2013},
pages = {90691Z},
title = {{Image segmentation using dictionary learning and compressed random features}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84922573046{\&}partnerID=40{\&}md5=27879ac41239894f34e2d4b32f1bed74},
volume = {9069},
year = {2015}
}
@article{Bullock2013,
abstract = {Robotic and prosthetic hand designers are challenged to replicate as much functionality of the human hand as possible, while minimizing cost and any unnecessary complexity. Selecting which aspects of human hand function to emulate can be difficult, especially when little data is available on unstructured human manipulation behavior. The present work analyzes 19 hours of video with over 9000 grasp instances from two housekeepers and two machinists to find small sets of versatile human grasps. A novel grasp span metric is used to evaluate sets of grasps and pick an optimal grasp set which can effectively handle as many different objects as possible. The results show medium wrap and lateral pinch are both important, versatile grasps for basic object handling. The results suggest that three-fingertip precision grasps such as thumb-2 finger, tripod, or lateral tripod can be used to handle dexterous manipulation of a wide range of objects. The recommended grasp sets can help aid difficult design decisions for robotic and prosthetic hands, as well as suggesting important human hand functionality to restore during hand surgery or rehabilitate in an impaired hand.},
author = {Bullock, Ian M. and Feix, Thomas and Dollar, Aaron M.},
doi = {10.1109/ICRA.2013.6630705},
file = {:home/abetan16/Dropbox/MendeleyV3/Bullock, Feix, Dollar - 2013 - Finding small, versatile sets of human grasps to span common objects.pdf:pdf},
isbn = {9781467356411},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {Biomimetics,Dexterous Manipulation,Grasping},
pages = {1068--1075},
title = {{Finding small, versatile sets of human grasps to span common objects}},
year = {2013}
}
@article{Bullock2013a,
abstract = {This paper presents a study on the usage frequency of different grasp types throughout the daily functions of a professional house maid and a machinist. Subjects wore a head-mounted camera that recorded their hand usage during their daily work activities. This video was then analyzed, recording grasp type and associated time stamps, as well as information related to the task and object. The results show that nearly 80{\%} of the time the house maid used just six grasps and the machinist used nine. This data, in conjunction with established grasp taxonomies, will enable a better understanding of how people utilize different grasps to accomplish tasks throughout the day, as well as inform the design of robotic and prosthetic hands.},
author = {Bullock, Ian M. and Zheng, Joshua Z. and {De La Rosa}, Sara and Guertler, Charlotte and Dollar, Aaron M.},
doi = {10.1109/TOH.2013.6},
file = {:home/abetan16/Dropbox/MendeleyV3/Bullock et al. - 2013 - Grasp frequency and usage in daily household and machine shop tasks.pdf:pdf},
isbn = {9781612843865},
issn = {19391412},
journal = {IEEE Transactions on Haptics},
keywords = {Human grasping,activities of daily living,manipulation,prosthetics,robotic hands},
number = {3},
pages = {296--308},
pmid = {24808326},
title = {{Grasp frequency and usage in daily household and machine shop tasks}},
volume = {6},
year = {2013}
}
@article{Burke2008,
abstract = {Recent studies have shown the potential benefits of applying technology to stroke rehabilitation. Traditional rehabilitation tasks are often mundane, and can lead to a lack of patient motivation, resulting in little or no independent patient exercise taking place. We describe a low-cost visual tracking system suitable for upper limb stroke rehabilitation in the home which does not require expensive or special equipment and which can be operated entirely independently by the patient. We look at past applications of imaging to stroke rehabilitation, and also game design theory and how it can be applied to stroke rehabilitation. We then describe two small prototype games which implement colour/object segmentation and motion detection to form the basis of our home rehabilitation system.},
author = {Burke, J. W. and Morrow, P. J. and McNeill, M. D J and McDonough, S. M. and Charles, D. K.},
doi = {10.1109/IMVIP.2008.16},
file = {:home/abetan16/Dropbox/MendeleyV3/Burke et al. - 2008 - Vision based games for upper-limb stroke rehabilitation.pdf:pdf},
isbn = {9780769533322},
journal = {Proceedings - IMVIP 2008, 2008 International Machine Vision and Image Processing Conference},
pages = {159--164},
title = {{Vision based games for upper-limb stroke rehabilitation}},
year = {2008}
}
@inproceedings{Buso2014,
abstract = {In the problem of ”human sensing”, videos recorded with wearable cameras give an ”egocentric” view of the world, capturing details of human activities. In this paper we con- tinue research on visual saliency for such kind of content with the goal of ”active” objects recognition in egocentric videos. In particular, a geometrical cue is considered in case when the central-bias hypothesis does not hold. The proposed visual saliency models are trained based on eye fixations of observers and incorporated into spatio-temporal saliency models. The proposed models have been compared to state of the art visual saliency models using a metric based on target object recognition performances. The results are promising:they highlight the necessity of a non-centered ge- ometric saliency cue.},
address = {New York, New York, USA},
author = {Buso, Vincent and Benois-Pineau, Jenny and Domenger, Jean-Philippe},
booktitle = {Proceedings of the 1st International Workshop on Perception Inspired Video Processing - PIVP '14},
doi = {10.1145/2662996.2663007},
file = {:home/abetan16/Dropbox/MendeleyV3/Buso, Benois-Pineau, Domenger - 2014 - Geometrical Cues in Visual Saliency Models for Active Object Recognition in Egocentric Videos.pdf:pdf},
isbn = {9781450331258},
keywords = {egocentric vision,object recognition,saliency maps,spatio-temporal model},
pages = {9--14},
publisher = {ACM Press},
title = {{Geometrical Cues in Visual Saliency Models for Active Object Recognition in Egocentric Videos}},
url = {http://dl.acm.org/citation.cfm?id=2662996.2663007},
year = {2014}
}
@article{Buso2015,
author = {Buso, Vincent and Gonz{\'{a}}lez-D{\'{i}}az, Iv{\'{a}}n and Benois-Pineau, Jenny},
doi = {10.1016/j.image.2015.05.006},
file = {:home/abetan16/Dropbox/MendeleyV3/Buso, Gonz{\'{a}}lez-D{\'{i}}az, Benois-Pineau - 2015 - Goal-oriented top-down probabilistic visual attention model for recognition of manipulated.pdf:pdf},
issn = {09235965},
journal = {Signal Processing: Image Communication},
number = {June},
title = {{Goal-oriented top-down probabilistic visual attention model for recognition of manipulated objects in egocentric videos}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0923596515000892},
year = {2015}
}
@article{Byrne2010,
abstract = {The Microsoft SenseCam is a small lightweight wearable camera used to passively capture photos and other sensor readings from a user's day-to-day activi- ties. It captures on average 3,000 images in a typical day, equating to almost 1 million images per year. It can be used to aid memory by creating a personal multimedia lifelog, or visual recording of the wearer's life. However the sheer volume of image data captured within a visual lifelog creates a number of challenges, particularly for locating relevant content. Within this work, we explore the applicability of semantic concept detection, a method often used within video retrieval, on the domain of visual lifelogs. Our concept detector models the correspondence between low-level visual features and high-level semantic concepts (such as indoors, outdoors, people, buildings, etc.) using supervised machine learning. By doing so it determines the probability of a concept's presence. We apply detection of 27 everyday semantic concepts on a lifelog collection composed of 257,518 SenseCam images from 5 users. The results were evaluated on a subset of 95,907 images, to determine the accuracy for detection of each semantic concept. We conducted further analysis on the temporal consistency, co-occurance and relationships within the detected concepts to more extensively investigate the robustness of the detectors within this domain.},
author = {Byrne, Daragh and Doherty, Aiden R. and Snoek, Cees G. M. and Jones, Gareth J. F. and Smeaton, Alan F.},
doi = {10.1007/s11042-009-0403-8},
file = {:home/abetan16/Downloads/LifelogEverydayConceptsCRC{\_}LowRes.pdf:pdf},
issn = {13807501},
journal = {Multimedia Tools and Applications},
number = {1},
pages = {119--144},
title = {{Everyday concept detection in visual lifelogs: validation, relationships and trends}},
url = {http://link.springer.com/article/10.1007/s11042-009-0403-8},
volume = {49},
year = {2010}
}
@article{Cai2015,
abstract = {— Our goal is to automatically recognize hand grasps and to discover the visual structures (relationships) between hand grasps using wearable cameras. Wearable cameras pro-vide a first-person perspective which enables continuous visual hand grasp analysis of everyday activities. In contrast to previous work focused on manual analysis of first-person videos of hand grasps, we propose a fully automatic vision-based approach for grasp analysis. A set of grasp classifiers are trained for discriminating between different grasp types based on large margin visual predictors. Building on the output of these grasp classifiers, visual structures among hand grasps are learned based on an iterative discriminative clustering procedure. We first evaluated our classifiers on a controlled indoor grasp dataset and then validated the analytic power of our approach on real-world data taken from a machinist. The average F1 score of our grasp classifiers achieves over 0.80 for the indoor grasp dataset. Analysis of real-world video shows that it is possible to automatically learn intuitive visual grasp structures that are consistent with expert-designed grasp taxonomies.},
author = {Cai, M and Kitani, K and Sato, Y},
doi = {10.1109/ICRA.2015.7139367},
file = {:home/abetan16/Dropbox/MendeleyV3/Cai, Kitani, Sato - 2015 - A Scalable Approach for Understanding the Visual Structures of Hand Grasps.pdf:pdf},
isbn = {9781479969227},
journal = {IEEE International Conference on Robotics and Automation},
pages = {1360--1366},
title = {{A Scalable Approach for Understanding the Visual Structures of Hand Grasps}},
year = {2015}
}
@inproceedings{Calonder2010,
abstract = {We propose to use binary strings as an efficient feature point descriptor, which we call BRIEF.We show that it is highly discriminative even when using relatively few bits and can be computed using simple intensity difference tests. Furthermore, the descriptor similarity can be evaluated using the Hamming distance, which is very efficient to com- pute, instead of the L2 norm as is usually done. As a result, BRIEF is very fast both to build and to match.We compare it against SURF and U-SURF on standard benchmarks and show that it yields a similar or better recognition performance, while running in a fraction of the time required by either.},
author = {Calonder, M. and Lepetit, V. and Strecha, C. and Fua, P.},
booktitle = {European Conference on Computer Vision (ECCV)},
doi = {10.1007/978-3-642-15561-1_56},
file = {:home/abetan16/Downloads/calonder{\_}eccv10.pdf:pdf},
pages = {778--792},
title = {{BRIEF : Binary Robust Independent Elementary Features}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-15561-1{\_}56},
year = {2010}
}
@book{Camastra2007,
abstract = {Machine Learning involves several scientific domains including mathematics, computer science, statistics and biology, and is an approach that enables computers to automatically learn from data. Focusing on complex media and how to convert raw data into useful information, this book offers both introductory and advanced material in the combined fields of machine learning and image/video processing. The machine learning techniques presented enable readers to address many real world problems involving complex data. Examples covering areas such as automatic speech and handwriting transcription, automatic face recognition, and semantic video segmentation are included, along with detailed introductions to algorithms and examples of their applications. The book is organized in four parts: The first focuses on technical aspects, basic mathematical notions and elementary machine learning techniques. The second provides an extensive survey of most relevant machine learning techniques for media processing, while the third part focuses on applications and shows how techniques are applied in actual problems. The fourth part contains detailed appendices that provide notions about the main mathematical instruments used throughout the text. Students and researchers needing a solid foundation or reference, and practitioners interested in discovering more about the state-of-the-art will find this book invaluable. Examples and problems are based on data and software packages publicly available on the web.},
author = {Camastra, Francesco and Vinciarelli, Alessandro},
isbn = {9781848000063},
pages = {487},
publisher = {Springer},
title = {{Machine learning for audio, image and video analysis: theory and applications}},
url = {http://books.google.com/books?id=YXPqVIHaVYsC},
year = {2008}
}
@inproceedings{Carreira2011,
abstract = {We present a novel framework for creating and ranking plausible object hypotheses in an image using bottom-up generation processes and mid-level selection cues. The object hypotheses are represented as figure-ground segmentations, and are extracted automatically, without prior knowledge about the properties of individual object classes, by solving a sequence of constrained parametric min-cut problems (CPMC) on a regular image grid. In a subsequent step, we learn to rank the corresponding segments by training a continuous model to predict their plausibility (putative overlap with ground truth) based on their mid-level region properties, then diversify the estimated overlap using maximum marginal relevance measures. We show that this algorithm significantly outperforms the state of the art for low-level segmentation in the VOC 2009 and 2010 datasets. It achieves the same average best segmentation covering on VOC2009 as the best performing technique to date [1], 0.61 when using just the top 7 ranked segments, instead of the full hierarchy in [1]. Our method achieves 0.78 average best covering using 154 segments. An extended version of the basic algorithm achieves 83{\%} average per class object recall, using 200 segments per image on the VOC2010 segmentation dataset.},
author = {Carreira, Joao and Sminchisescu, Cristian},
booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
doi = {10.1109/TPAMI.2011.231},
file = {:home/abetan16/Dropbox/MendeleyV3/Carreira, Sminchisescu - 2012 - CPMC Automatic Object Segmentation Using Constrained Parametric Min-Cuts.pdf:pdf},
issn = {0162-8828},
month = {nov},
number = {7},
pages = {1312--1328},
pmid = {22144523},
publisher = {IEEE},
title = {{CPMC: Automatic Object Segmentation Using Constrained Parametric Min-Cuts}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6095566},
volume = {34},
year = {2012}
}
@article{Casella1992,
author = {Casella, G and George, EI},
file = {:home/abetan16/Dropbox/MendeleyV3/Casella, George - 1992 - Explaining the Gibbs Sampler.pdf:pdf},
journal = {The American Statistician},
keywords = {data augmentation,markov chains,monte carlo methods,resampling techniques},
number = {3},
pages = {167--174},
title = {{Casella, George - 1992 - Explaining the Gibbs Sampler}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:No+Title{\#}0 http://www.tandfonline.com/doi/abs/10.1080/00031305.1992.10475878},
volume = {46},
year = {1992}
}
@incollection{Castellano2007,
abstract = {We present an approach for the recognition of acted emotional states based on the analysis of body movement and gesture expressivity. According to research showing that distinct emotions are often associated with different qualities of body movement, we use nonpropositional movement qualities (e.g. amplitude, speed and fluidity of movement) to infer emotions, rather than trying to recognise different gesture shapes expressing specific emotions. We propose a method for the analysis of emotional behaviour based on both direct classification of time series and a model that provides indicators describing the dynamics of expressive motion cues. Finally we show and interpret the recognition rates for both proposals using different classification algorithms.},
author = {Castellano, Ginevra and Villalba, Santiago D and Camurri, Antonio},
booktitle = {Affective Computing and Intelligent Interaction},
doi = {10.1007/978-3-540-74889-2_7},
file = {:home/abetan16/Dropbox/MendeleyV3/Castellano, Villalba, Camurri - 2007 - Recognising Human Emotions from Body Movement and Gesture Dynamics.pdf:pdf},
isbn = {2002507422},
issn = {0302-9743},
pages = {71{\_}82},
publisher = {Springerlink},
title = {{Recognising Human Emotions from Body Movement and Gesture Dynamics}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-74889-2{\_}7},
volume = {4738},
year = {2007}
}
@inproceedings{Castle2008,
abstract = {We show how a system for video-rate parallel camera tracking and 3D map-building can be readily extended to allow one or more cameras to work in several maps, separately or simultaneously. The ability to handle several thousand features per map at video-rate, and for the cameras to switch automatically between maps, allows spatially localized AR workcells to be constructed and used with very little intervention from the user of a wearable vision system. The user can explore an environment in a natural way, acquiring local maps in real-time. When revisiting those areas the camera will select the correct local map from store and continue tracking and structural acquisition, while the user views relevant AR constructs registered to that map.},
address = {Pittsburgh, PA},
author = {Castle, R. and Klein, G. and Murray, D.W.},
booktitle = {IEEE International Symposium on Wearable Computers},
doi = {10.1109/ISWC.2008.4911577},
file = {:home/abetan16/Dropbox/MendeleyV3/Castle, Klein, Murray - 2008 - Video-rate Localization in Multiple Maps for Wearable Augmented Reality.pdf:pdf},
isbn = {978-1-4244-2637-9},
issn = {1550-4816},
pages = {15--22},
publisher = {IEEE},
title = {{Video-rate localization in multiple maps for wearable augmented reality}},
url = {http://doi.ieeecomputersociety.org/10.1109/ISWC.2008.4911577},
year = {2008}
}
@inproceedings{Castle2007,
abstract = {Using simultaneous localization and mapping to determine the 3D surroundings and pose of a wearable or hand-held camera provides the geometrical foundation for several capabilities of value to an autonomous wearable vision system. The one explored here is the ability to incorporate recognized objects into the map of the surroundings and refer to them. Established methods for feature cluster recognition are used to identify and localize known planar objects, and their geometry is incorporated into the map of the surrounds using a minimalist representation. Continued measurement of these mapped objects improves both the accuracy of estimated maps and the robustness of the tracking system. In the context of wearable (or hand-held) vision, the system's ability to enhance generated maps with known objects increases the map's value to human operators, and also enables meaningful automatic annotation of the user's surroundings.},
address = {Warwick},
author = {Castle, R.O. and Gawley, D.J. and Klein, G. and Murray, D.W.},
booktitle = {IEEE Transactions on Robotics},
doi = {10.5244/C.21.112},
file = {:home/abetan16/Dropbox/Mendeley/Castle et al. - 2007 - Video-rate recognition and localization for wearable cameras.pdf:pdf},
isbn = {1-901725-34-0},
pages = {112.1--112.10},
publisher = {British Machine Vision Association},
title = {{Video-rate recognition and localization for wearable cameras}},
url = {http://www.bmva.org/bmvc/2007/papers/paper-97.html},
year = {2007}
}
@article{Cayton2005,
abstract = {Manifold learning is a popular recent approach to nonlinear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high; though each data point consists of perhaps thousands of features, it may be described as a function of only a few underlying parameters. That is, the data points are actually samples from a low-dimensional manifold that is embedded in a high-dimensional space. Manifold learning algorithms attempt to uncover these parameters in order to find a low-dimensional representation of the data. In this paper, we discuss the motivation, background, and algorithms proposed for manifold learning. Isomap, Locally Linear Embedding, Laplacian Eigenmaps, Semidefinite Embedding, and a host of variants of these algorithms are examined.},
author = {Cayton, Lawrence},
file = {:home/abetan16/Dropbox/MendeleyV3/Cayton - 2005 - Algorithms for manifold learning.pdf:pdf},
isbn = {CS2008-0923},
journal = {Univ of California at San Diego Tech Rep},
number = {CS2008-0923},
pages = {973--80},
title = {{Algorithms for manifold learning}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.117.511{\&}rep=rep1{\&}type=pdf},
volume = {44},
year = {2005}
}
@article{Chaaraoui2012,
abstract = {Human Behaviour Analysis (HBA) is more and more being of interest for computer vision and artificial intelligence researchers. Its main application areas, like Video Surveillance and Ambient-Assisted Living (AAL), have been in great demand in recent years. This paper provides a review on HBA for AAL and ageing in place purposes focusing specially on vision techniques. First, a clearly defined taxonomy is presented in order to classify the reviewed works, which are consequently presented following a bottom-up abstraction and complexity order. At the motion level, pose and gaze estimation as well as basic human movement recognition are covered. Next, the mainly used action and activity recognition approaches are presented with examples of recent research works. Increasing the degree of semantics and the time interval involved in the HBA, finally the behaviour level is reached. Furthermore, useful tools and datasets are analysed in order to provide help for initiating projects.},
author = {Chaaraoui, Alexandros Andr{\'{e}} and Climent-P{\'{e}}rez, Pau and Fl{\'{o}}rez-Revuelta, Francisco},
doi = {10.1016/j.eswa.2012.03.005},
file = {:home/abetan16/Dropbox/MendeleyV3//Chaaraoui, Climent-P{\'{e}}rez, Fl{\'{o}}rez-Revuelta - 2012 - A review on vision techniques applied to Human Behaviour Analysis for Ambient-Assis.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
month = {sep},
number = {12},
pages = {10873--10888},
title = {{A review on vision techniques applied to Human Behaviour Analysis for Ambient-Assisted Living}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0957417412004757},
volume = {39},
year = {2012}
}
@article{Chapelle2002,
abstract = {The problem of automatically tuning multiple parameters for pattern recognition Support Vector$\backslash$nMachines (SVMs) is considered. This is done by minimizing some estimates of the generalization error of SVMs$\backslash$nusing a gradient descent algorithm over the set of parameters. Usual methods for choosing parameters, based$\backslash$non exhaustive search become intractable as soon as the number of parameters exceeds two. Some experimental$\backslash$nresults assess the feasibility of our approach for a large number of parameters (more than 100) and demonstrate$\backslash$nan improvement of generalization performance.$\backslash$n},
author = {Chapelle, Olivier and Vapnik, Vladimir and Bousquet, Olivier and Mukherjee, Sayan},
doi = {10.1023/A:1012450327387},
file = {:home/abetan16/Dropbox/MendeleyV3/Chapelle et al. - 2002 - Choosing Multiple Parameters for Support Vector Machines.pdf:pdf},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {Feature selection,Gradient descent,Kernel selection,Leave-one-out procedure,Support vector machines},
number = {1-3},
pages = {131--159},
title = {{Choosing multiple parameters for support vector machines}},
url = {http://link.springer.com/article/10.1023/A:1012450327387},
volume = {46},
year = {2002}
}
@inproceedings{Cheatle2004,
abstract = { A system is described for summarizing head-mounted or hand-carried "always-on" video. The example used is a tourist walking around a historic city with friends and family. The summary consists of a mixture of stills, panoramas and video clips. The system identifies both the scenes to appear in the summary and the media type used to represent them. As there are few shot boundaries in this class of video, the decisions are based on the system's classification of the user's behaviour demonstrated by the motion of the camera, and motion in the scene.},
author = {Cheatle, Phil},
booktitle = {Proceedings - International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2004.1333937},
file = {:home/abetan16/Dropbox/MendeleyV3/Cheatle - 2004 - Media Content and Type Selection from Always-on Wearable Video.pdf:pdf},
isbn = {0769521282},
issn = {10514651},
pages = {979--982},
title = {{Media content and type selection from always-on wearable video}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1333937},
volume = {4},
year = {2004}
}
@article{Chelouah2003,
abstract = {A hybrid method combining two algorithms is proposed for the global optimization of multiminima functions. To localize a "promising area", likely to contain a global minimum, it is necessary to well "explore" the whole search domain. When a promising area is detected, the appropriate tools must be used to "exploit" this area and obtain the optimum as accurately and quickly as possible. Both tasks are hardly performed through only one method. We propose an algorithm using two processes, each one devoted to one task. Global metaheuristics, such as simulated annealing, tabu search, and genetic algorithms (GAs) are efficient to localize the "best" areas. On the other hand, local search methods are classically available: in particular the hill climbing (e.g. the quasi-Newton method), and the Nelder-Mead simplex search (SS). Therefore we worked out an hybrid method, called continuous hybrid algorithm (CHA), performing the exploration with a GA, and the exploitation with a Nelder-Mead SS. To evaluate the efficiency of CHA, we implemented a set of benchmark functions, and compared our results to the ones supplied by other competitive methods. ?? 2002 Elsevier Science B.V. All rights reserved.},
author = {Chelouah, Rachid and Siarry, Patrick},
doi = {10.1016/S0377-2217(02)00401-0},
file = {:home/abetan16/Dropbox/MendeleyV3/Chelouah, Siarry - 2003 - Genetic and Nelder-Mead algorithms hybridized for a more accurate global optimization of continuous multiminim.pdf:pdf},
issn = {03772217},
journal = {European Journal of Operational Research},
keywords = {Continuous variables,Genetic algorithm,Global optimization,Multiminima functions,Simplex search},
month = {jul},
number = {2},
pages = {335--348},
title = {{Genetic and Nelder-Mead algorithms hybridized for a more accurate global optimization of continuous multiminima functions}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0377221702004010},
volume = {148},
year = {2003}
}
@article{Chen2003,
abstract = {In this paper, we introduce a hand gesture recognition system to recognize continuous gesture before stationary background. The system consists of four modules: a real time hand tracking and extraction, feature extraction, hidden Markov model (HMM) training, and gesture recognition. First, we apply a real-time hand tracking and extraction algorithm to trace the moving hand and extract the hand region, then we use the Fourier descriptor (FD) to characterize spatial features and the motion analysis to characterize the temporal features. We combine the spatial and temporal features of the input image sequence as our feature vector. After having extracted the feature vectors, we apply HMMs to recognize the input gesture. The gesture to be recognized is separately scored against different HMMs. The model with the highest score indicates the corresponding gesture. In the experiments, we have tested our system to recognize 20 different gestures, and the recognizing rate is above 90{\%}.},
author = {Chen, Feng-Sheng and Fu, Chih-Ming and Huang, Chung-Lin},
doi = {10.1016/S0262-8856(03)00070-2},
file = {:home/abetan16/Dropbox/MendeleyV3/Chen, Fu, Huang - 2003 - Hand Gesture Recognition Using a Real-time Tracking Method and Hidden Markov Models.pdf:pdf},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {hand gesture recognition,hand tracking,hidden markov model},
month = {aug},
number = {8},
pages = {745--758},
title = {{Hand gesture recognition using a real-time tracking method and hidden Markov models}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0262885603000702},
volume = {21},
year = {2003}
}
@article{Chen2006,
abstract = {Multiple-instance problems arise from the situations where training class labels are attached to sets of samples (named bags), instead of individual samples within each bag (called instances). Most previous multiple-instance learning (MIL) algorithms are developed based on the assumption that a bag is positive if and only if at least one of its instances is positive. Although the assumption works well in a drug activity prediction problem, it is rather restrictive for other applications, especially those in the computer vision area. We propose a learning method, MILES (Multiple-Instance Learning via Embedded instance Selection), which converts the multiple-instance learning problem to a standard supervised learning problem that does not impose the assumption relating instance labels to bag labels. MILES maps each bag into a feature space defined by the instances in the training bags via an instance similarity measure. This feature mapping often provides a large number of redundant or irrelevant features. Hence, 1-norm SVM is applied to select important features as well as construct classifiers simultaneously. We have performed extensive experiments. In comparison with other methods, MILES demonstrates competitive classification accuracy, high computation efficiency, and robustness to labeling uncertainty.},
author = {Chen, Yixin and Bi, Jinbo and Wang, James Z.},
doi = {10.1109/TPAMI.2006.248},
file = {:home/abetan16/Dropbox/MendeleyV3/Chen, Bi, Wang - 2006 - MILES Multiple-instance Learning via Embedded Instance Selection.pdf:pdf},
isbn = {0162-8828 VO - 28},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {1-norm support vector machine,Drug activity prediction,Feature subset selection,Image categorization,Multiple-instance learning,Object recognition},
month = {dec},
number = {12},
pages = {1931--1947},
pmid = {17108368},
title = {{MILES: Multiple-instance learning via embedded instance selection}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17108368},
volume = {28},
year = {2006}
}
@incollection{Chiappino2013,
abstract = {Human behavior analysis is one of the most important applications in Intelligent Video Surveillance (IVS) field. In most recent systems addressed by research, automatic support to the human decisions based on object detection, tracking and situation assessment tools is integrated as a part of a complete cognitive artificial process including security maintenance procedures actions that are in the scope of the system. In such cases an IVS needs to represent complex situations that describe alternative possible real time interactions between the dynamic observed situation and operators' actions. To obtain such knowledge, particular types of Event based Dynamic Bayesian Networks E-DBNs are here proposed that can switch among alternative Bayesian filtering and control lower level modules to capture adaptive reactions of human operators. It is shown that after the off line learning phase Switched E-DBNs can be used to represent and anticipate possible operators' actions within the IVS. In this sense acquired knowledge can be used for either fully autonomous security preserving systems or for training of new operators. Results are shown by considering a crowd monitoring application in a critical infrastructure. A system is presented where a Cognitive Node (CN) embedding in a structured way Switched E-DBN knowledge can interact with an active visual simulator of crowd situations. It is also shown that outputs from such a simulator can be easily compared with video signals coming from real cameras and processed by typical Bayesian tracking methods.},
address = {Berlin Heidelberg},
author = {Chiappino, Simone and Marcenaro, Lucio and Morerio, Pietro and Regazzoni, Carlo},
booktitle = {Wide Area Surveillance (Augmented Vision and Reality)},
doi = {10.1007/978-3-642-37841-6},
editor = {Springerlink},
file = {:home/abetan16/Dropbox/Articles/Mendeley/Chiappino et al. - 2013 - Event Based Switched Dynamic Bayesian Networks for Autonomous Cognitive Crowd Monitoring.pdf:pdf},
isbn = {978-3-642-37840-9},
pages = {93--122},
publisher = {Springer Berlin Heidelberg},
title = {{Event Based Switched Dynamic Bayesian Networks for Autonomous Cognitive Crowd Monitoring}},
url = {http://link.springer.com/10.1007/978-3-642-37841-6},
volume = {6},
year = {2014}
}
@article{Chiappino2014b,
abstract = {Cognitive algorithms, integrated in intelligent systems, represent an important innovation in designing interactive smart environments. More in details, Cognitive Systems have important applications in anomaly detection and management in advanced video surveillance. These algorithms mainly address the problem of modelling interactions and behaviours among the main entities in a scene. A bio-inspired structure is here proposed, which is able to encode and synthesize signals, not only for the description of single entities behaviours, but also for modelling cause–effect relationships between user actions and changes in environment configurations. Such models are stored within a memory (Autobiographical Memory) during a learning phase. Here the system operates an effective knowledge transfer from a human operator towards an automatic systems called Cognitive Surveillance Node (CSN), which is part of a complex cognitive JDL-based and bio-inspired architecture. After such a knowledge-transfer phase, learned representations can be used, at different levels, either to support human decisions, by detecting anomalous interaction models and thus compensating for human shortcomings, or, in an automatic decision scenario, to identify anomalous patterns and choose the best strategy to preserve stability of the entire system. Results are presented in a video surveillance scenario , where the CSN can observe two interacting entities consisting in a simulated crowd and a human operator. These can interact within a visual 3D simulator, where crowd behaviour is modelled by means of Social Forces. The way anomalies are detected and consequently handled is demonstrated, on synthetic and also on real video sequences, in both the user-support and automatic modes.},
author = {Chiappino, Simone and Morerio, Pietro and Marcenaro, Lucio and Regazzoni, Carlo S.},
doi = {10.1007/s12652-014-0224-0},
file = {:home/abetan16/Dropbox/MendeleyV3/Chiappino et al. - 2014 - Bio-inspired relevant interaction modelling in cognitive crowd management(2).pdf:pdf},
issn = {1868-5137},
journal = {Journal of Ambient Intelligence and Humanized Computing},
keywords = {anomalous interactions {\'{a}} crowd,cognitive systems {\'{a}} bio-inspired,learning {\'{a}},monitoring {\'{a}} self},
month = {feb},
number = {1},
pages = {1--22},
title = {{Bio-inspired relevant interaction modelling in cognitive crowd management}},
url = {http://link.springer.com/10.1007/s12652-014-0224-0},
volume = {Feb},
year = {2014}
}
@article{Chiappino2014a,
abstract = {Cognitive algorithms, integrated in intelligent systems, represent an important innovation in designing interactive smart environments. More in details, Cognitive Systems have important applications in anomaly detection and management in advanced video surveillance. These algorithms mainly address the problem of modelling interactions and behaviours among the main entities in a scene. A bio-inspired structure is here proposed, which is able to encode and synthesize signals, not only for the description of single entities behaviours, but also for modelling cause–effect relationships between user actions and changes in environment configurations. Such models are stored within a memory (Autobiographical Memory) during a learning phase. Here the system operates an effective knowledge transfer from a human operator towards an automatic systems called Cognitive Surveillance Node (CSN), which is part of a complex cognitive JDL-based and bio-inspired architecture. After such a knowledge-transfer phase, learned representations can be used, at different levels, either to support human decisions, by detecting anomalous interaction models and thus compensating for human shortcomings, or, in an automatic decision scenario, to identify anomalous patterns and choose the best strategy to preserve stability of the entire system. Results are presented in a video surveillance scenario , where the CSN can observe two interacting entities consisting in a simulated crowd and a human operator. These can interact within a visual 3D simulator, where crowd behaviour is modelled by means of Social Forces. The way anomalies are detected and consequently handled is demonstrated, on synthetic and also on real video sequences, in both the user-support and automatic modes.},
author = {Chiappino, Simone and Morerio, Pietro and Marcenaro, Lucio and Regazzoni, Carlo S.},
doi = {10.1007/s12652-014-0224-0},
file = {:home/abetan16/Dropbox/MendeleyV3/Chiappino et al. - 2014 - Bio-inspired relevant interaction modelling in cognitive crowd management.pdf:pdf},
issn = {1868-5137},
journal = {Journal of Ambient Intelligence and Humanized Computing},
keywords = {anomalous interactions {\'{a}} crowd,cognitive systems {\'{a}} bio-inspired,learning {\'{a}},monitoring {\'{a}} self},
month = {feb},
number = {1},
pages = {1--22},
title = {{Bio-inspired relevant interaction modelling in cognitive crowd management}},
url = {http://link.springer.com/10.1007/s12652-014-0224-0},
volume = {Feb},
year = {2014}
}
@inproceedings{Choi2011,
abstract = {In this paper we present a framework for the recognition of collective human activities. A collective activity is defined or reinforced by the existence of coherent behavior of individuals in time and space. We call such coherent behavior {\&}{\#}x2018;Crowd Context{\&}{\#}x2019;. Examples of collective activities are {\&}{\#}x201C;queuing in a line{\&}{\#}x201D; or {\&}{\#}x201C;talking{\&}{\#}x201D;. Following [7], we propose to recognize collective activities using the crowd context and introduce a new scheme for learning it automatically. Our scheme is constructed upon a Random Forest structure which randomly samples variable volume spatio-temporal regions to pick the most discriminating attributes for classification. Unlike previous approaches, our algorithm automatically finds the optimal configuration of spatio-temporal bins, over which to sample the evidence, by randomization. This enables a methodology for modeling crowd context. We employ a 3D Markov Random Field to regularize the classification and localize collective activities in the scene. We demonstrate the flexibility and scalability of the proposed framework in a number of experiments and show that our method outperforms state-of-the art action classification techniques [7, 19].},
address = {Providence, RI},
author = {Choi, W. and Shahid, K. and Savarese, S.},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2011.5995707},
file = {:home/abetan16/Dropbox/MendeleyV3/Choi, Shahid, Savarese - 2011 - Learning Context for Collective Activity Recognition.pdf:pdf},
isbn = {9781457703942},
issn = {10636919},
month = {jun},
pages = {3273--3280},
publisher = {Ieee},
title = {{Learning context for collective activity recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5995707 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5995707},
year = {2011}
}
@inproceedings{Clarkson2000,
abstract = {We describe experiments in recognizing a person's situation from$\backslash$nonly a wearable camera and microphone. The types of situations$\backslash$nconsidered in these experiments are coarse locations (such as at work,$\backslash$nin a subway or in a grocery store) and coarse events (such as in a$\backslash$nconversation or walking down a busy street) that would require only$\backslash$nglobal, non-attentional features to distinguish them},
address = {Atlanta, GA, USA},
author = {Clarkson, B. and Mase, K. and Pentland, a.},
booktitle = {Digest of Papers. Fourth International Symposium on Wearable Computers},
doi = {10.1109/ISWC.2000.888467},
file = {:home/abetan16/Dropbox/MendeleyV3/Clarkson, Mase, Pentland - 2000 - Recognizing User Context Via Wearable Wensors.pdf:pdf},
isbn = {0-7695-0795-6},
issn = {15300811},
keywords = {computer audition,computer vision,contextual computing,hidden,hmm,markov models,peripheral sensing},
pages = {1--7},
publisher = {IEEE Comput. Soc},
title = {{Recognizing user context via wearable sensors}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=888467},
year = {2000}
}
@inproceedings{Clarkson1999,
abstract = {A truly personal and reactive computer system should have access$\backslash$nto the same information as its user, including the ambient sights and$\backslash$nsounds. To this end, we have developed a system for extracting events$\backslash$nand scenes from natural audio/visual input. We find our system can$\backslash$n(without any prior labeling of data) cluster the audio/visual data into$\backslash$nevents, such as passing through doors and crossing the street. Also, we$\backslash$nhierarchically cluster these events into scenes and get clusters that$\backslash$ncorrelate with visiting the supermarket, or walking down a busy street$\backslash$n},
address = {Phoenix, AZ},
author = {Clarkson, B. and Pentland, a.},
booktitle = {1999 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings. ICASSP99 (Cat. No.99CH36258)},
doi = {10.1109/ICASSP.1999.757481},
file = {:home/abetan16/Dropbox/MendeleyV3/Clarkson, Pentland - 1999 - Unsupervised Clustering of Ambulatory Audio and Video.pdf:pdf},
isbn = {0-7803-5041-3},
issn = {1520-6149},
pages = {3037--3040},
publisher = {IEEE},
title = {{Unsupervised clustering of ambulatory audio and video}},
url = {papers://e7d065ae-9998-4287-8af0-c9fa85af8e96/Paper/p36036},
volume = {6},
year = {1999}
}
@article{Conati2002,
abstract = {We present a probabilistic model to monitor a user's emotions and engagement during the interaction with educational games. We illustrate how our probabilistic model assesses affect by integrating evidence on both possible causes of the user's emotional arousal (i.e., the state of the interaction) and its effects (i.e., bodily expressions that are known to be influenced by emotional reactions). The probabilistic model relies on a Dynamic Decision Network to leverage any indirect evidence on the user's emotional state, in order to estimate this state and any other related variable in the model. This is crucial in a modeling task in which the available evidence usually varies with the user and with each particular interaction. The probabilistic model we present is to be used by decision theoretic pedagogical agents to generate interventions aimed at achieving the best tradeoff between a user's learning and engagement during the interaction with educational games.},
author = {Conati, Cristina},
doi = {10.1080/08839510290030390},
file = {:home/abetan16/Dropbox/MendeleyV3/Conati - 2002 - Probabilistic Assessment of User's Emotions in Educational Games.pdf:pdf},
isbn = {0883951029},
issn = {0883-9514},
journal = {Applied Artificial Intelligence},
number = {7-8},
pages = {555--575},
title = {{Probabilistic assessment of user's emotions in educational games}},
url = {http://www.tandfonline.com/doi/abs/10.1080/08839510290030390},
volume = {16},
year = {2002}
}
@article{Cook2013,
abstract = {Individuals with autism spectrum conditions have difficulties in understanding and responding appropriately to others. Additionally, they demonstrate impaired perception of biological motion and problems with motor control. Here we investigated whether individuals with autism move with an atypical kinematic profile, which might help to explain perceptual and motor impairments, and in principle may contribute to some of their higher level social problems. We recorded trajectory, velocity, acceleration and jerk while adult participants with autism and a matched control group conducted horizontal sinusoidal arm movements. Additionally, participants with autism took part in a biological motion perception task in which they classified observed movements as 'natural' or 'unnatural'. Results show that individuals with autism moved with atypical kinematics; they did not minimize jerk to the same extent as the matched typical control group, and moved with greater acceleration and velocity. The degree to which kinematics were atypical was correlated with a bias towards perceiving biological motion as 'unnatural' and with the severity of autism symptoms as measured by the Autism Diagnostic Observation Schedule. We suggest that fundamental differences in movement kinematics in autism might help to explain their problems with motor control. Additionally, developmental experience of their own atypical kinematic profiles may lead to disrupted perception of others' actions.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Cook, Jennifer L. and Blakemore, Sarah Jayne and Press, Clare},
doi = {10.1093/brain/awt208},
eprint = {9605103},
file = {:home/abetan16/Dropbox/MendeleyV3/Cook, Blakemore, Press - 2013 - Atypical basic movement kinematics in autism spectrum conditions.pdf:pdf},
isbn = {1460-2156 (Electronic)$\backslash$r0006-8950 (Linking)},
issn = {14602156},
journal = {Brain},
keywords = {autism,biological motion,kinematics,motor control},
number = {9},
pages = {2816--2824},
pmid = {23983031},
primaryClass = {cs},
title = {{Atypical basic movement kinematics in autism spectrum conditions}},
volume = {136},
year = {2013}
}
@article{Couprie2013,
abstract = {Numerous approaches in image processing and computer vision are making use of super-pixels as a pre-processing step. Among the different methods producing such over-segmentation of an image, the graph-based approach of Felzenszwalb and Huttenlocher is broadly employed. One of its interesting properties is that the regions are computed in a greedy manner in quasi-linear time. The algorithm may be trivially extended to video segmentation by considering a video as a 3D volume, however, this can not be the case for causal segmentation, when subsequent frames are unknown. We propose an efficient video segmentation approach that computes temporally consistent pixels in a causal manner, filling the need for causal and real time applications.},
archivePrefix = {arXiv},
arxivId = {1301.1671},
author = {Couprie, Camille and Farabet, Cl{\'{e}}ment and LeCun, Yann},
doi = {10.1109/ICIP.2013.6738875},
eprint = {1301.1671},
file = {:home/abetan16/Dropbox/MendeleyV3/Couprie, Farabet, LeCun - 2013 - Causal graph-based video segmentation.pdf:pdf},
isbn = {9781479923410},
title = {{Causal graph-based video segmentation}},
url = {http://arxiv.org/abs/1301.1671},
year = {2013}
}
@misc{Curie2016,
annote = {Check the total budget and what it implies in terms of candidates and stuff.

Check if the industry is interested or sells something.


Create 3 lines, one per problem, environment, user, and routines. interaction models, how the individual interacts with the enviroment and learn this models. (Machine Learning)

How one of the two entities should modify the state, for example with training step, deviating from normality (ill people, how they are deviating from normality, and how to push them back)

Understand normality
When
How to modify

Online learning.
Deep learning to understand where are the users.

Transfer knowledge and coeficients of impacts.

Deep learning to predict effects of a new message.

If the enviroment assumes a given state and the message is this which is the probability that he will change its behaviour},
author = {Curie, Marie},
file = {:home/abetan16/Dropbox/MendeleyV3/Curie - 2016 - Marie Curie DRAFT 2016-2017.pdf:pdf},
number = {September 2015},
title = {{Marie Curie DRAFT 2016-2017}},
year = {2016}
}
@inproceedings{Dabcevic2014,
abstract = {On-the-fly reconfigurability capabilities and learning prospectives of Cognitive Radios inherently bring a set of new security issues. One of them is intelligent radio frequency jamming, where adversary is able to deploy advanced jamming strategies to degrade performance of the communication system. In this paper, we observe the jamming/antijamming problem from a game-theoretical perspective. A game with incomplete information on opponent's payoff and strategy is modelled as a Markov Decision Process (MDP). A variant of fictitious play learning algorithm is deployed to find optimal strategies in terms of combination of channel hopping and power alteration anti-jamming schemes.},
author = {Dabcevic, K and Betancourt, A and Marcenaro, L and Regazzoni, C.S.},
booktitle = {International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2014.6855191},
file = {:home/abetan16/Dropbox/MendeleyV3/Dabcevic et al. - 2014 - A Fictitious Play-based Game-theoretical Approach to Alleviating Jamming Attacks for Cognitive Radios.pdf:pdf},
isbn = {978-1-4799-2893-4},
issn = {15206149},
keywords = {adaptive,anti-jamming,fictitious play,frequency hopping,game theory,jamming,markov models,non-zero-sum game,play,power allocation,smart transmission layer,stochastic game},
month = {may},
pages = {8158--8162},
publisher = {IEEE Signal Processing},
title = {{A Fictitious Play-based Game-theoretical Approach to Alleviating Jamming Attacks for Cognitive Radios}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6855191},
year = {2014}
}
@article{Dabcevic2014a,
abstract = {Cognitive radio (CR) promises to be a solution for the spectrum underutilization problems. However, security issues pertaining to cognitive radio technology are still an understudied topic. One of the prevailing such issues are intelligent radio frequency (RF) jamming attacks, where adversaries are able to exploit on-the-fly reconfigurability potentials and learning mechanisms of cognitive radios in order to devise and deploy advanced jamming tactics. In this paper, we use a game-theoretical approach to analyze jamming/anti-jamming behavior between cognitive radio systems. A non-zero-sum game with incomplete information on an opponent's strategy and payoff is modelled as an extension of Markov decision process (MDP). Learning algorithms based on adaptive payoff play and fictitious play are considered. A combination of frequency hopping and power alteration is deployed as an anti-jamming scheme. A real-life software-defined radio (SDR) platform is used in order to perform measurements useful for quantifying the jamming impacts, as well as to infer relevant hardware-related properties. Results of these measurements are then used as parameters for the modelled jamming/anti-jamming game and are compared to the Nash equilibrium of the game. Simulation results indicate, among other, the benefit provided to the jammer when it is employed with the spectrum sensing algorithm in proactive frequency hopping and power alteration schemes.},
author = {Dabcevic, K and Betancourt, A and Marcenaro, L and Regazzoni, C.S.},
doi = {10.1186/1687-6180-2014-171},
file = {:home/abetan16/Dropbox/MendeleyV3/Dabcevic et al. - 2014 - Intelligent cognitive radio jamming-a game-theoretical approach.pdf:pdf},
issn = {16876180},
journal = {EURASIP Journal on Advances in Signal Processing},
keywords = {adaptive,anti-jamming,fictitious play,frequency hopping,game theory,jamming,markov models,non-zero-sum game,play,power allocation,smart transmission layer,stochastic game},
pages = {1--18},
title = {{Intelligent cognitive radio jamming-a game-theoretical approach}},
url = {http://asp.eurasipjournals.com/content/2014/1/171/abstract},
volume = {2014},
year = {2014}
}
@article{Dael2013,
abstract = {Recent judgment studies have shown that people are able to fairly correctly attribute emotional states to others' bodily expressions. It is, however, not clear which movement qualities are salient, and how this applies to emotional gesture during speech-based interaction. In this study we investigated how the expression of emotions that vary on three major emotion dimensions—that is, arousal, valence, and potency—affects the perception of dynamic arm gestures. Ten professional actors enacted 12 emotions in a scenario-based social interaction setting. Participants (N = 43) rated all emotional expressions with muted sound and blurred faces on six spatiotemporal characteristics of gestural arm movement that were found to be related to emotion in previous research (amount of movement, movement speed, force, fluency, size, and height/vertical position). Arousal and potency were found to be strong determinants of the perception of gestural dynamics, whereas the differences between positive or negative emotions were less pronounced. These results confirm the importance of arm movement in communicating major emotion dimensions and show that gesture forms an integrated part of multimodal nonverbal emotion communication.},
author = {Dael, Nele and Goudbeek, Martijn and Scherer, K R},
doi = {10.1068/p7364},
file = {:home/abetan16/Dropbox/MendeleyV3/Dael, Goudbeek, Scherer - 2013 - Perceived gesture dynamics in nonverbal expression of{\&}{\#}xA0emotion.pdf:pdf},
issn = {0301-0066},
journal = {Perception},
keywords = {dynamics,emotion dimensions,emotion expression,gesture,movement perception},
number = {6},
pages = {642--657},
title = {{Perceived gesture dynamics in nonverbal expression of{\&}{\#}xA0;emotion}},
url = {http://pec.sagepub.com/lookup/doi/10.1068/p7364},
volume = {42},
year = {2013}
}
@article{Dael2012,
abstract = {Emotion communication research strongly focuses on the face and voice as expressive modalities, leaving the rest of the body relatively understudied. Contrary to the early assumption that body movement only indicates emotional intensity, recent studies have shown that body movement and posture also conveys emotion specific information. However, a deeper understanding of the underlying mechanisms is hampered by a lack of production studies informed by a theoretical framework. In this research we adopted the Body Action and Posture (BAP) coding system to examine the types and patterns of body movement that are employed by 10 professional actors to portray a set of 12 emotions. We investigated to what extent these expression patterns support explicit or implicit predictions from basic emotion theory, bidimensional theory, and componential appraisal theory. The overall results showed partial support for the different theoretical approaches. They revealed that several patterns of body movement systematically occur in portrayals of specific emotions, allowing emotion differentiation. Although a few emotions were prototypically expressed by one particular pattern, most emotions were variably expressed by multiple patterns, many of which can be explained as reflecting functional components of emotion such as modes of appraisal and action readiness. It is concluded that further work in this largely underdeveloped area should be guided by an appropriate theoretical framework to allow a more systematic design of experiments and clear hypothesis testing.},
author = {Dael, Nele and Mortillaro, Marcello and Scherer, Klaus R.},
doi = {10.1037/a0025737},
file = {:home/abetan16/Dropbox/MendeleyV3/Dael, Mortillaro, Scherer - 2012 - Emotion Expression in Body Action and Posture.pdf:pdf},
isbn = {1528-3542},
issn = {1528-3542},
journal = {Emotion},
keywords = {10,1037,a0025737,appraisal,doi,dx,emotion,expression,gesture,http,org,posture,supp,supplemental materials},
month = {oct},
number = {5},
pages = {1085--1101},
pmid = {22059517},
title = {{Emotion expression in body action and posture.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22059517},
volume = {12},
year = {2012}
}
@inproceedings{Dalal2005,
abstract = {We study the question of feature sets for robust visual ob- ject recognition, adopting linear SVM based human detec- tion as a test case. After reviewing existing edge and gra- dient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors sig- nificantly outperform existing feature sets for human detec- tion. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping de- scriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds. 1},
author = {Dalal, N. and Triggs, B.},
booktitle = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
doi = {10.1109/CVPR.2005.177},
file = {:home/abetan16/Dropbox/MendeleyV3/Dalal, Triggs - 2005 - Histograms of Oriented Gradients for Human Detection.pdf:pdf},
isbn = {0-7695-2372-2},
pages = {886--893},
publisher = {Ieee},
title = {{Histograms of Oriented Gradients for Human Detection}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1467360},
volume = {1},
year = {2005}
}
@inproceedings{Damen2012,
abstract = {We describe an integrated system for personal workspace monitoring based around an RGB-D sensor. The approach is egocentric, facilitating full flexibility, and operates in real-time, providing object detection and recognition, and 3D trajectory estimation whilst the user undertakes tasks in the workspace. A prototype on-body system developed in the context of work-flow analysis for industrial manipulation and assembly tasks is described. The system is evaluated on two tasks with multiple users, and results indicate that the method is effective, giving good accuracy performance.},
author = {Damen, Dima and Gee, Andrew and Mayol-Cuevas, Walterio and Calway, Andrew},
booktitle = {Iros},
doi = {10.1109/IROS.2012.6385829},
file = {:home/abetan16/Dropbox/MendeleyV3/Damen et al. - 2012 - Egocentric Real-time Workspace Monitoring using an RGB-D camera.pdf:pdf},
isbn = {9781467317375},
issn = {21530858},
month = {oct},
pages = {1029--1036},
publisher = {IEEE},
title = {{Egocentric Real-time Workspace Monitoring using an RGB-D camera}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6385829},
year = {2012}
}
@inproceedings{Damen2014,
abstract = {We present an online fully unsupervised approach for auto- matically extracting video guides of how objects are used from wearable gaze trackers worn bymultiple users. Given egocentric video and eye gaze from multiple users performing tasks, the system discovers task-relevant objects and automatically extracts guidance videos on how these objects have been used. In the assistive mode, the paper proposes a method for selecting a suitable video guide to be displayed to a novice user indi- cating how to use an object, purely triggered by the user's gaze. The approach is tested on a variety of daily tasks ranging from opening a door, to preparing coffee and operating a gym machine.},
annote = {Activity recognition
- Gaze movement
gaze trackers
-Object Detection
-edges
- 2D mapping
SLAM},
author = {Damen, Dima and Haines, Osian and Leelasawassuk, Teesid},
booktitle = {ECCV Workshop on Assistive Computer Vision and Robotics (ACVR)},
doi = {10.1007/978-3-319-16199-0},
file = {:home/abetan16/Dropbox/MendeleyV3/Damen, Haines, Leelasawassuk - 2014 - Multi-User Egocentric Online System for Unsupervised Assistance on Object Usage.pdf:pdf},
isbn = {978-3-319-16198-3},
keywords = {assistive computing,object discovery,object usage,real-time computer,video guidance,vision,wearable computing},
title = {{Multi-User Egocentric Online System for Unsupervised Assistance on Object Usage}},
url = {http://www.cs.bris.ac.uk/publications/Papers/2001710.pdf},
year = {2014}
}
@article{Damerau1964,
abstract = {The method described assumes that a word which cannot be found in a dictionary has at most one error, which might be a wrong, missing or extra letter or a single transposition. The unidentified input word is compared to the dictionary again, testing each time to see if the words match—assuming one of these errors occurred. During a test run on garbled text, correct identifications were made for over 95 percent of these error types.},
author = {Damerau, Fred J.},
doi = {10.1145/363958.363994},
isbn = {0000000000},
issn = {00010782},
journal = {Communications of the ACM},
month = {mar},
number = {3},
pages = {171--176},
title = {{A technique for computer detection and correction of spelling errors}},
url = {http://portal.acm.org/citation.cfm?doid=363958.363994},
volume = {7},
year = {1964}
}
@article{Davison2007,
abstract = {We present a real-time algorithm which can recover the 3D trajectory of a monocular camera, moving rapidly through a previously unknown scene. Our system, which we dub MonoSLAM, is the first successful application of the SLAM methodology from mobile robotics to the "pure vision" domain of a single uncontrolled camera, achieving real time but drift-free performance inaccessible to Structure from Motion approaches. The core of the approach is the online creation of a sparse but persistent map of natural landmarks within a probabilistic framework. Our key novel contributions include an active approach to mapping and measurement, the use of a general motion model for smooth camera movement, and solutions for monocular feature initialization and feature orientation estimation. Together, these add up to an extremely efficient and robust algorithm which runs at 30 Hz with standard PC and camera hardware. This work extends the range of robotic systems in which SLAM can be usefully applied, but also opens up new areas. We present applications of MonoSLAM to real-time 3D localization and mapping for a high-performance full-size humanoid robot and live augmented reality with a hand-held camera.},
archivePrefix = {arXiv},
arxivId = {there is not},
author = {Davison, Andrew J. and Reid, Ian D. and Molton, Nicholas D. and Stasse, Olivier},
doi = {10.1109/TPAMI.2007.1049},
eprint = {there is not},
file = {:home/abetan16/Dropbox/MendeleyV3/Davison et al. - 2007 - MonoSLAM Real-time Single Camera SLAM.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {3D/stereo scene analysis,Autonomous vehicles,Tracking},
month = {jun},
number = {6},
pages = {1052--1067},
pmid = {17431302},
title = {{MonoSLAM: Real-time single camera SLAM}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17431302},
volume = {29},
year = {2007}
}
@article{DeBoer2013,
abstract = {Quantification of eye-hand coordinated behaviour is a relatively new tool to study neurodegeneration in humans. Its sensitivity depends on the assessment of different behavioural strategies, multiple task testing and repeating tasks within one session. However, large numbers of repetition trials pose a significant burden on subjects. To introduce this method in large-scale population studies, it is necessary to determine whether reducing the number of task repetitions, which will lower subject burden, still leads to acceptable measurement accuracy. The objective of this study was to investigate the validity and reliability of eye-hand coordination outcome parameters in eight healthy volunteers using a test-retest approach. Subjects were assessed during a shortened test procedure consisting of eight repetitions of three behavioural tasks: a reflex-based tapping task, a planning-based tapping task and a memory-based tapping task. Eye-hand coordination was quantified in terms of timing (eye and hand latencies), kinematics and accuracy. Eye and hand latencies were found within a normal range (between 150 and 450. ms). A paired samples t-test revealed no differences in timing parameters between the first and second measurements.It was concluded that eight trial repetitions are sufficient for quantifying eye-hand coordination in terms of timing, kinematics and accuracy. This approach demonstrates the testing of multiple visuomotor behaviours within a reasonable time span of a few minutes per task. ?? 2013 Elsevier B.V.},
author = {{De Boer}, C. and {Van der Steen}, J. and Schol, R. J. and Pel, J. J M},
doi = {10.1016/j.jneumeth.2013.05.011},
file = {:home/abetan16/Dropbox/MendeleyV3/De Boer et al. - 2013 - Repeatability of the timing of eye-hand coordinated movements across different cognitive tasks.pdf:pdf},
isbn = {0165-0270},
issn = {01650270},
journal = {Journal of Neuroscience Methods},
keywords = {Eye-hand coordination,Parietal cortex,Repeatability,Timing,Visuomotor integration},
month = {aug},
number = {1},
pages = {131--138},
pmid = {23732535},
publisher = {Elsevier B.V.},
title = {{Repeatability of the timing of eye-hand coordinated movements across different cognitive tasks}},
url = {http://dx.doi.org/10.1016/j.jneumeth.2013.05.011},
volume = {218},
year = {2013}
}
@incollection{Schiele1999,
abstract = {In this paper, we present a vision-based system that estimates the pose of users as well as the gestures they perform in real$\backslash$n  time. This system allow users to interact naturally with an application (virtual reality, gaming) or a robot.$\backslash$n  $\backslash$n  The main components of our system are a 3D upper-body tracker, which estimates human body pose in real-time from a stereo$\backslash$n  sensor and a gesture recognizer, which classifies output from temporal tracker into gesture classes. The main novelty of our$\backslash$n  system is the bag-of-features representation for temporal sequences. This representation, though simple, proves to be surprisingly$\backslash$n  powerful and able to implicitly learn sequence dynamics. Based on this representation, a multi-class classifier, treating$\backslash$n  the bag of features as the feature vector is applied to estimate the corresponding gesture class.$\backslash$n  $\backslash$n  $\backslash$n  $\backslash$n  We show with experiments performed on a HCI gesture dataset that our method performs better than state-of-the-art algorithms$\backslash$n  and has some nice generalization properties. Finally, we describe virtual and real world applications, in which our system$\backslash$n  was integrated for multimodal interaction.},
address = {Berlin, Heidelberg},
author = {Demirdjian, David and Varri, Chenna},
booktitle = {Computer Vision Systems},
doi = {10.1007/978-3-642-04667-4},
file = {:home/abetan16/Dropbox/MendeleyV3/Demirdjian, Varri - 2009 - Computer Vision Systems.pdf:pdf},
isbn = {978-3-642-04666-7},
issn = {0932-8092},
keywords = {peg-in-hole insertion,pose estimation,tool tip detection},
month = {sep},
pages = {1--10},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Computer Vision Systems}},
url = {http://www.springerlink.com/content/r70g5626t36w61h4},
volume = {5815},
year = {2009}
}
@inproceedings{DeVaul2003,
abstract = {Wearables are frequently designed to support users en- gaged in complex “real world” activities, ranging from food inspection to ground combat. Unfortunately, wearables also have the potential to interfere with the very tasks they are designed to support, either by distracting the user or providing them with misleading information. In 2002 we published a pilot study suggesting that a subliminal visual cuing system might be an effective low- attention interaction strategy for just-in-time memory sup- port. In this paper we present the results of a larger study demonstrating that not only is wearable subliminal cuing significantly effective (increasing performance by a factor of approximately 1.5, p = 0.02), but even incorrect sublim- inal cues can actually improve performance. By contrast, consciously-visible incorrect cues caused performance to degrade.},
author = {DeVaul, R.W. and Pentland, a. and Corey, V.R.},
booktitle = {Seventh IEEE International Symposium on Wearable Computers, 2003. Proceedings.},
doi = {10.1109/ISWC.2003.1241404},
file = {:home/abetan16/Dropbox/MendeleyV3/DeVaul, Pentland, Corey - 2003 - The Memory Glasses Subliminal Vs. Overt Memory Support with Imperfect Information.pdf:pdf},
isbn = {0-7695-2034-0},
issn = {1530-0811},
pages = {146--153},
publisher = {Ieee},
title = {{The memory glasses: subliminal vs. overt memory support with imperfect information}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1241404},
year = {2003}
}
@inproceedings{MichaelS.DevyverAkihiroTsukada2011,
abstract = {We have developed, built and tested a wearable device made to monitor, record and assist people in their daily lives, also known as First Person Vision device. It consists of a scene camera and a non-active lighting eye camera as well as audio and movements sensors. It is built to be worn on any type of eyeglasses and optimized for shape, size and weight. The resulting data are recorded on-board or transmitted to an external computer for further processing. Some images are captured and used successfully in vision algorithms. They show how such a product is useful to improve the quality of life of persons with disabilities.},
author = {Devyver, M and Tsukada, a and Kanade, T},
booktitle = {3rd International Symposium on Quality of Life {\ldots}},
file = {:home/abetan16/Dropbox/MendeleyV3//Devyver, Tsukada, Kanade - 2011 - A Wearable Device for First Person Vision.pdf:pdf},
number = {1},
pages = {1--6},
title = {{A wearable device for first person vision}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:A+Wearable+Device+for+First+Person+Vision{\#}7},
volume = {Jul},
year = {2011}
}
@article{DiazRodriguez2014,
author = {{D{\'{i}}az Rodr{\'{i}}guez}, Natalia and Cu{\'{e}}llar, Manuel P. and Lilius, Johan and {Delgado Calvo-Flores}, Miguel},
doi = {10.1016/j.knosys.2014.04.016},
file = {:home/abetan16/Dropbox/MendeleyV3/D{\'{i}}az Rodr{\'{i}}guez et al. - 2014 - A fuzzy ontology for semantic modelling and recognition of human behaviour.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
month = {aug},
pages = {46--60},
title = {{A fuzzy ontology for semantic modelling and recognition of human behaviour}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0950705114001385},
volume = {66},
year = {2014}
}
@article{Diaz-Rodriguez2014,
abstract = {Human activity recognition is a key task in ambient intelligence applications to achieve proper ambient assisted living. There has been remarkable progress in this domain, but some challenges still remain to obtain robust methods. Our goal in this work is to provide a system that allows the modeling and recognition of a set of complex activities in real life scenarios involving interaction with the environment. The proposed framework is a hybrid model that comprises two main modules: a low level sub-activity recognizer, based on data-driven methods, and a high-level activity recognizer, implemented with a fuzzy ontology to include the semantic interpretation of actions performed by users. The fuzzy ontology is fed by the sub-activities recognized by the low level data-driven component and provides fuzzy ontological reasoning to recognize both the activities and their influence in the environment with semantics. An additional benefit of the approach is the ability to handle vagueness and uncertainty in the knowledge-based module, which substantially outperforms the treatment of incomplete and/or imprecise data with respect to classic crisp ontologies. We validate these advantages with the public CAD-120 dataset (Cornell Activity Dataset), achieving an accuracy of 90.1{\%} and 91.07{\%} for low-level and Sensors 2014, 14 18132 high-level activities, respectively. This entails an improvement over fully data-driven or ontology-based approaches.},
author = {D{\'{i}}az-Rodr{\'{i}}guez, Natalia and Cadah{\'{i}}a, Olmo and Cu{\'{e}}llar, Manuel and Lilius, Johan and Calvo-Flores, Miguel},
doi = {10.3390/s141018131},
file = {:home/abetan16/Dropbox/MendeleyV3/D{\'{i}}az-Rodr{\'{i}}guez et al. - 2014 - Handling real-world context awareness, uncertainty and vagueness in real-time human activity tracking a.pdf:pdf},
issn = {1424-8220},
journal = {Sensors},
keywords = {"3D depth sensors,activity recognition,ambient intelligence,context awareness,fuzzy ontology,hybrid systems ",semantic web,uncertainty,vagueness},
month = {jan},
number = {10},
pages = {18131--18171},
pmid = {25268914},
title = {{Handling Real-World Context Awareness, Uncertainty and Vagueness in Real-Time Human Activity Tracking and Recognition with a Fuzzy Ontology-Based Hybrid Method}},
url = {http://www.mdpi.com/1424-8220/14/10/18131/},
volume = {14},
year = {2014}
}
@inproceedings{Ding2014,
abstract = {We propose a novel approach for designing kernels for sup- port vectormachines (SVMs)when the class label is linked to the observation through a latent state and the likelihood func- tion of the observation given the state (the sensing model) is available. We show that the Bayes-optimum decision bound- ary is a hyperplane under amapping defined by the likelihood function. Combining thiswith themaximummargin principle yields kernels for SVMs that leverage knowledge of the sens- ing model in an optimal way. We derive the optimum kernel for the bag-of-words (BoWs) sensing model and demonstrate its superior performance over other kernels in document and image classification tasks. These results indicate that such optimum sensing-aware kernel SVMs can match the perfor- mance of rather sophisticated state-of-the-art approaches.},
address = {Florence, Itaty},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.0512v2},
author = {Ding, W and Ishwar, P and Saligrama, V and Karl, WC},
booktitle = {International Conference on Acoustic, Speech and Signal Processing},
eprint = {arXiv:1312.0512v2},
file = {:home/abetan16/Dropbox/MendeleyV3/Ding et al. - 2014 - Sensing-Aware Kernel SVM.pdf:pdf},
isbn = {9550101045},
number = {1},
pages = {2971--2975},
publisher = {IEEE Signal Processing},
title = {{Sensing-Aware Kernel SVM}},
url = {http://arxiv.org/abs/1312.0512},
volume = {1},
year = {2014}
}
@article{Dixon,
abstract = {Expressive performance of traditional Western music is a complex phenomenon which is mastered by few, and yet appreciated by many. In this paper we explore various ways of interacting with expressive performances using methods that are accessible to non-expert music-lovers. A digital theremin is used as an input device, and users can control the two most important expressive parame- ters, tempo and loudness, during playback of an audio or MIDI file. Several modes of operation are possible: the Air Worm builds on previous work in performance vi- sualisation, where the tempo is displayed on the horizontal axis and loudness on the vertical axis in a two-dimensional animation; the Air Tapper uses a conducting metaphor where the beat is given by the minimum vertical point in a quasi-periodic trajectory; and the Mouse-Worm allows users without a theremin to use a standard input device as controller.},
author = {Dixon, Simon and Goebl, Werner and Widmer, Gerhard},
doi = {10.1.1.1.3452},
file = {:home/abetan16/Dropbox/MendeleyV3/Dixon, Goebl, Widmer - 2005 - the “ Air Worm ” an Interface for Real-Time Manipulation of Expressive Music Performance.pdf:pdf},
journal = {Journal of New Music Research},
pages = {1--4},
title = {{the “ Air Worm ”: an Interface for Real-Time Manipulation of Expressive Music Performance}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:The+?air+worm?:+an+interface+for+real-time+manipulation+of+expressive+music+performance{\#}0},
year = {2005}
}
@article{Doherty2011,
abstract = {Lifelogging is the process of automatically recording aspects of one's life in digital form. This includes visual lifelogging using wearable cameras such as the SenseCam and in recent years many interesting applications for this have emerged and are being actively researched. One of the most interesting of these, and possibly the most far-reaching, is using visual lifelogs as a memory prosthesis but there are also applications in job-specific activity recording, general lifestyle analysis and market analysis. In this work we describe a technique which allowed us to develop automatic classifiers for visual lifelogs to infer different lifestyle traits or characteristics. Their accuracy was validated on a set of 95 k manually annotated images and through one-on-one interviews with those who gathered the images. These automatic classifiers were then applied to a collection of over 3 million lifelog images collected by 33 individuals sporadically over a period of 3.5 years. From this collection we present a number of anecdotal observations to demonstrate the future potential of lifelogging to capture human behaviour. These anecdotes include: the eating habits of office workers; to the amount of time researchers spend outdoors through the year; to the observation that retired people in our study appear to spend quite a bit of time indoors eating with friends. We believe this work demonstrates the potential of lifelogging techniques to assist behavioural scientists in future. {\textcopyright} 2011 Elsevier Ltd. All rights reserved.},
author = {Doherty, Aiden R. and Caprani, Niamh and Conaire, Ciar{\'{a}}n {\'{O}} and Kalnikaite, Vaiva and Gurrin, Cathal and Smeaton, Alan F. and O'Connor, Noel E. and O'Connor, Noel E.},
doi = {10.1016/j.chb.2011.05.002},
file = {:home/abetan16/Dropbox/MendeleyV3/Doherty, Caprani - 2011 - Passively Recognising Human Activities Through Lifelogging.pdf:pdf},
isbn = {0747-5632},
issn = {07475632},
journal = {Computers in Human Behavior},
keywords = {Algorithms,Lifelogging,Psychology,SenseCam,Sociology},
number = {5},
pages = {1948--1958},
title = {{Passively recognising human activities through lifelogging}},
url = {http://www.sciencedirect.com/science/article/pii/S0747563211000963},
volume = {27},
year = {2011}
}
@inproceedings{Doherty2008,
abstract = {The SenseCam is a passive capture wearable camera and when worn continuously it takes an average of 1,900 images per day. It can be used to create a personal lifelog or visual recording of a wearer's life which can be helpful as an aid to human memory. For such a large amount of visual information to be useful, it needs to be structured into "events", which can be achieved through automatic segmentation. An important component of this structuring process is the selection of keyframes to represent individual events. This work investigates a variety of techniques for the selection of a single representative keyframe image from each event, in order to provide the user with an instant visual summary of that event. In our experiments we use a large test set of 2,232 lifelog events collected by 5 users over a time period of one month each. We propose a novel keyframe selection technique which seeks to select the image with the highest "quality" as the keyframe. The inclusion of "quality" approaches in keyframe selection is demonstrated to be useful owing to the high variability in image visual quality within passively captured image collections.},
address = {New York, New York, USA},
author = {Doherty, Aiden Roger and Byrne, Daragh and Smeaton, Alan F. and Jones, Gareth J. F. and Hughes, Mark},
booktitle = {International conference on Content-based image and video retrieval CIVR},
doi = {10.1145/1386352.1386389},
file = {:home/abetan16/Dropbox/MendeleyV3/Doherty et al. - 2008 - Investigating Keyframe Selection Methods in the Novel Domain of Passively Captured Visual Lifelogs.pdf:pdf},
isbn = {9781605580708},
pages = {259--268},
publisher = {ACM Press},
title = {{Investigating keyframe selection methods in the novel domain of passively captured visual lifelogs}},
url = {http://portal.acm.org/citation.cfm?doid=1386352.1386389},
year = {2008}
}
@article{Doherty2013,
abstract = {The relationships between lifestyle behaviors and health outcomes usually are based on selfreported data. Such data are prone to measurement error. In response, there has been a movement toward objective forms of measurement that have low participant and researcher burden. The papers in this theme issue in the American Journal of Preventive Medicine assess the utility of a new form of objective measurement in health research, namely wearable cameras. These devices can be worn all day and automatically record images from a fırst-person point of view, requiring no intervention or attention from the subject or the researcher. The most mature visual lifelogging device is Microsofts SenseCam, a wearable camera worn via a lanyard around the neck. The SenseCam has been increasingly used in healthrelated research for several years. These theme papers report current research into wearable cameras in health, as presented at the SenseCam 2012 Symposium. Wearable cameras and their associated software analysis tools have developed to the point that they now appear well suited to measure sedentary behavior, active travel, and nutrition-related behaviors. Individuals may recall events more accurately after reviewing images from their wearable cameras. Aspects of their immediate cognitive functioning may also improve. Despite the benefıts of wearable cameras, there are still challenges remaining before their use becomes widespread. Ethical and privacy concerns are important issues that need to be addressed, as well as easy access to devices. In response, an ethical framework and smartphone-based wearable camera capture platform are proposed.5 In sum, this body of work suggests that the use of wearable cameras will soon be appropriate to understand lifestyle behaviors and the context in which the occur.},
author = {Doherty, Aiden Roger and Hodges, Steve E and King, Abby C and Smeaton, Alan F. and Berry, Emma and Moulin, Chris J A and Lindley, Si{\^{a}}n E. and Kelly, Paul and Foster, Charlie},
doi = {10.1016/j.amepre.2012.11.008},
file = {:home/abetan16/Dropbox/MendeleyV3/Doherty et al. - 2013 - Wearable cameras in health The state of the art and future possibilities.pdf:pdf},
issn = {1873-2607},
journal = {American Journal of Preventive Medicine},
keywords = {active travel,health,nutrition,sedentary behaviour,wearable cameras},
number = {3},
pages = {320--323},
pmid = {23415132},
title = {{Wearable cameras in health: The state of the art and future possibilities}},
url = {http://www.ajpmonline.org/webfiles/images/journals/amepre/AMEPRE{\_}3675-stamped.pdf},
volume = {44},
year = {2013}
}
@article{Doherty2013a,
abstract = {Traditionally, health researchers have used large-scale travel surveys to measure existing travel behavior and identify the determinants driving it. However, such surveys rely on self-reporting, which can be unreliable. Here, the authors discuss using wearable cameras that capture first-person point-of-view images to help objectively identify the duration, frequency, and mode of journeys and reveal potential errors inherent in self-reporting. Their approach could ultimately lead to a better understanding of the environments offering individuals opportunities to engage in more active forms of transportation. This column is part of a special issue on transit and transport.},
author = {Doherty, Aiden and Kelly, Paul and Foster, Charlie},
doi = {10.1109/MPRV.2013.21},
file = {:home/abetan16/Dropbox/MendeleyV3/Doherty, Kelly, Foster - 2013 - Wearable cameras Identifying healthy transportation choices.pdf:pdf},
issn = {15361268},
journal = {IEEE Pervasive Computing},
keywords = {SenseCam,transportation,wearable cameras,wearable computing},
pages = {44--47},
title = {{Wearable cameras: Identifying healthy transportation choices}},
volume = {12},
year = {2013}
}
@inproceedings{Dollar2009,
abstract = {Pedestrian detection is a key problem in computer vision, with several applications including robotics, surveillance and automotive safety. Much of the progress of the past few years has been driven by the availability of challenging public datasets. To continue the rapid rate of innovation, we introduce the Caltech Pedestrian Dataset, which is two orders of magnitude larger than existing datasets. The dataset contains richly annotated video, recorded from a moving vehicle, with challenging images of low resolution and frequently occluded people. We propose improved evaluation metrics, demonstrating that commonly used per-window measures are flawed and can fail to predict performance on full images. We also benchmark several promising detection systems, providing an overview of state-of-the-art performance and a direct, unbiased comparison of existing methods. Finally, by analyzing common failure cases, we help identify future research directions for the field.},
address = {Miami, FL},
author = {Doll{\'{a}}r, Piotr and Wojek, Christian and Schiele, Bernt and Perona, Pietro and Darmstadt, Tu},
booktitle = {In CVPR},
file = {:home/abetan16/Dropbox/MendeleyV3/Doll{\'{a}}r, Wojek - 2009 - Pedestrian Detection a Benchmark.pdf:pdf},
isbn = {9781424439911},
pages = {304--311},
publisher = {IEEE},
title = {{Pedestrian detection: A benchmark}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5206631},
year = {2009}
}
@inproceedings{Domingos1995,
abstract = {This paper presents a uni ed bias-variance decomposition that is applicable to squared loss, zero-one loss, variable misclassi cation costs, and other loss functions. The uni ed decomposition sheds light on a number of sig- ni cant issues: the relation between some of the previously-proposed decompositions for zero-one loss and the original one for squared loss, the relation between bias, variance and Schapire et al.'s (1997) notion of margin, and the nature of the trade-o between bias and variance in classi cation. While the bias- variance behavior of zero-one loss and vari- able misclassi cation costs is quite di erent from that of squared loss, this di erence de- rives directly from the di erent de nitions of loss. We have applied the proposed decom- position to decision tree learning, instance- based learning and boosting on a large suite of benchmark data sets, and made several sig- ni cant observations.},
address = {Stanford CA},
author = {Domingos, Pedro},
booktitle = {Icml},
file = {:home/abetan16/Dropbox/MendeleyV3/Domingos - 2000 - A Unified Bias-Variance Decomposition and its Applications.pdf:pdf},
isbn = {2065432969},
pages = {231--238},
title = {{A Unified Bias-Variance Decomposition and its Applications}},
year = {2000}
}
@article{Dore2010b,
abstract = {A Dynamic Bayesian Network model uses the Instantaneous Topological Map algorithm to process couples of observed interacting trajectories. It recognizes human interactions through the analysis of their patterns of movement.},
annote = {Preguntas:


1. Only detection of activity, for example a state variable wich store the current activity of the body. Meeting, walking, running ... etc?
2. Al comprender mejor la estadistica bayesiana podrian surgir muchas ideas en el ambito economico.


Concluciones:
1. Podria pensarse en la velocidad de las manos como entrada al modelo para estimar el etado de la persona
2. Podria considerase un modelo de este tipo (interaccion) entre las manos para determinar el estado de las personas?},
author = {Dore, a. and Regazzoni, C.S.},
doi = {10.1109/MIS.2010.37},
file = {:home/abetan16/Dropbox/MendeleyV3/Dore, Regazzoni - 2010 - Interaction Analysis with a Bayesian Trajectory Model.pdf:pdf},
isbn = {1541-1672},
issn = {1541-1672},
journal = {IEEE Intelligent Systems},
keywords = {Dynamic Bayesian Networks,Interaction analysis,activity recognition,intelligent systems,trajectory analysis},
number = {0},
pages = {1--14},
title = {{Interaction Analysis with a Bayesian Trajectory Model}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5445068},
volume = {25},
year = {2010}
}
@article{Dore2010a,
abstract = {Visual tracking represents the basic processing step for most video analytics applications where the aim is to automatically understand the actions occurring in a monitored scene. Consequently, the performances of these applications are significantly dependent on the accuracy and robustness of the tracking algorithm. Bayesian state estimation and probabilistic graphical models (PGMs) have proved to be very powerful and appropriate mathematical tools to efficiently solve the inference problem of motion estimation by combining object dynamics and observations. In this article, the impact of these signal processing techniques on the development of recent tracking algorithms is shown and a categorization of the most common approaches is proposed. This categorization intends to logically organize different concepts related to Bayesian visual tracking to give a global overview to the reader. Finally, general considerations on the design of visual trackers for video analytics systems are discussed, focusing on the tradeoff that is usually performed between the accuracy of the target motion assumptions and the robustness of the object appearance representation.},
author = {Dore, a. and Soto, M. and Regazzoni, C.S.},
doi = {10.1109/MSP.2010.937395},
file = {:home/abetan16/Dropbox/MendeleyV3/Dore, Soto, Regazzoni - 2010 - Bayesian Tracking for Video Analytics.pdf:pdf},
isbn = {1053-5888},
issn = {1053-5888},
journal = {IEEE Signal Processing Magazine},
number = {5},
pages = {46--55},
pmid = {18505522},
title = {{Bayesian Tracking for Video Analytics}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5562665},
volume = {27},
year = {2010}
}
@article{Dore2010,
abstract = {In Smart Spaces (SSs), the capability of learning from experience is fundamental for autonomous adaptation to environmental changes and for proactive interaction with users. New research trends for reaching this goal are based on neurophysiological observations of human brain structure and functioning. A learning technique that is used to provide the SS with the so-called Autobiographical Memory is presented here by drawing inspiration from a bio-inspired model of the interactions occurring between the system and the user. Starting from the hypothesis that user's actions have a direct influence on the internal system state variables and vice versa, a statistical voting algorithm is proposed for inferring the cause/effect relationships among users and the system. The main contribution of this paper lies in proposing a general framework that is able to allow the SS to be aware of its present state as well as of the behavior of its users and to be able to predict the expected consequences of user actions.},
author = {Dore, Alessio and Cattoni, Andrea Fabio and Regazzoni, Carlo S.},
doi = {10.1109/TSMCA.2010.2052600},
file = {:home/abetan16/Dropbox/MendeleyV3/Dore, Cattoni, Regazzoni - 2010 - Interaction Modeling And Prediction In Smart Spaces A Bio-Inspired Approach Based On Autobiographical.pdf:pdf},
isbn = {1083-4427},
issn = {1083-4427},
journal = {IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
keywords = {Ambient intelligence,Artificial intelligence,Bio-inspired learning,Biomedical engineering,Humans,Intelligent robots,Intelligent sensors,Learning,Predictive models,Sensor systems,Smart Space (SS),Space technology,autobiographical memory,bio-inspired materials,bioinspired approach,brain models,brain-computer interfaces,dynamic interactions modeling,event prediction,human brain structure,interaction modeling,learning (artificial intelligence),learning technique,neurophysiological observation,neurophysiology,proactive interaction,smart space,state variable,statistical analysis,statistical voting algorithm},
number = {6},
pages = {1191--1205},
title = {{Interaction Modeling and Prediction in Smart Spaces: A Bio-Inspired Approach Based on Autobiographical Memory}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5518436},
volume = {40},
year = {2010}
}
@article{Dovern2012,
abstract = {Upper limb apraxia, a disorder of higher motor cognition, is a common consequence of left-hemispheric stroke. Contrary to common assumption, apraxic deficits not only manifest themselves during clinical testing but also have delirious effects on the patients' everyday life and rehabilitation. Thus, a reliable diagnosis and efficient treatment of upper limb apraxia is important to improve the patients' prognosis after stroke. Nevertheless, to date, upper limb apraxia is still an underdiagnosed and ill-treated entity. Based on a systematic literature search, this review summarizes the current tools of diagnosis and treatment strategies for upper limb apraxia. It furthermore provides clinicians with graded recommendations. In particular, a short screening test for apraxia, and a more comprehensive diagnostic apraxia test for clinical use are recommended. Although currently only a few randomized controlled studies investigate the efficacy of different apraxia treatments, the gesture training suggested by Smania and colleagues can be recommended for the therapy of apraxia, the effects of which were shown to extend to activities of daily living and to persist for at least 2 months after completion of the training. This review aims at directing the reader's attention to the ecological relevance of apraxia. Moreover, it provides clinicians with appropriate tools for the reliable diagnosis and effective treatment of apraxia. Nevertheless, this review also highlights the need for further research into how to improve diagnosis of apraxia based on neuropsychological models and to develop new therapeutic strategies.},
author = {Dovern, a. and Fink, G. R. and Weiss, P. H.},
doi = {10.1007/s00415-011-6336-y},
file = {:home/abetan16/Dropbox/MendeleyV3/Dovern, Fink, Weiss - 2012 - Diagnosis and treatment of upper limb apraxia.pdf:pdf},
isbn = {1432-1459 (Electronic)$\backslash$r0340-5354 (Linking)},
issn = {03405354},
journal = {Journal of Neurology},
keywords = {Motor cognition,Neuropsychology,Neurorehabilitation,Stroke},
number = {7},
pages = {1269--1283},
pmid = {22215235},
title = {{Diagnosis and treatment of upper limb apraxia}},
volume = {259},
year = {2012}
}
@inproceedings{Dovgalecs2010,
abstract = {This paper tackles the problem of image-based indoor location recognition. The context of the present work is activity monitoring using a wearable video camera data. Because application constraints necessitate weak supervision, a semi-supervised approach has been adopted which leverages the large amount of unlabeled images. The proposed method is based on the Bag of Features approach for image description followed by spectral dimensionality reduction in a transductive setup. Additional information from geometrical verification constraints are also considered which allowed to reach higher performance levels. The considered algorithms are compared experimentally on the data acquired in the wearable camera setup.},
author = {Dovgalecs, Vladislavs and Megret, R{\'{e}}mi and Wannous, Hazem and Berthoumieu, Yannick},
booktitle = {Proceedings - International Workshop on Content-Based Multimedia Indexing},
doi = {10.1109/CBMI.2010.5529903},
file = {:home/abetan16/Dropbox/MendeleyV3/Dovgalecs et al. - 2010 - Semi-supervised Learning for Location Recognition from Wearable Video.pdf:pdf},
isbn = {9781424480296},
issn = {19493991},
month = {jun},
pages = {112--118},
publisher = {Ieee},
title = {{Semi-supervised learning for location recognition from wearable video}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5529903},
year = {2010}
}
@article{Duque2013a,
author = {Duque, J and Betancourt, A and Marin, F},
doi = {10.2139/ssrn.2393179},
file = {:home/abetan16/Dropbox/MendeleyV3/Duque, Betancourt, Marin - 2013 - An Algorithmic Approach for Simulating Realistic Irregular Lattices.pdf:pdf},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
keywords = {computation,experiment,keywords,lattices,mr-polygons,regional science,ri-maps,s},
number = {13},
pages = {0--25},
title = {{An Algorithmic Approach for Simulating Realistic Irregular Lattices.}},
url = {http://repository.eafit.edu.co:8080/handle/10784/1003},
volume = {13-20},
year = {2013}
}
@article{Duque2010,
abstract = {In this paper, we present an efficiency improvement for the algorithm called AMOEBA, A Multidirectional Optimum Ecotope-Based Algorithm, devised by Aldstadt and Getis (Geogr Anal 38(4):327–343, 2006). AMOEBA embeds a local spatial autocorrelation statistic in an iterative procedure in order to identify spatial clusters (ecotopes) of related spatial units. We provide an analysis of the computational complexity of the original AMOEBA and develop an alternative formulation that reduces computational time without losing optimality. Empirical evidence is provided using georeferenced socio-demographic data in Accra, Ghana.},
author = {Duque, J.C. and Aldstadt, J and Velasquez, E and Franco, J.L. and Betancourt, A},
doi = {10.1007/s10109-010-0137-1},
file = {:home/abetan16/Dropbox/MendeleyV3/Duque et al. - 2011 - A computationally efficient method for delineating irregularly shaped spatial clusters.pdf:pdf},
isbn = {1010901001371},
issn = {1435-5930, 1435-5949},
journal = {Journal of Geographical Systems},
keywords = {AMOEBA,Cluster detection,G statistic. Ecotope,econometry,mathematical methods,special topics},
number = {4},
pages = {355--372},
publisher = {Springer},
title = {{A computationally efficient method for delineating irregularly shaped spatial clusters}},
url = {http://link.springer.com/article/10.1007/s10109-010-0137-1$\backslash$nhttp://link.springer.com/article/10.1007/s10109-010-0137-1{\#}page-1},
volume = {13},
year = {2011}
}
@article{Duque2013,
abstract = {El presente estudio presenta un an{\'{a}}lisis, espacialmente desagregado, de la disminuci{\'{o}}n en el consumo residencial de agua en el {\'{A}}rea Metropolitana del Valle de Aburr{\'{a}} (AMVA) durante el periodo 2005 – 2010 e identificar las caracter{\'{i}}sticas socioecon{\'{o}}micas asociadas a estos patrones. Adem{\'{a}}s, se analiza la forma c{\'{o}}mo las pol{\'{i}}ticas y campa{\~{n}}as para incentivar la disminuci{\'{o}}n del consumo de agua potable han sido acogidas en el AMVA. Los resultados obtenidos muestran claras diferencias espaciales (y por estrato) en los niveles de consumo de agua, as{\'{i}} como en los niveles de reducci{\'{o}}n de dicho consumo en el per{\'{i}}odo analizado. Tambi{\'{e}}n, por medio de modelos de econometr{\'{i}}a espacial, se encuentra que las caracter{\'{i}}sticas socioecon{\'{o}}micas juegan un papel relevante a la hora de explicar los niveles de consumo de agua y que estos consumos presentan autocorrelaci{\'{o}}n espacial sustantiva que indica que los niveles de consumo de agua potable en un {\'{a}}rea determinada no solo dependen de las caracter{\'{i}}sticas socioecon{\'{o}}micas del {\'{a}}rea, sino tambi{\'{e}}n de los niveles de consumo de las {\'{a}}reas vecinas. Por {\'{u}}ltimo, se encuentra que los impactos derivados del desincentivo econ{\'{o}}mico al consumo excesivo, tiene un efecto inmediato que no perdura en el tiempo},
author = {Duque, J.C. and Gutierrez, D.C. and Betancourt, A and Patino, J},
doi = {10.2139/ssrn.2503129},
file = {:home/abetan16/Dropbox/MendeleyV3/Duque et al. - 2013 - Annlisis De La Distribuciin Espacial De La Reducciin En La Demanda De Agua Potable Como Efecto De Pollticas De Aho.pdf:pdf},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
keywords = {Demanda de agua potable,pol{\'{i}}ticas p{\'{u}}blicas,regresi{\'{o}}n espacial},
number = {13},
pages = {0--34},
title = {{Annlisis De La Distribuciin Espacial De La Reducciin En La Demanda De Agua Potable Como Efecto De Pollticas De Ahorro En Su Consumo En El rea Metropolitana Del Valle De Aburrr (Analysis of Spatial Distribution of Water Demand as an Effect of Consumption S}},
url = {http://www.ssrn.com/abstract=2503129},
year = {2013}
}
@misc{Duque2011b,
address = {Colombia},
author = {Duque, Juan C. and Dev, B and Betancourt, A and Franco, J. L.},
publisher = {RiSE-group (Research in Spatial Economics). EAFIT University.},
title = {{ClusterPy: Library of spatially constrained clustering algorithms}},
year = {2011}
}
@article{Duque2013b,
author = {Duque, Juan C. and Ye, Xinyue and Folch, David C.},
doi = {10.1111/pirs.12088},
file = {:home/abetan16/Dropbox/MendeleyV3/Duque, Ye, Folch - 2013 - Spmorph an Exploratory Space‐time Analysis Tool for Describing Processes of Spatial Redistribution.pdf:pdf},
issn = {10568190},
journal = {Papers in Regional Science},
keywords = {analytical regions,space-time,spatial clustering},
number = {3},
pages = {629--651},
title = {{spMorph: An exploratory space-time analysis tool for describing processes of spatial redistribution}},
url = {http://doi.wiley.com/10.1111/pirs.12088},
volume = {94},
year = {2015}
}
@inproceedings{Durand1999,
abstract = {It is usually said that genetic algorithm shold be used when nothing else works. In practice, genetic algorithm are very often used for large sized global optimization problems, but are noy very efficient for local optimization problems. The Nelder-Mead simplex algorithm has some common characteristics with genetic algorithm, but it can only find a local optimum close to the starting point. In this article, a combined Nelder-Mead Simplex and Genetic algorithm is introduced and tested on classical test functions on which both genetic algorithm or local optimization techniques are not efficient when separately used.},
author = {Durand, Nicolas and Alliot, Jean-Marc},
booktitle = {Gecco 99},
file = {:home/abetan16/Dropbox/MendeleyV3/Durand, Alliot - 1999 - A Combined Nelder-Mead Simplex and Genetic Algorithm.pdf:pdf},
pages = {1--7},
title = {{A Combined Nelder Mead Simplex and Genetic Algorithm}},
url = {http://recherche.enac.fr/opti/papers/articles/gecco99.pdf},
volume = {012},
year = {1999}
}
@inproceedings{Egarter2013,
abstract = {Non-Intrusive Load Monitoring is a single-point metering approach to identify and to monitor household appliances according their appliance power characteristics. In this paper, we propose an unsupervised classification approach for appliance state estimation of on/off-appliances modeled by a Hidden Markov Model (HMM). To estimate the states of appliances, we use the sequential Monte Carlo or particle filtering (PF) method. The proposed algorithm is tested with MATLAB simulations and is evaluated according to correctly or incorrectly detected on/off events.},
address = {New York, New York, USA},
author = {Egarter, Dominik and Bhuvana, Venkata Pathuri and Elmenreich, Wilfried},
booktitle = {Proceedings of the 5th ACM Workshop on Embedded Systems For Energy-Efficient Buildings - BuildSys'13},
doi = {10.1145/2528282.2528306},
file = {:home/abetan16/Dropbox/MendeleyV3/Egarter, Bhuvana, Elmenreich - 2013 - Appliance State Estimation based on Particle Filtering.pdf:pdf},
isbn = {9781450324311},
pages = {1--2},
publisher = {ACM Press},
title = {{Appliance State Estimation based on Particle Filtering}},
url = {http://dl.acm.org/citation.cfm?doid=2528282.2528306},
year = {2013}
}
@article{Einhauser2008,
author = {Einh{\"{a}}user, Wolfgang and Perona, Pietro},
doi = {10.1167/8.14.18.Introduction},
file = {:home/abetan16/Dropbox/MendeleyV3/Einh{\"{a}}user, Perona - 2008 - Objects predict fi xations better than early saliency.pdf:pdf},
journal = {Journal of Vision},
keywords = {1,10,1167,14,18,2008,26,8,attention,better than early saliency,citation,doi,einh{\"{a}}user,eye movements,http,journal of vision,journalofvision,m,object recognition,objects predict fi xations,org,p,perona,scene recognition,spain,w},
pages = {1--26},
title = {{Objects predict fi xations better than early saliency}},
url = {http://ww.w.journalofvision.org/content/8/14/18.short},
volume = {8},
year = {2008}
}
@incollection{Elliott1999,
abstract = {Lifelike pedagogical agents have been the subject of increasing attention in the agents and knowledge-based learning environment communities [2, 17, 19—21]. In parallel developments, recent years have witnessed great strides in work on cognitive models of emotion and affective reasoning [4,18, 22]. As a result, the time is now ripe for exploring how affective reasoning can be incorporated into pedagogical agents to improve students' learning experiences.},
author = {Elliott, Clark and Rickel, Jeff and Lester, James C},
booktitle = {Collection of Artificial Intelligence Today},
doi = {10.1007/3-540-48317-9},
file = {:home/abetan16/Dropbox/Articles/Mendeley/Elliott, Rickel, Lester - 1999 - Lifelike Pedagogical Agents And Affective Computing An Exploratory Synthesis.pdf:pdf},
isbn = {978-3-540-66428-4},
pages = {195--211},
publisher = {SpringerVerlag},
title = {{Lifelike Pedagogical Agents and Affective Computing: An Exploratory Synthesis}},
url = {http://link.springer.com/chapter/10.1007/3-540-48317-9{\_}8},
year = {1999}
}
@article{Ellouze2010,
abstract = {The need of summarization methods and systems has become more and more crucial as the audio-visual material continues its critical growth. This paper presents a novel vision and a novel system for movies summarization. A video summary is an audio-visual document displaying the essential parts of an original document. However, the definition of the term "essential" is user-dependent. The advantage of this work, unlike the others, is the involvement of users in the summarization process. By means of IM(S) 2, people generate on the fly customized video summaries responding to their preferences. IM(S)2 is made up of an offline part and an online part. In the offline, we segment the movies into shots and we compute features describing them. In the online part users inform about their preferences by selecting interesting shots. After that, the system will analyze the selected shots to bring out the user's preferences. Finally the system will generate a summary from the whole movie which will provide more focus on the user's preferences. To show the efficiency of IM(S)2, it was tested on the database of the European project MUSCLE made up of five movies. We invited 10 users to evaluate the usability of our system by generating for every movie of the database a semi-supervised summary and to judge at the end its quality. Obtained results are encouraging and show the merits of our approach. ?? 2009 Elsevier Inc. All rights reserved.},
author = {Ellouze, Mehdi and Boujemaa, Nozha and Alimi, Adel M.},
doi = {10.1016/j.jvcir.2010.01.007},
file = {:home/abetan16/Dropbox/MendeleyV3/Ellouze, Boujemaa, Alimi - 2010 - IM(S)2 Interactive Movie Summarization System.pdf:pdf},
isbn = {1047-3203},
issn = {10473203},
journal = {Journal of Visual Communication and Image Representation},
keywords = {Content analysis,Genetic algorithm,Interactive multimedia system,One-class SVM,Pattern recognition,Users' preferences,Video analysis,Video summarization},
month = {may},
number = {4},
pages = {283--294},
publisher = {Elsevier Inc.},
title = {{IM(S)2: Interactive movie summarization system}},
url = {http://dx.doi.org/10.1016/j.jvcir.2010.01.007},
volume = {21},
year = {2010}
}
@misc{Joyce2009,
abstract = {I'll try to make this introduction to Bayesian statistics clear and short. First we'll look as a specific example, then the general setting, then Bayesian statistics for the Bernoulli process, for the Poisson process, and for normal distributions.},
author = {Ena, G W},
file = {:home/abetan16/Dropbox/MendeleyV3/Ena - 2008 - a Short Introduction To.pdf:pdf},
pages = {1--14},
title = {{a Short Introduction To}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:A+short+introduction+to+Bayesian+statistics{\#}0},
year = {2008}
}
@inproceedings{Endres2010,
abstract = {We propose a category-independent method to produce a bag of regions and rank them, such that top-ranked regions are likely to be good segmentations of different objects. Our key objectives are completeness and diversity: every object should have at least one good proposed region, and a diverse set should be top-ranked. Our approach is to generate a set of segmentations by performing graph cuts based on a seed region and a learned affinity function. Then, the regions are ranked using structured learning based on various cues. Our experiments on BSDS and PASCAL VOC 2008 demonstrate our ability to find most objects within a small bag of proposed regions.},
address = {Crete, Greece},
author = {Endres, Ian and Hoiem, Derek},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-15555-0_42},
file = {:home/abetan16/Dropbox/MendeleyV3/Endres, Hoiem - 2010 - Category Independent Object Proposals.pdf:pdf},
isbn = {3642155545},
issn = {03029743},
number = {PART 5},
pages = {575--588},
pmid = {24356345},
publisher = {Springerlink},
title = {{Category independent object proposals}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-15555-0{\_}42},
volume = {6315 LNCS},
year = {2010}
}
@article{Eng2007,
abstract = {We present a virtual reality (VR)-based motor neurorehabilitation system for stroke patients with upper limb paresis. It is based on two hypotheses: (1) observed actions correlated with self-generated or intended actions engage cortical motor observation, planning and execution areas ("mirror neurons"); (2) activation in damaged parts of motor cortex can be enhanced by viewing mirrored movements of non-paretic limbs. We postulate that our approach, applied during the acute post-stroke phase, facilitates motor re-learning and improves functional recovery. The patient controls a first-person view of virtual arms in tasks varying from simple (hitting objects) to complex (grasping and moving objects). The therapist adjusts weighting factors in the non-paretic limb to move the paretic virtual limb, thereby stimulating the mirror neuron system and optimizing patient motivation through graded task success. We present the system's neuroscientific background, technical details and preliminary results.},
author = {Eng, Kynan and Siekierka, Ewa and Pyk, Pawel and Chevrier, Edith and Hauser, Yves and Cameirao, Monica and Holper, Lisa and H{\"{a}}gni, Karin and Zimmerli, Lukas and Duff, Armin and Schuster, Corina and Bassetti, Claudio and Verschure, Paul and Kiper, Daniel},
doi = {10.1007/s11517-007-0239-1},
file = {:home/abetan16/Dropbox/MendeleyV3/Eng et al. - 2007 - Interactive visuo-motor therapy system for stroke rehabilitation.pdf:pdf},
isbn = {0140-0118 (Print)$\backslash$n0140-0118 (Linking)},
issn = {01400118},
journal = {Medical and Biological Engineering and Computing},
keywords = {Rehabilitation,Stroke,Therapy,Virtual reality},
month = {sep},
number = {9},
pages = {901--907},
pmid = {17687578},
title = {{Interactive visuo-motor therapy system for stroke rehabilitation}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17687578},
volume = {45},
year = {2007}
}
@article{Everingham2010,
abstract = {The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection. This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.},
author = {Everingham, Mark and Everingham, Mark and Gool, Luc Van and Gool, Luc Van and Williams, Christopher K I and Williams, Christopher K I and Winn, John and Winn, John and Zisserman, Andrew and Zisserman, Andrew},
doi = {10.1007/s11263-009-0275-4},
file = {:home/abetan16/Dropbox/Articles/Mendeley/Everingham, Gool - 2010 - The Pascal Visual Object Classes (VOC) Challenge.pdf:pdf},
journal = {International Journal},
keywords = {benchmark,database,object recognition},
number = {2},
pages = {303--338},
title = {{Visual Object Classes (VOC) Challenge}},
url = {http://link.springer.com/article/10.1007/s11263-009-0275-4},
volume = {88},
year = {2009}
}
@article{Fan2008,
abstract = {LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets.},
author = {Fan, Re and Chang, Kw and Hsieh, Cj},
doi = {10.1038/oby.2011.351},
file = {:home/abetan16/Dropbox/Articles/Mendeley/Fan, Chang, Hsieh - 2008 - LIBLINEAR a Library for Large Linear Classification.pdf:pdf},
isbn = {089791497X},
issn = {15324435},
journal = {The Journal of Machine Learning},
keywords = {large-scale linear classification,logistic regression,machine learning,open,source,support vector machines},
number = {2008},
pages = {1871--1874},
pmid = {22173572},
title = {{LIBLINEAR: A library for large linear classification}},
url = {http://dl.acm.org/citation.cfm?id=1442794},
volume = {9},
year = {2008}
}
@inproceedings{Farringdon2000,
abstract = {Augmented Memory is widely regarded as a key wearable computing application. Retrieval cues are effective means for improving memory recall. The Visual Augmented Memory (VAM) application is a fully automated wearable implementation for the identification, storage, and subsequent retrieval and presentation of visual memory cues based upon face recognition.},
address = {Atlanta GA},
author = {Farringdon, J and Oni, V},
booktitle = {International Symposium on wearable computers},
doi = {10.1109/ISWC.2000.888484},
file = {:home/abetan16/Dropbox/MendeleyV3/Farringdon, Oni - 2000 - Visual Augmented Memory.pdf:pdf},
isbn = {0769507956},
pages = {167--168},
title = {{Visual Augmented Memory}},
url = {http://www.computer.org/csdl/proceedings/iswc/2000/0795/00/07950167.pdf},
year = {2000}
}
@inproceedings{Fathi2012,
abstract = {This paper presents a method for the detection and recognition of social interactions in a day-long first-person video of a social event, like a trip to an amusement park. The location and orientation of faces are estimated and used to compute the line of sight for each face. The context provided by all the faces in a frame is used to convert the lines of sight into locations in space to which individuals at- tend. Further, individuals are assigned roles based on their patterns of attention. The roles and locations of individu- als are analyzed over time to detect and recognize the types of social interactions. In addition to patterns of face loca- tions and attention, the head movements of the first-person can provide additional useful cues as to their attentional fo- cus. We demonstrate encouraging results on detection and recognition of social interactions in first-person videos cap- tured from multiple days of experience in amusement parks},
address = {Providence, RI},
author = {Fathi, Alircza and Hodgins, Jessica K. and Rehg, James M.},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2012.6247805},
file = {:home/abetan16/Dropbox/MendeleyV3/Fathi, Hodgins, Rehg - 2012 - Social Interactions A First-Person Perspective.pdf:pdf},
isbn = {9781467312264},
issn = {10636919},
month = {jun},
pages = {1226--1233},
publisher = {IEEE},
title = {{Social interactions: A first-person perspective}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6247805},
year = {2012}
}
@inproceedings{Fathi2011,
abstract = {We present a method to analyze daily activities, such as meal preparation, using video from an egocentric camera. Our method performs inference about activities, actions, hands, and objects. Daily activities are a challenging domain for activity recognition which are well-suited to an egocentric approach. In contrast to previous activity recognition methods, our approach does not require pre-trained detectors for objects and hands. Instead we demonstrate the ability to learn a hierarchical model of an activity by exploiting the consistent appearance of objects, hands, and actions that results from the egocentric context. We show that joint modeling of activities, actions, and objects leads to superior performance in comparison to the case where they are considered independently. We introduce a novel representation of actions based on object-hand interactions and experimentally demonstrate the superior performance of our representation in comparison to standard activity representations such as bag of words.},
author = {Fathi, Alireza and Farhadi, Ali and Rehg, James M.},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2011.6126269},
file = {:home/abetan16/Dropbox/MendeleyV3//Fathi, Ren, Rehg - 2011 - Learning to recognize objects in egocentric activities.pdf:pdf},
isbn = {9781457711015},
issn = {1550-5499},
month = {nov},
pages = {407--414},
publisher = {IEEE},
title = {{Understanding egocentric activities}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6126269},
year = {2011}
}
@inproceedings{Fathi2012a,
abstract = {We present a probabilistic generative model for simultaneously recognizing daily actions and predicting gaze locations in videos recorded from an egocentric camera. We focus on activities requiring eye-hand coordination and model the spatio-temporal relationship between the gaze point, the scene objects, and the action label. Our model captures the fact that the distribution of both visual features and object occurrences in the vicinity of the gaze point is correlated with the verb-object pair describing the action. It explicitly incorporates known properties of gaze behavior from the psychology literature, such as the temporal delay between fixation and manipulation events. We present an inference method that can predict the best sequence of gaze locations and the associated action label from an input sequence of images. We demonstrate improvements in action recognition rates and gaze prediction accuracy relative to state-of-the-art methods, on two new datasets that contain egocentric videos of daily activities and gaze.},
address = {Florence, Itaty},
author = {Fathi, Alireza and Li, Yin and Rehg, James M.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-33718-5_23},
file = {:home/abetan16/Dropbox/MendeleyV3/Fathi, Ren, Rehg - 2011 - Learning to recognize objects in egocentric activities(3).pdf:pdf},
isbn = {9783642337178},
issn = {03029743},
number = {PART 1},
pages = {314--327},
publisher = {Georgia Institute of Technology},
title = {{Learning to recognize daily actions using gaze}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-33718-5{\_}23},
volume = {7572 LNCS},
year = {2012}
}
@inproceedings{Fathi2013,
abstract = {In this paper we present a model of action based on the change in the $\backslash$nstate of the environment. Many actions involve similar dynamics and hand-object $\backslash$nrelationships, but differ in their purpose and meaning. The key to $\backslash$ndifferentiating these actions is the ability to identify how they change the $\backslash$nstate of objects and materials in the environment. We propose a weakly $\backslash$nsupervised method for learning the object and material states that are necessary $\backslash$nfor recognizing daily actions. Once these state detectors are learned, we can $\backslash$napply them to input videos and pool their outputs to detect actions. We further $\backslash$ndemonstrate that our method can be used to segment discrete actions from a $\backslash$ncontinuous video of an activity. Our results outperform state-of-the-art action $\backslash$nrecognition and activity segmentation results.},
annote = {Activity recogntion
- Object recognition
- Activity as a sequence
- Optical flow
- SVM
- Optimization








Video segmentation
- SVM
- Optimization},
author = {Fathi, Alireza and Rehg, James M.},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer  Vision and Pattern Recognition},
doi = {10.1109/CVPR.2013.333},
file = {:home/abetan16/Dropbox/MendeleyV3/Fathi, Rehg - 2013 - Modeling Actions through State Changes.pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {10636919},
keywords = {Action Recognition,Egocentric,Object,Smi-Supervised Learning,State},
month = {jun},
pages = {2579--2586},
publisher = {Ieee},
title = {{Modeling actions through state changes}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6619177},
year = {2013}
}
@inproceedings{Fathi2011a,
abstract = {This paper addresses the problem of learning object models from egocentric video of household activities, using extremely weak supervision. For each activity sequence, we know only the names of the objects which are present within it, and have no other knowledge regarding the appearance or location of objects. The key to our approach is a robust, unsupervised bottom up segmentation method, which exploits the structure of the egocentric domain to partition each frame into hand, object, and background categories. By using Multiple Instance Learning to match object instances across sequences, we discover and localize object occurrences. Object representations are refined through transduction and object-level classifiers are trained. We demonstrate encouraging results in detecting novel object instances using models produced by weakly-supervised learning.},
address = {Providence, RI},
author = {Fathi, Alireza and Ren, Xiaofeng and Rehg, James M.},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2011.5995444},
file = {:home/abetan16/Dropbox/MendeleyV3/Fathi, Ren, Rehg - 2011 - Learning to recognize objects in egocentric activities(3).pdf:pdf},
isbn = {9781457703942},
issn = {10636919},
month = {jun},
pages = {3281--3288},
publisher = {IEEE},
title = {{Learning to recognize objects in egocentric activities}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5995444},
year = {2011}
}
@article{Fei-Fei2007,
abstract = {Current computational approaches to learning visual object categories require thousands of training images, are slow, cannot learn in an incremental manner and cannot incorporate prior information into the learning process. In addition, no algorithm presented in the literature has been tested on more than a handful of object categories. We present an method for learning object categories from just a few training images. It is quick and it uses prior information in a principled way. We test it on a dataset composed of images of objects belonging to 101 widely varied categories. Our proposed method is based on making use of prior information, assembled from (unrelated) object categories which were previously learnt. A generative probabilistic model is used, which represents the shape and appearance of a constellation of features belonging to the object. The parameters of the model are learnt incrementally in a Bayesian manner. Our incremental algorithm is compared experimentally to an earlier batch Bayesian algorithm, as well as to one based on maximum likelihood. The incremental and batch versions have comparable classification performance on small training sets, but incremental learning is significantly faster, making real-time learning feasible. Both Bayesian methods outperform maximum likelihood on small training sets. ?? 2006 Elsevier Inc. All rights reserved.},
author = {Fei-Fei, Li and Fergus, Rob and Perona, Pietro},
doi = {10.1016/j.cviu.2005.09.012},
file = {:home/abetan16/Dropbox/MendeleyV3/Fei-Fei, Fergus, Perona - 2007 - Learning Generative Visual Models from Few Training Examples an Incremental Bayesian Approach Tested on.pdf:pdf},
isbn = {1077-3142},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {Bayesian model,Categorization,Generative model,Incremental learning,Object recognition},
number = {1},
pages = {59--70},
publisher = {Ieee},
title = {{Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1384978 http://www.sciencedirect.com/science/article/pii/S1077314206001688},
volume = {106},
year = {2007}
}
@article{Feix2014,
abstract = {—This paper is the second in a two-part series analyzing human grasping behavior during a wide range of unstructured tasks. It investigates the tasks performed during the daily work of two housekeepers and two machinists and correlates grasp type and object properties with the attributes of the tasks being performed. The task or activity is classified according to the force required, the degrees of freedom, and the functional task type. We found that 46 percent of tasks are constrained, where the manipulated object is not allowed to move in a full six degrees of freedom. Analyzing the interrelationships between the grasp, object, and task data show that the best predictors of the grasp type are object size, task constraints, and object mass. Using these attributes, the grasp type can be predicted with 47 percent accuracy. Those parameters likely make useful heuristics for grasp planning systems. The results further suggest the common sub-categorization of grasps into power, intermediate, and precision categories may not be appropriate, indicating that grasps are generally more multi-functional than previously thought. We find large and heavy objects are grasped with a power grasp, but small and lightweight objects are not necessarily grasped with precision grasps—even with grasped object size less than 2 cm and mass less than 20 g, precision grasps are only used 61 percent of the time. These results have important implications for robotic hand design and grasp planners, since it appears while power grasps are frequently used for heavy objects, they can still be quite practical for small, lightweight objects.},
author = {Feix, Thomas and Bullock, Ian M and Dollar, Aaron M},
doi = {10.1109/TOH.2014.2326867},
file = {:home/abetan16/Dropbox/MendeleyV3/Feix, Bullock, Dollar - 2014 - Analysis of Human Grasping Behavior Correlating Tasks, Objects and Grasps(2).pdf:pdf},
issn = {19391412},
journal = {IEEE Transactions on Haptics},
keywords = {Index Terms—Human grasping,activities of daily living,manipulation,prosthetics,robotic hands},
number = {4},
pages = {430--441},
pmid = {25248214},
title = {{Analysis of Human Grasping Behavior: Correlating Tasks, Objects and Grasps}},
volume = {7},
year = {2014}
}
@article{Felzenszwalb2010,
abstract = {We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI--SVM in terms of latent variables. A latent SVM is semiconvex, and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function.},
author = {Felzenszwalb, P F and Girshick, R B and McAllester, D and Ramanan, D},
doi = {10.1109/TPAMI.2009.167},
file = {:home/abetan16/Dropbox/MendeleyV3/Felzenszwalb et al. - 2010 - Object Detection with Discriminatively Trained Part-based Models.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Computer-Assisted,Computer-Assisted: methods,Discriminant Analysis,Image Enhancement,Image Enhancement: methods,Image Interpretation,Imaging,Pattern Recognition,Reproducibility of Results,Sensitivity and Specificity,Three-Dimensional,Three-Dimensional: methods},
month = {sep},
number = {9},
pages = {1627--1645},
pmid = {20634557},
title = {{Object Detection with Discriminatively Trained Part-Based Models}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5255236},
volume = {32},
year = {2010}
}
@article{Feng2014,
abstract = {We demonstrate a Google Glass-based rapid diagnostic test (RDT) reader platform capable of qualitative and quantitative measurements of various lateral flow immunochromatographic assays and similar biomedical diagnostics tests. Using a custom-written Glass application and without any external hardware attachments, one or more RDTs labeled with Quick Response (QR) code identifiers are simultaneously imaged using the built-in camera of the Google Glass that is based on a hands-free and voice-controlled interface and digitally transmitted to a server for digital processing. The acquired JPEG images are automatically processed to locate all the RDTs and, for each RDT, to produce a quantitative diagnostic result, which is returned to the Google Glass (i.e., the user) and also stored on a central server along with the RDT image, QR code, and other related information (e.g., demographic data). The same server also provides a dynamic spatiotemporal map and real-time statistics for uploaded RDT results accessible through Internet browsers. We tested this Google Glass-based diagnostic platform using qualitative (i.e., yes/no) human immunodeficiency virus (HIV) and quantitative prostate-specific antigen (PSA) tests. For the quantitative RDTs, we measured activated tests at various concentrations ranging from 0 to 200 ng/mL for free and total PSA. This wearable RDT reader platform running on Google Glass combines a hands-free sensing and image capture interface with powerful servers running our custom image processing codes, and it can be quite useful for real-time spatiotemporal tracking of various diseases and personal medical conditions, providing a valuable tool for epidemiology and mobile health.},
author = {Feng, Steve and Caire, Romain and Cortazar, Bingen and Turan, Mehmet and Wong, Andrew and Ozcan, Aydogan},
doi = {10.1021/nn500614k},
file = {:home/abetan16/Dropbox/MendeleyV3/Feng et al. - 2014 - Immunochromatographic Diagnostic Test Analysis Using Google Glass.pdf:pdf},
issn = {1936-0851},
journal = {ACS Nano},
keywords = {assays,colorimetric sensor,google glass,hiv testing,lateral fl ow immunochromatographic,mobile health,prostate-speci fi c antigen,psa,rapid diagnostic test reader,test},
month = {feb},
number = {3},
pages = {3069--3079},
pmid = {24571349},
title = {{Immunochromatographic Diagnostic Test Analysis Using Google Glass}},
url = {http://pubs.acs.org/doi/abs/10.1021/nn500614k},
volume = {8},
year = {2014}
}
@article{Fenza,
abstract = {Musical interpretations are often the result of a wide range of requirements on expressiveness rendering and technical skills. Aspects indicated by the term expressive intention and which refer to the communication of moods and feelings, are being considered more and more important in performer-computer interaction during music performance. Recent studies demonstrate the possibility of conveying different sensitive content like expressive intentions and emotions by opportunely modifying systematic deviations introduced by the musician. In this paper, we present a control strategy based on a multi-layer representation with three different stages of mapping, to explore the analogies between sound and movement spaces. The mapping between the performer (dancer and/or musician) movements and the expressive audio rendering engine resulting by two 3D "expressive" spaces, one obtained by the Laban and Lawrence's effort's theory, the other by means of a multidimensional analysis of perceptual tests carried out on various professionally performed pieces ranging from western classical to popular music. As an example, an application based on this model is presented: the system is developed using the eMotion SMART motion capture system and the Eyesweb software.},
author = {Fenza, D and Mion, L and Canazza, S and Rod{\`{a}}, A},
file = {:home/abetan16/Dropbox/MendeleyV3/Fenza et al. - 2005 - Physical movement and musical gestures A multilevel mapping strategy.pdf:pdf},
journal = {Proceedings of Sound and Music Computing Conference, SMC 2005},
pages = {27},
title = {{Physical movement and musical gestures: A multilevel mapping strategy}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84905187477{\&}partnerID=40{\&}md5=8ecf57292b9c0a47f55b32bfb4acc4a4},
year = {2005}
}
@inproceedings{Fergus2010,
abstract = {In an object recognition scenario with tens of thousands of categories, even a small number of labels per category leads to a very large number of total labels required. We propose a simple method of label sharing between semantically similar categories. We leverage the WordNet hierarchy to define semantic distance between any two categories and use this semantic distance to share labels. Our approach can be used with any classifier. Experimental results on a range of datasets, up to 80 million images and 75,000 categories in size, show that despite the simplicity of the approach, it leads to significant improvements in performance.},
address = {Heraklion, Crete},
author = {Fergus, Rob and Bernal, Hector and Weiss, Yair and Torralba, Antonio},
booktitle = {European Conference on Computer Vision: Part I},
file = {:home/abetan16/Dropbox/MendeleyV3/Fergus et al. - 2014 - Semantic Label Sharing for Learning with Many Categories.pdf:pdf},
keywords = {clustering,learning on graphs,transductive classification,transductive regression},
pages = {762--775},
publisher = {Springer-Verlag},
title = {{Semantic Label Sharing for Learning with Many Categories}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-15549-9{\_}55},
year = {2014}
}
@article{Fitzgibbon1995,
abstract = {In this paper we evaluate several methods of fitting data to conic sections. Conic fitting is a commonly required task in machine vision, but many algorithms perform badly on incomplete or noisy data. We evaluate several algorithms under various noise and degeneracy conditions, identify the key parameters which affect sensitivity, and present the results of comparative experiments which emphasize the algorithms' behaviours under common examples of degenerate data. In addition, complexity analyses in terms of flop counts are provided in order to further inform the choice of algorithm for a specific application.},
author = {Fitzgibbon, Andrew W and Fisher, Robert B},
doi = {10.5244/C.9.51},
file = {:home/abetan16/Dropbox/MendeleyV3/Fitzgibbon, Fisher - 1995 - A Buyer's Guide to Conic Fitting.pdf:pdf},
isbn = {0-9521898-2-8},
issn = {00319228},
journal = {British Machine Vision Conference},
pages = {513--522},
pmid = {1366},
publisher = {British Machine Vision Association},
title = {{A Buyer ' s Guide to Conic Fitting}},
url = {http://www.bmva.org/bmvc/1995/bmvc-95-050.html},
year = {1995}
}
@article{Folch2013,
abstract = {The identification of regions is both a computational and conceptual challenge. Even with growing computational power, regionalization algorithms must rely on heuristic approaches in order to find solutions. Therefore, the constraints and evaluation criteria that define a region must be translated into an algorithm that can efficiently and effectively navigate the solution space to find the best solution. One limitation of many existing regionalization algorithms is a requirement that the number of regions be selected a priori. The recently introduced max-p algorithm does not have this requirement, and thus the number of regions is an output of, not an input to, the algorithm. In this paper, we extend the max-p algorithm to allow for greater flexibility in the constraints available to define a feasible region, placing the focus squarely on the multidimensional characteristics of the region. We also modify technical aspects of the algorithm to provide greater flexibility in its ability to search the solution space. Using synthetic spatial and attribute data, we are able to show the algorithm's broad ability to identify regions in maps of varying complexity. We also conduct a large-scale computational experiment to identify parameter settings that result in the greatest solution accuracy under various scenarios. The rules of thumb identified from the experiment produce maps that correctly assign areas to their ‘true' region with 94{\%} average accuracy, with nearly 50{\%} of the simulations reaching 100{\%} accuracy.},
author = {Folch, David C. and Spielman, Seth E.},
doi = {10.1080/13658816.2013.848986},
file = {:home/abetan16/Dropbox/MendeleyV3/Folch, Spielman - 2014 - Identifying regions based on flexible user-defined constraints.pdf:pdf},
issn = {1365-8816},
journal = {International Journal of Geographical Information Science},
keywords = {functional regions,max-p,regionalization,regions,tabu search},
month = {oct},
number = {1},
pages = {164--184},
publisher = {Taylor {\&} Francis},
title = {{Identifying regions based on flexible user-defined constraints}},
url = {http://www.tandfonline.com/doi/abs/10.1080/13658816.2013.848986},
volume = {28},
year = {2014}
}
@article{Follmi2013,
author = {F{\"{o}}llmi, C},
file = {:home/abetan16/Dropbox/MendeleyV3/F{\"{o}}llmi - 2013 - Body-Mounted Cameras.pdf:pdf},
journal = {Citeseer},
keywords = {camera,eyetap,lifelogging,mediated reality,motion capture,sensecam,sousveillance,wearable computing},
title = {{Body-Mounted Cameras}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.416.1856{\&}rep=rep1{\&}type=pdf},
year = {2013}
}
@incollection{Ford1996,
author = {Ford, L},
booktitle = {The Geographical Review},
number = {3},
pages = {437--440},
title = {{A new and improved model of Latin American city structure}},
url = {http://www.jstor.org/stable/215506},
volume = {86},
year = {1996}
}
@article{Friedman1997,
abstract = {The classification problem is considered in which an outputvariable y assumes discrete values with respectiveprobabilities that depend upon the simultaneous values of a set of input variablesx = {\{}x{\_}1,....,x{\_}n{\}}. At issue is how error in the estimates of theseprobabilities affects classification error when the estimates are used ina classification rule. These effects are seen to be somewhat counterintuitive in both their strength and nature. In particular the bias andvariance components of the estimation error combine to influenceclassification in a very different way than with squared error on theprobabilities themselves. Certain types of (very high) bias can becanceled by low variance to produce accurate classification. This candramatically mitigate the effect of the bias associated with some simpleestimators like “naive” Bayes, and the bias induced by thecurse-of-dimensionality on nearest-neighbor procedures. This helps explainwhy such simple methods are often competitive with and sometimes superiorto more sophisticated ones for classification, and why“bagging/aggregating” classifiers can often improveaccuracy. These results also suggest simple modifications to theseprocedures that can (sometimes dramatically) further improve theirclassification performance.},
author = {Friedman, Jerome H.},
doi = {10.1023/A:1009778005914},
file = {:home/abetan16/Dropbox/MendeleyV3/Friedman - 1997 - On bias, variance, 01—loss, and the curse-of-dimensionality.pdf:pdf},
issn = {1384-5810},
journal = {Data mining and knowledge discovery},
keywords = {bagging,bias,classification,curse-of-dimensionality,naive bayes,nearest-neighbors,variance},
number = {1},
pages = {55--77},
title = {{On bias, variance, 0/1—loss, and the curse-of-dimensionality}},
url = {http://link.springer.com/article/10.1023/A:1009778005914},
volume = {77},
year = {1997}
}
@inproceedings{Gemmell2002,
abstract = {Storage trends have brought us to the point where it is affordable to keep a complete digital record of one's life, and capture methods are multiplying. To experiment with a lifetime store, we are digitizing everything possible from Gordon Bell's life. The MyLifeBits system is designed to store and manage a lifetime's worth of data. MyLifeBits enables the capture of web pages, telephone, radio and television. This demonstration highlights the application of typed links and database features to make a lifetime store something that is truly useful.},
address = {New York, New York, USA},
author = {Gemmell, Jim and Lueder, Roger and Bell, Gordon},
booktitle = {Proceedings of the 2003 ACM SIGMM workshop on Experiential telepresence ETP 03},
doi = {10.1145/982484.982500},
file = {:home/abetan16/Dropbox/MendeleyV3/Gemmell, Lueder, Bell - 2002 - The Mylifebits Lifetime Store.pdf:pdf},
isbn = {1581137753},
pages = {80--83},
publisher = {ACM Press},
title = {{The MyLifeBits lifetime store}},
url = {http://portal.acm.org/citation.cfm?doid=982484.982500},
year = {2003}
}
@inproceedings{Gemmell2004,
abstract = {Passive capture lets people record their experiences without having to operate recording equipment, and without even having to give recording conscious thought. The advantages are increased capture, and improved participation in the event itself. However, passive capture also presents many new challenges. One key challenge is how to deal with the increased volume of media for retrieval, browsing, and organizing. This paper describes the SenseCam device, which combines a camera with a number of sensors in a pendant worn around the neck. Data from SenseCam is uploaded into a MyLifeBits repository, where a number of features, but especially correlation and relationships, are used to manage the data.},
address = {New York, NY},
author = {Gemmell, Jim and Williams, Lyndsay and Wood, Ken and Lueder, Roger and Bell, Gordon},
booktitle = {Proceedings of the the 1st ACM workshop on Continuous archival and retrieval of personal experiences CARPE04},
doi = {10.1145/1026653.1026660},
file = {:home/abetan16/Dropbox/MendeleyV3/Gemmell et al. - 2004 - Passive Capture and Ensuing Issues for a Personal Lifetime Store.pdf:pdf},
isbn = {1581139322},
keywords = {database,multimedia,mylifebits,photo,sensecam},
number = {October},
pages = {48--55},
title = {{Passive capture and ensuing issues for a personal lifetime store}},
url = {http://portal.acm.org/citation.cfm?doid=1026653.1026660},
year = {2004}
}
@article{Ghahramani2001,
abstract = {We provide a tutorial on learning and inference in hidden Markov models in the context of the recent literature on Bayesian networks. This perspective makes it possible to consider novel generalizations of hidden Markov models with multiple hidden state variables, multiscale representations, and mixed discrete and continuous variables. Although exact inference in these generalizations is usually intractable, one can use approximate inference algorithms such as Markov chain sampling and variational methods. We describe how such methods are applied to these generalized hidden Markov models. We conclude this review with a discussion of Bayesian methods for model selection in generalized HMMs.},
author = {GHAHRAMANI, ZOUBIN},
doi = {10.1142/S0218001401000836},
file = {:home/abetan16/Dropbox/MendeleyV3/Ghahramani - 2001 - An Introduction to Hidden Markov Models and Bayesian networks.pdf:pdf},
isbn = {981-02-4564-5},
issn = {0218-0014},
journal = {International Journal of Pattern Recognition and Artificial Intelligence},
number = {01},
pages = {9--42},
pmid = {18428778},
title = {{an Introduction To Hidden Markov Models and Bayesian Networks}},
url = {http://www.worldscientific.com/doi/pdf/10.1142/S0218001401000836},
volume = {15},
year = {2001}
}
@article{Girault2010,
abstract = {The aim of this study was to investigate the effects of a linear filter on the regularity of a given stochastic process in terms of the fractal dimension. This general approach, described in a continuous time domain, is new and is characterized by its simplicity. The framework of this problem is general since it emerges when a fractal process undertakes a transformation, as is the case in denoising or measurement processes.},
author = {Girault, J and Kouam{\'{e}}, D and Ouahabi, A},
doi = {10.1016/j.sigpro.2010.03.019},
file = {:home/abetan16/Dropbox/MendeleyV3/Girault, Kouam{\'{e}}, Ouahabi - 2010 - Analytical Formulation of the Fractal Dimension of Filtered Stochastic Signals.pdf:pdf},
isbn = {0165-1684},
issn = {01651684},
journal = {Signal Processing},
month = {sep},
number = {9},
pages = {2690--2697},
title = {{Analytical Formulation of the Fractal Dimension of Filtered Stochastic Signals}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0165168410001210},
volume = {90},
year = {2010}
}
@misc{Goble2008,
abstract = {Asymmetries in upper limb performance are a fundamental aspect of human behavior. This phenomenon, commonly known as handedness, has inspired a great deal of research over the course of the past century garnering interest across a multitude of scientific domains. In the present paper, a thorough review of this literature is provided focusing on the current state of knowledge regarding neuro-anatomical and behavior-based arm asymmetries. It is hoped that this information will provide a basis for new insights regarding the design and implementation of future studies regarding arm laterality.},
author = {Goble, Daniel J. and Brown, Susan H.},
booktitle = {Neuroscience and Biobehavioral Reviews},
doi = {10.1016/j.neubiorev.2007.10.006},
file = {:home/abetan16/Dropbox/MendeleyV3/Goble, Brown - 2008 - The biological and behavioral basis of upper limb asymmetries in sensorimotor performance.pdf:pdf},
isbn = {0149-7634 (Print)$\backslash$r0149-7634 (Linking)},
issn = {01497634},
keywords = {Anatomical structure,Handedness,Laterality,Motor control,Sensorimotor function},
number = {3},
pages = {598--610},
pmid = {18160103},
title = {{The biological and behavioral basis of upper limb asymmetries in sensorimotor performance}},
volume = {32},
year = {2008}
}
@article{Goldenberg1996,
abstract = {OBJECTIVES: Defective imitation of meaningless gestures has repeatedly been demonstrated in patients with apraxia and has been interpreted as being due to a deficit of motor execution. There is, however, controversy as to whether some impairment of imitation also occurs in patients with right brain damage. The aim was to compare defective imitation in patients with left and right brain damage and to explore whether there are qualitative differences between them. METHODS: Imitation was examined in 80 patients with left brain damage (LBD) and aphasia, 40 patients with right brain damage (RBD), and 60 controls for three types of gestures:hand positions, finger configurations, and combined gestures which required a defined hand position as well as a defined configuration of the fingers. RESULTS: Regardless of whether imitation of hand positions and finger configurations were tested each on their own or together, they showed differential susceptibility to RBD and LBD. Whereas imitation of finger configurations was about equally impaired in RBD and LBD, defective imitation of hand positions occurred almost exclusively in patients with LBD, and whereas controls as well as patients with RBD committed less errors with hand positions than with finger configurations, the reverse was the case in patients with LBD. CONCLUSIONS: The pattern of results goes against a deficit of motor execution as being the cause of defective imitation in patients with LBD, as it is difficult to see why such a deficit should affect proximal movements necessary for reaching hand positions more than differential finger movements. An alternative explanation would be that in patients with LBD errors are due to defective mediation by knowledge about the human body whereas in patients with RBD they stem from faulty visuospatial analysis of the demonstrated gesture.},
author = {Goldenberg, G},
doi = {10.1136/jnnp.61.2.176},
file = {:home/abetan16/Dropbox/MendeleyV3/Goldenberg - 1996 - Defective imitation of gestures in patients with damage in the left or right hemispheres.pdf:pdf},
isbn = {0028-3932 (Print) 0028-3932 (Linking)},
issn = {0022-3050},
journal = {Journal of neurology, neurosurgery, and psychiatry},
keywords = {aphasia,apraxia,configurations were obtunded,motor control,visuospatial tions and finger},
number = {2},
pages = {176--180},
pmid = {8708686},
title = {{Defective imitation of gestures in patients with damage in the left or right hemispheres.}},
volume = {61},
year = {1996}
}
@article{Goldman2006,
abstract = {We present a method for visualizing short video clips in a single static image, using the visual language of storyboards. These schematic storyboards are composed from multiple input frames and annotated using outlines, arrows, and text describing the motion in the scene. The principal advantage of this storyboard representation over standard representations of video – generally either a static thumbnail image or a playback of the video clip in its entirety – is that it requires only a moment to observe and comprehend but at the same time retains much of the detail of the source video},
author = {Goldman, Dan B and Curless, Brian and Salesin, David and Seitz, Steven M.},
doi = {10.1145/1141911.1141967},
file = {:home/abetan16/Dropbox/Articles/Mendeley/Goldman, Curless - 2006 - Schematic Storyboarding for Video Visualization and Editing.pdf:pdf},
isbn = {1-59593-364-6},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {storyboards,tion,video editing,video interaction,video summarization,video visualiza-},
number = {3},
pages = {862},
pmid = {424938664808875751},
title = {{Schematic storyboarding for video visualization and editing}},
url = {http://dl.acm.org/citation.cfm?id=1141967},
volume = {25},
year = {2006}
}
@incollection{Gong2012,
abstract = {We address the problem of unsupervised online segmenting human motion sequences into different actions. Kernelized Temporal Cut (KTC), is pro- posed to sequentially cut the structured sequential data into different regimes. KTC extends previous works on online change-point detection by incorporating Hilbert space embedding of distributions to handle the nonparametric and high dimensionality issues. Based on KTC, a realtime online algorithm and a hier- archical extension are proposed for detecting both action transitions and cyclic motions at the same time.We evaluate and compare the approach to state-of-the- art methods on motion capture data, depth sensor data and videos. Experimental results demonstrate the effectiveness of our approach, which yields realtime seg- mentation, and produces higher action segmentation accuracy. Furthermore, by combining with sequence matching algorithms, we can online recognize actions of an arbitrary person from an arbitrary viewpoint, given realtime depth sensor input},
author = {Gong, Dian and Medioni, G{\'{e}}rard and Zhu, Sikai and Zhao, Xuemei},
booktitle = {Eccv},
doi = {10.1007/978-3-642-33712-3_17},
file = {:home/abetan16/Dropbox/MendeleyV3/Gong et al. - 2012 - Kernelized Temporal Cut for Online Temporal Segmentation and Recognition.pdf:pdf},
isbn = {9783642337116},
issn = {03029743},
pages = {229--243},
title = {{Kernelized temporal cut for online temporal segmentation and recognition}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-33712-3{\_}17},
year = {2012}
}
@inproceedings{GonzalezDiaz2013,
abstract = {In this paper we study the problem of recognizing Instrumental Activities of Daily Living (IADL) in egocentric camera view. The target application of this research is the indexing of videos of patients with Alzehimer disease, thus providing medical staff with fast access and easy navigation through the video contents and helping them while assessing patients' abilities to perform IADL. Driven by the consideration that an activity in egocentric videos can be defined as a sequence of interacted objects inside different rooms, we present a novel representation based on the output of object and room detectors over temporal segments. In addition, our object detection approach is extended by automatic detection of visually salient regions since distinguishing active objects from context has been proven to dramatically improve performances in egocentric ADL recognition. We have assessed our proposal on a publicly available egocentric dataset and show extensive experimental results that demonstrate our approach outperforms current state of the art for unconstrained scenarios in which training and testing environments may be notably different.},
address = {New York, New York, USA},
author = {{Gonz{\'{a}}lez D{\'{i}}az}, Iv{\'{a}}n and Buso, Vincent and Benois-Pineau, Jenny and Bourmaud, Guillaume and Megret, R{\'{e}}mi},
booktitle = {International workshop on Multimedia indexing and information retrieval for healthcare},
doi = {10.1145/2505323.2505328},
file = {:home/abetan16/Dropbox/MendeleyV3/Gonz{\'{a}}lez D{\'{i}}az et al. - 2013 - Modeling Instrumental Activities of Daily Living in Egocentric Vision as Sequences of Active Objects and.pdf:pdf},
isbn = {9781450323987},
keywords = {action recognition,context,egocentric vision,object recogni-,place recognition,tion},
pages = {11--14},
publisher = {ACM Press},
title = {{Modeling Instrumental Activities of Daily Living in Egocentric Vision as Sequences of Active Objects and Context for Alzheimer Disease Research}},
url = {http://dl.acm.org/citation.cfm?doid=2505323.2505328},
year = {2013}
}
@book{Malacara2005,
abstract = {For courses in Image Processing and Computer Vision. Completely self-contained-and heavily illustrated-this introduction to basic concepts and methodologies for digital image processing is written at a level that truly is suitable for seniors and first-year graduate students in almost any technical discipline. The leading textbook in its field for more than twenty years, it continues its cutting-edge focus on contemporary developments in all mainstream areas of image processing-e.g., image fundamentals, image enhancement in the spatial and frequency domains, restoration, color image processing, wavelets, image compression, morphology, segmentation, image description, and the fundamentals of object recognition. It focuses on material that is fundamental and has a broad scope of application.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Gonzalez, Rafael C.. and Woods, Richard Eugene},
doi = {10.1049/ep.1978.0474},
edition = {3rd Editio},
editor = {Pearson},
eprint = {arXiv:1011.1669v3},
file = {:home/abetan16/Dropbox/MendeleyV3/Gonzalez, Woods - 2010 - Digital Image Processing.pdf:pdf},
isbn = {0132345633},
issn = {10833668},
month = {mar},
pages = {976},
pmid = {21707695},
title = {{Digital Image Processing}},
url = {https://books.google.com/books?id=lDojQwAACAAJ{\&}pgis=1},
year = {2010}
}
@article{Goyal2014,
abstract = {Recognition of objects using Deep Neural Networks is an active area of research and many breakthroughs have been made in the last few years. The paper attempts to indicate how far this field has progressed. The paper briefly describes the history of research in Neural Networks and describe several of the recent advances in this field. The performances of recently developed Neural Network Algorithm over benchmark datasets have been tabulated. Finally, some the applications of this field have been provided.},
archivePrefix = {arXiv},
arxivId = {1412.3684},
author = {Goyal, Soren and Benjamin, Paul},
eprint = {1412.3684},
file = {:home/abetan16/Dropbox/MendeleyV3/Goyal, Benjamin - 2014 - Object Recognition Using Deep Neural Networks A Survey.pdf:pdf},
isbn = {0465029973 0465029973 0465001041},
month = {dec},
pages = {1--7},
title = {{Object Recognition Using Deep Neural Networks: A Survey}},
url = {http://arxiv.org/abs/1412.3684},
year = {2014}
}
@inproceedings{Grasset2012,
abstract = {No one is citing this so far, last checked: April 2013.},
author = {Grasset, Raphael and Langlotz, Tobias and Kalkofen, Denis and Tatzgern, Markus and Schmalstieg, Dieter},
booktitle = {ISMAR 2012 - 11th IEEE International Symposium on Mixed and Augmented Reality 2012, Science and Technology Papers},
doi = {10.1109/ISMAR.2012.6402555},
file = {:home/abetan16/Dropbox/MendeleyV3/Grasset, Langlotz, Kalkofen - 2012 - Image-driven View Management for Augmented Reality Browsers.pdf:pdf},
isbn = {9781467346603},
keywords = {H.5.1 [Information Interfaces and Presentation]: M,H.5.2 [Information Interfaces and Presentation]: U},
month = {nov},
pages = {177--186},
publisher = {Ieee},
title = {{Image-driven view management for augmented reality browsers}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6402555 http://www.academia.edu/download/30247142/Image-Driven{\_}View{\_}Management{\_}for{\_}Augmented{\_}Reality{\_}Browsers{\_}.pdf},
year = {2012}
}
@inproceedings{Grauman2005,
abstract = {Discriminative learning is challenging when examples are sets of features, and the sets vary in cardinality and lack any sort of meaningful ordering. Kernel-based classification methods can learn complex decision boundaries, but a kernel over unordered set inputs must somehow solve for correspondences epsivnerally a computationally expensive task that becomes impractical for large set sizes. We present a new fast kernel function which maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in this space. This "pyramid match" computation is linear in the number of features, and it implicitly finds correspondences based on the finest resolution histogram cell where a matched pair first appears. Since the kernel does not penalize the presence of extra features, it is robust to clutter. We show the kernel function is positive-definite, making it valid for use in learning algorithms whose optimal solutions are guaranteed only for Mercer kernels. We demonstrate our algorithm on object recognition tasks and show it to be accurate and dramatically faster than current approaches},
address = {Beijing},
author = {Grauman, Kristen and Darrell, Trevor},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2005.239},
file = {:home/abetan16/Dropbox/MendeleyV3/Grauman, Darrell - 2005 - The Pyramid Match Kernel Discriminative Classification with Sets of Image Features.pdf:pdf},
isbn = {076952334X},
issn = {1550-5499},
number = {October},
pages = {1458--1465},
publisher = {IEEE},
title = {{The pyramid match kernel: Discriminative classification with sets of image features}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1544890},
volume = {II},
year = {2005}
}
@incollection{Mayol2005a,
abstract = {In this paper, we discuss the ¯eld of sampling-based motion planning. In contrast to methods that con- struct boundary representations of con¯guration space obstacles, sampling-based methods use only information from a collision detector as they search the con¯guration space. The simplicity of this approach, along with in- creases in computation power and the development of ef- ¯cient collision detection algorithms, has resulted in the introduction of a number of powerful motion planning algorithms, capable of solving challenging problems with many degrees of freedom. First, we trace how sampling- based motion planning has developed. We then discuss a variety of important issues for sampling-based motion planning, including uniform and regular sampling, topo- logical issues, and search philosophies. Finally, we ad- dress important issues regarding the role of randomiza- tion in sampling-based motion planning.},
address = {Berlin, Heidelberg},
author = {Gravot, Fabien and Cambon, Stephane and Alami, Rachid},
booktitle = {Robotics Research, The Eleventh International Symposium},
doi = {10.1007/b97958},
editor = {Dario, Paolo and Chatila, Raja},
file = {:home/abetan16/Dropbox/MendeleyV3/Mayol et al. - 2005 - Applying Active Vision and SLAM to Wearables.pdf:pdf},
isbn = {9783540232148},
issn = {16107438},
number = {1},
pages = {100--110},
publisher = {Springer Berlin Heidelberg},
series = {Springer Tracts in Advanced Robotics},
title = {{Robotics Research The Eleventh International Symposium}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.137.6106{\&}rep=rep1{\&}type=pdf$\backslash$nhttp://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.137.6106},
volume = {15},
year = {2005}
}
@article{Guan2011,
abstract = {Activity recognition (AR) has become a hot research topic due to its strength in providing personalized support for many diverse applications such as healthcare and security. Due to its importance, a considerable amount of AR systems have been developed. In general, these systems utilize diverse sensors to obtain the activity related information, which are then used by machine learning techniques to infer human's ongoing activity. According to the types of sensors used, existing AR systems can be roughly divided into two categories: 1. Video sensor based AR. It remotely observes human activity using video sensors; 2. Physical sensor based AR (PSAR). It attaches physical sensors to the body of human or objects (appliances) to infer human activity. Based on the location of attached sensors, PSAR consists of two subcategories: Wearable sensor based AR and object usage based AR. In this work, different types of AR are reviewed. We think this review is significant because no existing review papers include all the different types of AR as a whole. For each type of AR, its main techniques, characteristics, strengths and limitations are discussed and summarized. We also make a comparative analysis for them. Finally the main research challenges in AR are pointed out.},
author = {Guan, Donghai and Yuan, Weiwei and {Jehad Sarkar}, Am and Ma, Tinghuai and Lee, Young-Koo},
doi = {10.4103/0256-4602.85975},
file = {:home/abetan16/Dropbox/MendeleyV3/Guan et al. - 2011 - Review of Sensor-based Activity Recognition Systems.pdf:pdf},
isbn = {0256-4602},
issn = {0256-4602},
journal = {IETE Technical Review},
keywords = {activity recognition,object usage,physical sensor,video sensor,wearable sensor},
number = {5},
pages = {418},
title = {{Review of Sensor-based Activity Recognition Systems}},
url = {http://tr.ietejournals.org/text.asp?2011/28/5/418/85975},
volume = {28},
year = {2011}
}
@inproceedings{Gunes2010,
abstract = {This paper focuses on dimensional prediction of emotions from spontaneous conversational head gestures. It maps the amount and direction of head motion, and occurrences of head nods and shakes into arousal, expectation, intensity, power and valence level of the observed subject as there has been virtually no research bearing on this topic. Preliminary experiments show that it is possible to automatically predict emotions in terms of these five dimensions (arousal, expectation, intensity, power and valence) from conversational head gestures. Dimensional and continuous emotion prediction from spontaneous head gestures has been integrated in the SEMAINE project [1] that aims to achieve sustained emotionally-colored interaction between a human user and Sensitive Artificial Listeners.},
author = {Gunes, Hatice and Pantic, Maja},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-15892-6_39},
file = {:home/abetan16/Dropbox/MendeleyV3/Gunes, Pantic - 2010 - Dimensional Emotion Prediction from Spontaneous Head Gestures for Interaction with Sensitive Artificial Listeners.pdf:pdf},
isbn = {3642158919},
issn = {03029743},
keywords = {dimensional emotion prediction,spontaneous head movements,virtual character-human interaction},
pages = {371--377},
title = {{Dimensional emotion prediction from spontaneous head gestures for interaction with sensitive artificial listeners}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-15892-6{\_}39},
volume = {6356 LNAI},
year = {2010}
}
@article{Gupta2009a,
abstract = {Interpretation of images and videos containing humans interacting with different objects is a daunting task. It involves understanding scene/event, analyzing human movements, recognizing manipulable objects, and observing the effect of the human movement on those objects. While each of these perceptual tasks can be conducted independently, recognition rate improves when interactions between them are considered. Motivated by psychological studies of human perception, we present a Bayesian approach which integrates various perceptual tasks involved in understanding human-object interactions. Previous approaches to object and action recognition rely on static shape/appearance feature matching and motion analysis, respectively. Our approach goes beyond these traditional approaches and applies spatial and functional constraints on each of the perceptual elements for coherent semantic interpretation. Such constraints allow us to recognize objects and actions when the appearances are not discriminative enough. We also demonstrate the use of such constraints in recognition of actions from static images without using any motion information.},
author = {Gupta, Abhinav and Kembhavi, Aniruddha and Davis, Larry S.},
doi = {10.1109/TPAMI.2009.83},
file = {:home/abetan16/Dropbox/MendeleyV3//Gupta, Kembhavi, Davis - 2009 - Observing Human-object Interactions Using Spatial and Functional Compatibility for Recognition.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Action recognition,Functional recognition,Object recognition},
month = {oct},
number = {10},
pages = {1775--1789},
pmid = {19696449},
title = {{Observing human-object interactions: Using spatial and functional compatibility for recognition}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19696449},
volume = {31},
year = {2009}
}
@incollection{Gygli2014,
abstract = {This paper proposes a novel approach and a new benchmark for video summarization. Thereby we focus on user videos, which are raw videos containing a set of interesting events. Our method starts by segmenting the video by using a novel “superframe” segmentation, tailored to raw videos. Then, we estimate visual interestingness per superframe using a set of low-, mid- and high-level features. Based on this scoring, we select an optimal subset of superframes to create an informative and interesting summary. The introduced benchmark comes with multiple human created summaries, which were acquired in a controlled psychological experiment. This data paves the way to evaluate summarization methods objectively and to get new insights in video summarization. When evaluating our method, we find that it generates high-quality results, comparable to manual, human-created summaries.},
author = {Gygli, Michael and Grabner, Helmut},
booktitle = {Computer Vision–ECCV {\ldots}},
doi = {10.1007/978-3-319-10584-0_33},
file = {:home/abetan16/Dropbox/MendeleyV3/Gygli, Grabner - 2014 - Creating Summaries from User Videos.pdf:pdf},
isbn = {9783319105840},
keywords = {temporal segmentation,video analysis,video summarization},
pages = {505--520},
title = {{Creating Summaries from User Videos}},
url = {http://link.springer.com/chapter/10.1007/978-3-319-10584-0{\_}33},
year = {2014}
}
@inproceedings{Han2014,
address = {New York, New York, USA},
author = {Han, Seungyeop and Nandakumar, Rajalakshmi and Philipose, Matthai and Krishnamurthy, Arvind and Wetherall, David},
booktitle = {Proceedings of the 2014 workshop on physical analytics - WPA '14},
doi = {10.1145/2611264.2611269},
file = {:home/abetan16/Dropbox/MendeleyV3/Han et al. - 2014 - GlimpseData Towards Continuous Vision-based Personal Analytics.pdf:pdf},
isbn = {9781450328258},
keywords = {categories and subject descriptors,continuous mobile perception,detection,face,filtering,mobile phone sensors},
number = {2},
pages = {31--36},
publisher = {ACM Press},
title = {{GlimpseData}},
url = {http://dl.acm.org/citation.cfm?doid=2611264.2611269},
volume = {40},
year = {2014}
}
@article{Hanheide2006,
abstract = {Enabling artificial systems to recognize human actions is a requisite to develop intelligent assistance systems that are able to instruct and supervise users in accomplishing tasks. In order to enable an assistance system to be wearable, head-mounted cameras allow to perceive a scene visually from a user's perspective. But realizing action recognition without any static sensors causes special challenges. The movement of the camera is directly related to the user's head motion and not controlled by the system. In this paper we present how a trajectory-based action recognition can be combined with object recognition, visual tracking, and a background motion compensation to be applicable in such a wearable assistance system. The suitability of our approach is proved by user studies in an object manipulation scenario},
author = {Hanheide, M. Hanheide M. and Hofemann, N. Hofemann N. and Sagerer, G. Sagerer G.},
doi = {10.1109/ICPR.2006.204},
file = {:home/abetan16/Dropbox/MendeleyV3/Hanheide, Hofemann, Sagerer - 2006 - Action Recognition in a Wearable Assistance System.pdf:pdf},
isbn = {0-7695-2521-0},
issn = {1051-4651},
journal = {Icpr},
pages = {18--21},
title = {{Action Recognition in aWearable Assistance System}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1699437},
volume = {2},
year = {2006}
}
@article{Hantrakul2014,
address = {New York, New York, USA},
author = {Hantrakul, Lamtharn and Kaczmarek, Konrad},
doi = {10.1145/2617995.2618020},
file = {:home/abetan16/Dropbox/MendeleyV3/Hantrakul, Kaczmarek - 2014 - Implementations of the Leap Motion device in sound synthesis and interactive live performance.pdf:pdf},
isbn = {978-1-4503-2814-2},
journal = {Proceedings of the International Workshop on Movement and Computing},
keywords = {gestural control,leap motion,music expressivity},
pages = {142--145},
publisher = {ACM Press},
title = {{Implementations of the Leap Motion device in sound synthesis and interactive live performance}},
url = {http://dl.acm.org/citation.cfm?doid=2617995.2618020},
year = {2014}
}
@article{Harvey2016,
abstract = {Abstract Human memory is unquestionably a vital cognitive ability but one that can often be unreliable. External memory aids such as diaries, photos, alarms and calendars are often employed to assist in remembering important events in our past and future. The recent trend for lifelogging, continuously documenting ones life through wearable sensors and cameras, presents a clear opportunity to augment human memory beyond simple reminders and actually improve its capacity to remember. This article surveys work from the fields of computer science and psychology to understand the potential for such augmentation, the technologies necessary for realising this opportunity and to investigate what the possible benefits and ethical pitfalls of using such technology might be.},
author = {Harvey, Morgan and Langheinrich, Marc and Ward, Geoff},
doi = {http://dx.doi.org/10.1016/j.pmcj.2015.12.002},
file = {:home/abetan16/Dropbox/MendeleyV3/Harvey, Langheinrich, Ward - 2016 - Remembering through lifelogging A survey of human memory augmentation.pdf:pdf},
issn = {1574-1192},
journal = {Pervasive and Mobile Computing},
keywords = {Augmented human memory,Lifelogging,Personal life archives},
pages = {14--26},
publisher = {Elsevier B.V.},
title = {{Remembering through lifelogging: A survey of human memory augmentation}},
url = {http://www.sciencedirect.com/science/article/pii/S157411921500214X},
volume = {27},
year = {2016}
}
@book{Hastie2009,
abstract = {During the past decade there has been an explosion in computation and information technology. With it has come a vast amount of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics.},
author = {Hastie, Trevor and Tibshirani, Robert J. and Friedman, Jerome},
booktitle = {Elements},
doi = {10.1007/b94608},
edition = {10},
file = {:home/abetan16/Dropbox/MendeleyV3/Hastie, Tibshirani, Friedman - 2009 - The Elements of Statistical Learning.pdf:pdf},
isbn = {9780387848570},
issn = {03436993},
pages = {745},
pmid = {15512507},
publisher = {Springer},
title = {{The Elements of Statistical Learning}},
url = {http://www.springerlink.com/index/D7X7KX6772HQ2135.pdf},
volume = {1},
year = {2009}
}
@article{Hayhoe2005,
abstract = {The classic experiments of Yarbus over 50 years ago revealed that saccadic eye movements reflect cognitive processes. But it is only recently that three separate advances have greatly expanded our understanding of the intricate role of eye movements in cognitive function. The first is the demonstration of the pervasive role of the task in guiding where and when to fixate. The second has been the recognition of the role of internal reward in guiding eye and body movements, revealed especially in neurophysiological studies. The third important advance has been the theoretical developments in the fields of reinforcement learning and graphic simulation. All of these advances are proving crucial for understanding how behavioral programs control the selection of visual information.},
author = {Hayhoe, Mary and Ballard, Dana},
doi = {10.1016/j.tics.2005.02.009},
file = {:home/abetan16/Dropbox/MendeleyV3/Hayhoe, Ballard - 2005 - Eye Movements in Natural Behavior.pdf:pdf},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
keywords = {Ambulatory,Ambulatory: instrumentation,Ambulatory: methods,Behavior,Behavior: physiology,Cognition,Cognition: physiology,Eye Movements,Eye Movements: physiology,Humans,Monitoring,Ocular,Ocular: physiology,Vision},
month = {apr},
number = {4},
pages = {188--194},
pmid = {15808501},
title = {{Eye movements in natural behavior}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1364661305000598},
volume = {9},
year = {2005}
}
@article{Healey2000,
abstract = {Smart physiological sensors embedded in an automobile afford a$\backslash$nnovel opportunity to capture naturally occurring episodes of driver$\backslash$nstress. In a series of ten ninety minute drives on public roads and$\backslash$nhighways, ECG, EMG, respiration and skin conductance sensors were used$\backslash$nto measure the autonomic nervous system activation. The signals were$\backslash$ndigitized in real time and stored on the SmartCar's Pentium class$\backslash$ncomputer. Each drive followed a pre-specified route through fifteen$\backslash$ndifferent events, from which four stress level categories were created$\backslash$naccording to the results of the subjects self report questionnaires. In$\backslash$ntotal, 545 one minute segments were classified. A linear discriminant$\backslash$nfunction was used to rank each feature individually based on the$\backslash$nrecognition performance, and a sequential forward floating selection$\backslash$nalgorithm was used to find an optimal set of features for recognizing$\backslash$npatterns of driver stress. Using multiple features improved performance$\backslash$nsignificantly over the best single feature performance},
author = {Healey, J. and Picard, R.},
doi = {10.1109/ICPR.2000.902898},
file = {:home/abetan16/Dropbox/MendeleyV3/Healey, Picard - 2000 - SmartCar Detecting Driver Stress.pdf:pdf},
isbn = {0-7695-0750-6},
issn = {1051-4651},
journal = {Proceedings 15th International Conference on Pattern Recognition. ICPR-2000},
pages = {218--221},
publisher = {IEEE Comput. Soc},
title = {{SmartCar: detecting driver stress}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=902898 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=902898},
volume = {4},
year = {2000}
}
@article{Herold2003,
author = {Herold, Martin and Goldstein, Noah C. and Clarke, Keith C.},
doi = {10.1016/S0034-4257(03)00075-0},
file = {:home/abetan16/Dropbox/MendeleyV3/Herold, Goldstein, Clarke - 2003 - The spatiotemporal form of urban growth measurement, analysis and modeling.pdf:pdf},
issn = {00344257},
journal = {Remote Sensing of Environment},
keywords = {aerial photography,ikonos,modeling,spatial metrics,urban growth},
month = {aug},
number = {3},
pages = {286--302},
title = {{The spatiotemporal form of urban growth: measurement, analysis and modeling}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0034425703000750},
volume = {86},
year = {2003}
}
@inproceedings{Hodges2006,
abstract = {This paper presents a novel ubiquitous computing device, the Sense- Cam, a sensor augmented wearable stills camera. SenseCam is designed to capture a digital record of the wearer's day, by recording a series of images and capturing a log of sensor data. We believe that reviewing this information will help the wearer recollect aspects of earlier experiences that have subsequently been forgotten, and thereby form a powerful retrospective memory aid. In this paper we review existing work on memory aids and conclude that there is scope for an improved device. We then report on the design of SenseCam in some detail for the first time. We explain the details of a first in-depth user study of this device, a 12-month clinical trial with a patient suffering from amnesia. The results of this initial evaluation are extremely promising; periodic review of images of events recorded by SenseCam results in significant recall of those events by the patient, which was previously impossible. We end the paper with a discussion of future work, including the application of SenseCam to a wider audience, such as those with neurodegenerative conditions such as Alzheimer's disease.},
author = {Hodges, Steve. and Williams, Lyndsay. and Berry, Emma. and Izadi, Shahram. and Srinivasan, James. and Bulter, Alex. and Smyth, Gavin. and Kapur, Narinder. and Wood, Ken.},
booktitle = {UbiComp 2006: Ubiquitous Computing},
doi = {10.1007/11853565},
file = {:home/abetan16/Dropbox/MendeleyV3/Hodges et al. - 2006 - Sensecam a Retrospective Memory Aid.pdf:pdf},
isbn = {978-3-540-39634-5},
issn = {03029743},
keywords = {BF Psychology},
pages = {177--193},
publisher = {Springer Verlag},
title = {{SenseCam: a retrospective memory aid}},
url = {http://eprints.soton.ac.uk/55428/},
year = {2006}
}
@inproceedings{Hoiem2005,
abstract = {Many computer vision algorithms limit their performance by ignoring the underlying 3D geometric structure in the image. We show that we can estimate the coarse geometric properties of a scene by learning appearance-based mod- els of geometric classes, even in cluttered natural scenes. Geometric classes describe the 3D orientation of an image region with respect to the camera. We provide a multiple- hypothesis framework for robustly estimating scene struc- ture from a single image and obtaining confidences for each geometric label. These confidences can then be used to im- prove the performance of many other applications. We pro- vide a thorough quantitative evaluation of our algorithm on a set of outdoor images and demonstrate its usefulness in two applications: object detection and automatic single- view reconstruction.},
author = {Hoiem, D. and Efros, A.A. and Hebert, M.},
booktitle = {Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1},
doi = {10.1109/ICCV.2005.107},
file = {:home/abetan16/Dropbox/MendeleyV3/Hoiem, Efros, Hebert - 2005 - Geometric context from a single image.pdf:pdf},
isbn = {0-7695-2334-X},
pages = {654--661 Vol. 1},
publisher = {Ieee},
title = {{Geometric context from a single image}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1541316},
year = {2005}
}
@book{Holland1975,
abstract = {An abstract is not available.},
archivePrefix = {arXiv},
arxivId = {0262082136},
author = {Holland, John},
doi = {citeulike-article-id:1281572},
eprint = {0262082136},
isbn = {0-262-58111-6},
issn = {10834419},
pages = {211},
pmid = {15369078},
publisher = {University of Michigan Press},
title = {{Adaptation in Natural and Artificial Systems}},
year = {1992}
}
@article{Hongeng2004,
abstract = {We present a new representation and recognition method for human activities. An activity is considered to be composed of action threads, each thread being executed by a single actor. A single-thread action is represented by a stochastic finite automaton of event states, which are recognized from the characteristics of the trajectory and shape of moving blob of the actor using Bayesian methods. A multi-agent event is composed of several action threads related by temporal constraints. Multi-agent events are recognized by propagating the constraints and likelihood of event threads in a temporal logic network. We present results on real-world data and performance characterization on perturbed data.},
author = {Hongeng, Somboon and Nevatia, Ram and Bremond, Francois},
doi = {10.1016/j.cviu.2004.02.005},
file = {:home/abetan16/Dropbox/MendeleyV3/Hongeng, Nevatia, Bremond - 2004 - Video-based Event Recognition Activity Representation and Probabilistic Recognition Methods.pdf:pdf},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {activity recognition,event mining,video-based event detection},
month = {nov},
number = {2},
pages = {129--162},
title = {{Video-based event recognition: activity representation and probabilistic recognition methods}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1077314204000712},
volume = {96},
year = {2004}
}
@inproceedings{Hori2003,
abstract = {Recently, we have often heard the terms "Wearable computing" and "Ubiquitous computing". Our expectation for the future of such new computing environments is growing. One of the characteristics of these computing environments is that they embed computers in our lives. In such environments, digitization of personal experiences will be made possible by continuous recordings using a wearable video camera6, 7. This could lead to the "automatic life-log application". However, it is evident that the resulting amount of video content will be enormous. Accordingly, to retrieve and browse desired scenes, a vast quantity of video data must be organized using structural information.In this paper, we attempt to develop a "context-based video retrieval system for life-log applications". This wearable system is capable of continuously capturing data not only from a wearable camera and a microphone, but also from various kinds of sensors such as a brain-wave analyzer, a GPS receiver, an acceleration sensor, and a gyro sensor to extract the user's contexts. In addition, the system provides functions that make efficient video browsing and retrieval possible by using data from these sensors and some databases. For example, we can use the following query using this system. "I talked with Kenji while walking at a shopping center in Shinjuku on a cloudy day in mid-May. The conversation was very interesting! I want to see the video of our outing to remember the contents of the conversation."},
address = {New York, New York, USA},
author = {Hori, Tetsuro and Aizawa, Kiyoharu},
booktitle = {Proceedings of the 5th ACM SIGMM international workshop on Multimedia information retrieval - MIR '03},
doi = {10.1145/973264.973270},
file = {:home/abetan16/Dropbox/MendeleyV3/Hori, Aizawa - 2003 - Context-based Video Retrieval System for the Life-log Applications.pdf:pdf},
isbn = {1581137788},
keywords = {video indexing,video retrieval,wearable computing},
pages = {31},
publisher = {ACM Press},
title = {{Context-based video retrieval system for the life-log applications}},
url = {http://portal.acm.org/citation.cfm?doid=973264.973270},
year = {2003}
}
@inproceedings{Horvitz1999,
abstract = {Recent debate has centered on the relative promise of focusing user-interface research on developing new metaphors and tools that enhance users' abilities to directly manipulate objects versus directing effort toward developing interface agents that provide automation. In this paper, we review principles that show promise for allowing engineers to enhance human-computer interaction through an elegant coupling of automated services with direct manipulation. Key ideas will be highlighted in terms of the Lookout system for scheduling and meeting management.},
address = {New York, New York, USA},
author = {Horvitz, Eric},
booktitle = {Acm Sigchi 1999},
doi = {10.1145/302979.303030},
file = {:home/abetan16/Dropbox/MendeleyV3/Horvitz - 1999 - Conference on Human Factors in Computing Systems.pdf:pdf},
isbn = {0201485591},
keywords = {agents,attention,direct manipu,mixed initiative},
number = {May},
pages = {159--166},
publisher = {ACM Press},
title = {{Principles of mixed-initiative user interfaces}},
url = {http://portal.acm.org/citation.cfm?doid=302979.303030},
year = {1999}
}
@inproceedings{Horvitz1999a,
abstract = {We introduce utility-directed procedures for mediating the flow of potentially distracting alerts and communications to computer users. We present models and inference procedures that balance the context-sensitive costs of deferring alerts with the cost of interruption. We describe the challenge of reasoning about such costs under uncertainty via an analysis of user activity and the content of notifications. After introducing principles of attention-sensitive alerting, we focus on the problem of guiding alerts about email messages. We dwell on the problem of inferring the expected criticality of email and discuss work on the PRIORITIES system, centering on prioritizing email by criticality and modulating the communication of notifications to users about the presence and nature of incoming email.},
address = {San Francisco, CA, USA},
author = {Horvitz, Eric and Jacobs, Andy and Hovel, David},
booktitle = {Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence (UAI'99)},
file = {:home/abetan16/Dropbox/Articles/Mendeley/Horvitz, Jacobs, Hovel - 1999 - Attention-Sensitive Alerting.pdf:pdf},
isbn = {1-55860-614-9},
pages = {305--313},
pmid = {4142},
publisher = {Morgan Kaufmann Publishers Inc},
title = {{Attention-Sensitive Alerting}},
url = {http://dl.acm.org/citation.cfm?id=2073831},
year = {1999}
}
@article{Hoshen2014,
abstract = {Egocentric cameras are being worn by an increasing number of users, among them many security forces worldwide. GoPro cameras already penetrated the mass market, reporting substantial increase in sales every year. As head-worn cameras do not capture the photographer, it may seem that the anonymity of the photographer is preserved even when the video is publicly distributed. We show that camera motion, as can be computed from the egocentric video, provides unique identity information. The photographer can be reliably recognized from a few seconds of video captured when walking. The proposed method achieves more than 90{\%} recognition accuracy in cases where the random success rate is only 3{\%}. Applications can include theft prevention by locking the camera when not worn by its rightful owner. Searching video sharing services (e.g. YouTube) for egocentric videos shot by a specific photographer may also become possible. An important message in this paper is that photographers should be aware that sharing egocentric video will compromise their anonymity, even when their face is not visible.},
archivePrefix = {arXiv},
arxivId = {1411.7591},
author = {Hoshen, Yedid and Peleg, Shmuel},
eprint = {1411.7591},
file = {:home/abetan16/Dropbox/MendeleyV3/Hoshen, Peleg - 2014 - Egocentric Video Biometrics.pdf:pdf},
journal = {arXiv preprint},
month = {nov},
title = {{An Egocentric Look at Video Photographer Identity}},
url = {http://arxiv.org/abs/1411.7591},
year = {2014}
}
@article{Hudlicka2002,
abstract = {We describe anAffect andBeliefAdaptive Interface System (ABAIS) designedto com- pensate for performance biases caused by users' affective states and active beliefs. The ABAIS architecture implements an adaptivemethodology consistingof four steps: sensing/inferringuser affective state and performance-relevant beliefs; identifying their potential impact on performance; selecting a compensatory strategy; and implementing this strategy in terms of speci¢cGUIadaptations.ABAISprovides a generic adaptive framework for integrating a variety of user assessment methods (e.g. knowledge-based, self-reports, diagnostic tasks, physiological sensing), and GUI adaptation strategies (e.g. content- and format-based).The ABAIS perform- ancebiasprediction isbasedon empirical¢ndingsfromemotion research combinedwith detailed knowledge of the task context.The initialABAISprototypewas demonstrated inthe context ofan Air Force combat task, used a knowledge-based approach to assess the pilot's anxiety level, and adapted to the pilot's anxiety and belief states by modifying selected cockpit instrument displays in response to detected changes in these states.},
author = {Hudlicka, Eva and McNeese, Michael D.},
doi = {10.1023/A:1013337427135},
file = {:home/abetan16/Dropbox/MendeleyV3/Hudlicka, McNeese - 2002 - Assessment of User Affective and Belief States for Interface Adaptation Application to an Air Force Pilot Tas.pdf:pdf},
isbn = {0924-1868},
issn = {09241868},
journal = {User Modelling and User-Adapted Interaction},
keywords = {Adaptive interface,Affect adaptation,Affect assessment,Affective computing,Aviation,Human-computer interaction,User modeling},
number = {1},
pages = {1--47},
title = {{Assessment of user affective and belief states for interface adaptation: Application to an Air Force pilot task}},
url = {http://link.springer.com/article/10.1023/A:1013337427135},
volume = {12},
year = {2002}
}
@article{Hwang2014,
abstract = {PURPOSE: Google Glass provides a platform that can be easily extended to include a vision enhancement tool. We have implemented an augmented vision system on Glass, which overlays enhanced edge information over the wearer's real-world view, to provide contrast-improved central vision to the Glass wearers. The enhanced central vision can be naturally integrated with scanning. METHODS: Google Glass' camera lens distortions were corrected by using an image warping. Because the camera and virtual display are horizontally separated by 16 mm, and the camera aiming and virtual display projection angle are off by 10°, the warped camera image had to go through a series of three-dimensional transformations to minimize parallax errors before the final projection to the Glass' see-through virtual display. All image processes were implemented to achieve near real-time performance. The impacts of the contrast enhancements were measured for three normal-vision subjects, with and without a diffuser film to simulate vision loss. RESULTS: For all three subjects, significantly improved contrast sensitivity was achieved when the subjects used the edge enhancements with a diffuser film. The performance boost is limited by the Glass camera's performance. The authors assume that this accounts for why performance improvements were observed only with the diffuser filter condition (simulating low vision). CONCLUSIONS: Improvements were measured with simulated visual impairments. With the benefit of see-through augmented reality edge enhancement, natural visual scanning process is possible and suggests that the device may provide better visual function in a cosmetically and ergonomically attractive format for patients with macular degeneration. Copyright {\textcopyright} 2014 American Academy of Optometry.},
author = {Hwang, a D and Peli, E},
doi = {10.1097/OPX.0000000000000326},
file = {:home/abetan16/Dropbox/MendeleyV3/Hwang, Peli - 2014 - An augmented-reality edge enhancement application for Google Glass.pdf:pdf},
isbn = {15389235 (ISSN)},
issn = {1538-9235},
journal = {Optometry and Vision Science},
keywords = {Augmented reality,Camera lens distortions,Cameras,Computer vision,Contrast enhancement,Geometrical optics,Glass,Google Glass,HMD,Helmet mounted displays,Low vision,Macular degeneration,Real time performance,Three-dimensional transformation,Vision enhancement,Vision rehabilitation},
number = {8},
pages = {1021--1030},
pmid = {24978871},
title = {{An augmented-reality edge enhancement application for Google glass}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84905079705{\&}partnerID=40{\&}md5=c06a4d8ee4cd31bb4048662b3761ff79},
volume = {91},
year = {2014}
}
@inproceedings{Hwang2013,
abstract = {We address the problem of contour detection via per-pixel classifications of edge point. To facilitate the process, the proposed approach leverages with DenseNet, an efficient implementation of multiscale convolutional neural networks (CNNs), to extract an informative feature vector for each pixel and uses an SVM classifier to accomplish contour detection. In the experiment of contour detection, we look into the effectiveness of combining per-pixel features from different CNN layers and verify their performance on BSDS500.},
archivePrefix = {arXiv},
arxivId = {1504.01989},
author = {Hwang, Jyh-Jing and Liu, Tyng-Luh},
eprint = {1504.01989},
file = {:home/abetan16/Dropbox/MendeleyV3/Hwang, Liu - 2015 - PIXEL-WISE DEEP LEARNING FOR CONTOUR DETECTION.pdf:pdf},
pages = {4--5},
title = {{Pixel-wise Deep Learning for Contour Detection}},
url = {http://arxiv.org/abs/1504.01989},
volume = {1},
year = {2015}
}
@inproceedings{Hwang2010,
abstract = {We introduce a method for image retrieval that leverages the implicit information about object importance conveyed by the list of keyword tags a person supplies for an image. We propose an unsupervised learning procedure based on Kernel Canonical Correlation Analysis that discovers the relationship between how humans tag images (e.g., the order in which words are mentioned) and the relative importance of objects and their layout in the scene. Using this discovered connection, we show how to boost accuracy for novel queries, such that the search results may more closely match the user's mental image of the scene being sought. We evaluate our approach on two datasets, and show clear improvements over both an approach relying on image features alone, as well as a baseline that uses words and image features, but ignores the implied importance cues.},
author = {Hwang, Sung Ju and Grauman, Kristen},
booktitle = {Bmvc},
doi = {10.5244/C.24.58},
file = {:home/abetan16/Dropbox/MendeleyV3/Hwang, Grauman - 2010 - Accounting for the Relative Importance of Objects in Image Retrieval.pdf:pdf},
isbn = {1-901725-40-5},
keywords = {accordingly,as a loose,can thus be exploited,context,events of significance and,for auto-annotation of regions,form of labels and,including learning their correspondence,leverage images with as-,ob-,reflect the objects and,researchers have explored a,sociated text,variety of ways to},
pages = {1--12},
publisher = {British Machine Vision Association},
title = {{Accounting for the Relative Importance of Objects in Image Retrieval}},
url = {http://www.cs.utexas.edu/{~}sjhwang/bmvc10.html},
year = {2010}
}
@inproceedings{Ikizler-cinbis2010,
abstract = {In many cases, human actions can be identified not only by the singular observation of the human body in motion, but also properties of the surrounding scene and the related objects. In this paper, we look into this problem and propose an approach for human action recognition that integrates multiple feature channels from several entities such as objects, scenes and people. We formulate the problem in a multiple instance learning (MIL) framework, based on multiple feature channels. By using a discriminative approach, we join multiple feature channels embedded to the MIL space. Our experiments over the large YouTube dataset show that scene and object information can be used to complement person features for human action recognition.},
address = {Heraklion, Crete, Greece},
author = {Ikizler-cinbis, Nazli and Sclaroff, Stan},
booktitle = {Computer Vision – ECCV 2010},
file = {:home/abetan16/Dropbox/MendeleyV3/Ikizler-cinbis, Sclaroff - 2010 - LNCS 6311 - Object, Scene and Actions Combining Multiple Features for Human Action Recognition.pdf:pdf},
pages = {494--507},
publisher = {Springerlink},
title = {{LNCS 6311 - Object, Scene and Actions: Combining Multiple Features for Human Action Recognition}},
year = {2010}
}
@article{Kurata2001a,
abstract = {In this paper, we discuss the development of wearable systems which we collectively term VizWear. Vision plays an important role in both people's and computer's understanding of contextual information, and the use of augmented reality (AR) techniques is a good way to show information intuitively. This is the basis of our research on wearable computer vision and visualization systems. Our wearable systems enable us run different vision tasks in real-time. We secribe a novel approach not only to sensing the wearer's position and direction, but also to displaying video frames overlaid with 2-D annotations related to the wearer's view. We have also developed a method for 3-D graphical overlay by applying object recognition techniques and Hand Mouse, which enables the wearer to interact directly with an AR environment. We also describe an efficient method of face registration using wearable active vision.},
author = {Interaction, Toward Human-centered and Vision, Wearable and Science, Advanced Industrial},
file = {:home/abetan16/Dropbox/MendeleyV3/Kurata, Okuma, Kourogi - 2001 - Vizwear Toward Human-centered Interaction Through Wearable Vision and Visualization.pdf:pdf},
journal = {Lecture Notes in Computer Science},
number = {1},
pages = {40--47},
title = {{1 Introduction}},
url = {http://link.springer.com/chapter/10.1007/3-540-45453-5{\_}6},
volume = {2195},
year = {1999}
}
@inproceedings{MacCormick2000,
abstract = {Partitioned sampling is a technique which was introduced in (MacCormick
and Blake, 1999) for avoiding the high cost of particle filters when
tracking more than one object. In fact this technique can reduce
the curse of dimensionality in other situations too. This paper describes
how to use partitioned sampling on articulated objects, obtaining
results that would be impossible with standard sampling methods.
A new concept relating to particle filters, termed the $\backslash$emph{\{}survival
rate{\}} is introduced, which sheds light on the efficacy of partitioned
sampling. The domain of articulated objects also highlights two important
features of partitioned sampling which are discussed here for the
first time: firstly, that the number of particles allocated to each
partition can be varied to obtain the maximum benefit from a fixed
computational resource; and secondly, that the number of likelihood
evaluations (the most expensive operation in vision-based particle
filters) required can be halved by taking advantage of the way the
likelihood function factorises for an articulated object.


Another important contribution of the paper is the presentation of
a vision-based ``interface-quality'' hand tracker: a self-initialising,
real-time, robust and accurate system of sufficient quality to be
used for complex interactive tasks such as drawing packages. The
tracker models the hand as an articulated object and partitioned
sampling is the crucial component in achieving these favourable properties.
The system tracks a user's hand on an arbitrary background using
a standard colour camera, in such a way that the hand can be employed
as a 4-dimensional mouse (planar translation and orientation of thumb
and index finger).},
author = {Isard, M and McCormick, J},
booktitle = {in ECCV},
file = {:home/abetan16/Dropbox/MendeleyV3/Isard, McCormick - 2000 - Partitioned sampling, articulated objects, and interface-quality hand tracking.pdf:pdf},
pages = {3--19},
publisher = {Springerlink},
title = {{Partitioned sampling, articulated objects, and interface-quality hand tracking}},
url = {http://link.springer.com/chapter/10.1007/3-540-45053-X{\_}1},
year = {2000}
}
@article{Fuente2014,
abstract = {In Arabic, as in many languages, the future is “ahead” and the past is “behind.” Yet in the research reported here, we showed that Arabic speakers tend to conceptualize the future as behind and the past as ahead of them, despite using spoken metaphors that suggest the opposite. We propose a new account of how space-time mappings become activated in individuals' minds and entrenched in their cultures, the temporal-focus hypothesis: People should conceptualize either the future or the past as in front of them to the extent that their culture (or subculture) is future oriented or past oriented. Results support the temporal-focus hypothesis, demonstrating that the space-time mappings in people's minds are conditioned by their cultural attitudes toward time, that they depend on attentional focus, and that they can vary independently of the space-time mappings enshrined in language. Keywords},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Ishihara, Tatsuya and Kitani, Kris M. and Ma, Wei-Chiu and Takagi, Hironobu and Asakawa, Chieko},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:home/abetan16/Dropbox/MendeleyV3/Ishihara et al. - 2014 - Recognizing Hand-Object Interactions in Wearable Camera Videos.pdf:pdf},
isbn = {9780874216561},
issn = {1467-9280},
journal = {Psychological Science},
keywords = {conceptual metaphor,cross-cultural differences,mental models,open data,space,time},
number = {9},
pages = {1682--1690},
pmid = {25052830},
title = {{Recognizing Hand-Object Interactions in Wearable Camera Videos}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15003161 http://cid.oxfordjournals.org/lookup/doi/10.1093/cid/cir991 http://www.scielo.cl/pdf/udecada/v15n26/art06.pdf http://www.scopus.com/inward/record.url?eid=2-s2.0-84861150233{\&}partnerID=tZOtx3y1},
volume = {25},
year = {2014}
}
@article{Itti1998,
abstract = {A visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented. Multiscale image features are combined into a single topographical saliency map. A dynamical neural network then selects attended locations in order of decreasing saliency. The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail},
archivePrefix = {arXiv},
arxivId = {math/0504378},
author = {Itti, Laurent and Koch, Christof and Niebur, Ernst},
doi = {10.1109/34.730558},
eprint = {0504378},
file = {:home/abetan16/Dropbox/Articles/Mendeley/Itti, Koch, Niebur - 1998 - A Model of Saliency-based Visual Attention for Rapid Scene Analysis.pdf:pdf},
isbn = {0162-8828 VO - 20},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {11},
pages = {1254--1259},
pmid = {12780641},
primaryClass = {math},
title = {{A Model of Saliency-based Visual Attention for Rapid Scene Analysis}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=730558},
volume = {20},
year = {1998}
}
@article{Ivonin2013,
abstract = {Lifelogging tools aim to precisely capture daily experiences of people from the first-person perspective. Although there have been numerous lifelogging tools developed for users to record the external environment around them, the internal part of experience characterized by emotions seems to be neglected in the lifelogging field. However, the internal experiences of people are important and, therefore, lifelogging tools should be able to capture not only the environmental data, but also emotional experiences, thereby providing a more complete archive of past events. Moreover, there are implicit emotions that cannot be consciously experienced, but still influence human behaviors and memories. It has been proven that conscious emotions can be recognized from physiological signals of the human body. This fact may be used to enhance life-logs with information about unconscious emotions, which otherwise would remain hidden. On the other hand, it is not clear if unconscious emotions can be recognized from physiological signals and differentiated from conscious emotions. Therefore, an experiment was designed to elicit emotions (both conscious and unconscious) with visual and auditory stimuli and to record cardiovascular responses of 34 participants. The experimental results showed that heart rate responses to the presentation of the stimuli are unique for every category of the emotional stimuli and allow differentiation between various emotional experiences of the participants.},
author = {Ivonin, Leonid and Chang, Huang-Ming and Chen, Wei and Rauterberg, Matthias},
doi = {10.1007/s00779-012-0514-5},
file = {:home/abetan16/Dropbox/MendeleyV3/Ivonin et al. - 2013 - Unconscious Emotions Quantifying and Logging Something We Are Not Aware of.pdf:pdf},
issn = {1617-4909},
journal = {Personal and Ubiquitous Computing},
keywords = {archetypal symbols {\'{a}} lifelogging,emotions {\'{a}} unconscious {\'{a}},heart rate {\'{a}}},
month = {apr},
number = {4},
pages = {663--673},
title = {{Unconscious emotions: quantifying and logging something we are not aware of}},
url = {http://link.springer.com/10.1007/s00779-012-0514-5},
volume = {17},
year = {2013}
}
@article{Jameson2001,
author = {Jameson, A and Gro{\ss}mann-Hutter, B and Rummer, R and Bohnenberger, T and Wittig, F},
doi = {10.1016/S0950-7051(00)00097-6},
file = {:home/abetan16/Dropbox/MendeleyV3/Jameson et al. - 2001 - When Actions Have Consequences Empirically Based Decision Making for Intelligent User Interfaces.pdf:pdf},
issn = {09507051},
journal = {Knowledge-based Systems},
keywords = {bayesian networks,decision theory,intelligent user interfaces,machine learning},
month = {mar},
number = {1-2},
pages = {75--92},
title = {{When actions have consequences: Emprically based decision making for intelligent user interfaces}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0950705100000976},
volume = {14},
year = {2001}
}
@article{Je2007,
abstract = {This paper deals with the understanding of four musical time patterns and three tempos that are generated by a human conductor of robot orchestra or an operator of computer-based music play system using the hand gesture recognition. We use only a stereo vision camera with no extra special devices such as sensor glove, 3D motion capture system, infra-red camera, electronic baton and so on. We propose a simple and reliable vision-based hand gesture recognition using the conducting feature point (CFP), the motion-direction code, and the motion history matching. The proposed hand gesture recognition system operates as follows: First, it extracts the human hand region by segmenting the depth information generated by stereo matching of image sequences. Next, it follows the motion of the center of the gravity(COG) of the extracted hand region and generates the gesture features such as CFP and the direction-code. Finally, we obtain the current timing pattern of the music's beat and tempo by the proposed hand gesture recognition using either CFP tracking or motion histogram matching. The experimental results show that the musical time pattern and tempo recognition rate are over 86{\%} on the test data set when the motion histogram matching is used.},
author = {Je, Hongmo and Kim, Jiman and Kim, Daijin},
doi = {10.1109/ROMAN.2007.4415073},
file = {:home/abetan16/Dropbox/MendeleyV3/Je, Kim, Kim - 2007 - Hand Gesture Recognition To Understand Musical Conducting Action.pdf:pdf},
isbn = {1424416345},
journal = {Proceedings - IEEE International Workshop on Robot and Human Interactive Communication},
pages = {163--168},
publisher = {Ieee},
title = {{Hand gesture recognition to understand musical conducting action}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4415073},
year = {2007}
}
@article{Jing2011,
author = {Jing, Li and Shao, Chao},
doi = {10.4304/jsw.6.6.1034-1041},
file = {:home/abetan16/Dropbox/MendeleyV3/Jing, Shao - 2011 - Selection of the suitable parameter value for ISOMAP.pdf:pdf},
issn = {1796217X},
journal = {Journal of Software},
keywords = {Data visualization,Geodesic distance,ISOMAP,Neighborhood size,Residual variance,Shortest path distance},
number = {6},
pages = {1034--1041},
title = {{Selection of the suitable parameter value for ISOMAP}},
volume = {6},
year = {2011}
}
@inproceedings{Jojic2010,
abstract = {In order to study the properties of total visual input in humans, a single subject wore a camera for two weeks capturing, on average, an image every 20 seconds. The resulting new dataset contains a mix of indoor and outdoor scenes as well as numerous foreground objects. Our first analysis goal is to create a visual summary of the subject's two weeks of life using unsupervised algorithms that would automatically discover recurrent scenes, familiar faces or common actions. Direct application of existing algorithms, such as panoramic stitching (e.g. Photosynth) or appearance-based clustering models (e.g. the epitome), is impractical due to either the large dataset size or the dramatic variation in the lighting conditions. As a remedy to these problems, we introduce a novel image representation, the “stel epitome,” and an associated efficient learning algorithm. In our model, each image or image patch is characterized by a hidden mapping T, which, as in previous epitome models, defines a mapping between the image-coordinates and the coordinates in the large "all-I-have-seen" epitome matrix. The limited epitome real-estate forces the mappings of different images to overlap, with this overlap indicating image similarity. However, in our model the image similarity does not depend on direct pixel-to-pixel intensity/color/feature comparisons as in previous epitome models, but on spatial configuration of scene or object parts, as the model is based on the palette-invariant stel models. As a result, stel epitomes capture structure that is invariant to non-structural changes, such as illumination, that tend to uniformly affect pixels belonging to a single scene or object part.},
author = {Jojic, Nebojsa and Perina, Alessandro and Murino, Vittorio},
booktitle = {Neural Information Processing Systems},
file = {:home/abetan16/Dropbox/MendeleyV3/Jojic, Perina, Murino - 2010 - Structural Epitome a Way to Summarize one's Visual Experience.pdf:pdf},
isbn = {9781617823800},
pages = {1--9},
title = {{Structural Epitome: a Way to Summarize one's Visual Experience}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2010{\_}1298.pdf},
year = {2010}
}
@inproceedings{Jones1999,
abstract = {The existence of large image datasets such as photos on the World$\backslash$nWide Web make it possible to build powerful generic models for low-level$\backslash$nimage attributes like color using simple histogram learning techniques.$\backslash$nWe describe the construction of color models for skin and non-skin$\backslash$nclasses from a dataset of nearly 1 billion labeled pixels. These classes$\backslash$nexhibit a surprising degree of separability which we exploit by building$\backslash$na skin pixel detector that achieves an equal error rate of 88{\%}. We$\backslash$ncompare the performance of histogram and mixture models in skin$\backslash$ndetection and find histogram models to be superior in accuracy and$\backslash$ncomputational cost. Using aggregate features computed from the skin$\backslash$ndetector we build a remarkably effective detector for naked people. We$\backslash$nbelieve this work is the most comprehensive and detailed exploration of$\backslash$nskin color models to date},
address = {Fort Collins, CO},
author = {Jones, Michael J. and Rehg, James M.},
booktitle = {International Journal of Computer Vision},
doi = {10.1023/A:1013200319198},
file = {:home/abetan16/Dropbox/MendeleyV3/Jones, Rehg - 2002 - Statistical color models with application to skin detection.pdf:pdf},
isbn = {0-7695-0149-4},
issn = {09205691},
keywords = {Color models,Histograms,Skin detection},
number = {1},
pages = {81--96},
publisher = {IEEE Computer Society},
title = {{Statistical color models with application to skin detection}},
volume = {46},
year = {2002}
}
@book{Jurafsky2000,
abstract = {This book takes an empirical approach to language processing, based on applying statistical and other machine-learning algorithms to large corpora. Methodology boxes are included in each chapter. Each chapter is built around one or more worked examples to demonstrate the main idea of the chapter. Covers the fundamental algorithms of various fields, whether originally proposed for spoken or written language to demonstrate how the same algorithm can be used for speech recognition and word-sense disambiguation. Emphasis on web and other practical applications. Emphasis on scientific evaluation. Useful as a reference for professionals in any of the areas of speech and language processing.},
author = {Jurafsky, Daniel and Martin, James H},
booktitle = {Speech and Language Processing An Introduction to Natural Language Processing Computational Linguistics and Speech Recognition},
doi = {10.1162/089120100750105975},
file = {:home/abetan16/Dropbox/MendeleyV3/Jurafsky, Martin - 2009 - Speech and Language Processing An Introduction to Natural Language Processing, Computational Linguistics, and.pdf:pdf},
isbn = {0130950696},
issn = {08912017},
pages = {0--934},
publisher = {Prentice-Hall Inc},
title = {{Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition}},
url = {http://www.mitpressjournals.org/doi/pdf/10.1162/089120100750105975},
volume = {21},
year = {2009}
}
@article{Kakumanu2007,
abstract = {Skin detection plays an important role in a wide range of image processing applications ranging from face detection, face tracking, gesture analysis, content-based image retrieval systems and to various human computer interaction domains. Recently, skin detection methodologies based on skin-color information as a cue has gained much attention as skin-color provides computationally effective yet, robust information against rotations, scaling and partial occlusions. Skin detection using color information can be a challenging task as the skin appearance in images is affected by various factors such as illumination, background, camera characteristics, and ethnicity. Numerous techniques are presented in literature for skin detection using color. In this paper, we provide a critical up-to-date review of the various skin modeling and classification strategies based on color information in the visual spectrum. The review is divided into three different categories: first, we present the various color spaces used for skin modeling and detection. Second, we present different skin modeling and classification approaches. However, many of these works are limited in performance due to real-world conditions such as illumination and viewing conditions. To cope up with the rapidly changing illumination conditions, illumination adaptation techniques are applied along with skin-color detection. Third, we present various approaches that use skin-color constancy and dynamic adaptation techniques to improve the skin detection performance in dynamically changing illumination and environmental conditions. Wherever available, we also indicate the various factors under which the skin detection techniques perform well.},
author = {Kakumanu, P. and Makrogiannis, S. and Bourbakis, N.},
doi = {10.1016/j.patcog.2006.06.010},
file = {:home/abetan16/Dropbox/MendeleyV3/Kakumanu, Makrogiannis, Bourbakis - 2007 - A Survey of Skin-color Modeling and Detection Methods.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {color spaces and color,constancy,skin detection,skin-color modeling},
month = {mar},
number = {3},
pages = {1106--1122},
title = {{A survey of skin-color modeling and detection methods}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0031320306002767},
volume = {40},
year = {2007}
}
@book{Ertel2011,
abstract = {Artificial intelligence is widely accepted as a technology offering an alternative way to tackle complex and ill-defined problems. Artificial intelligence (AI) systems comprise areas like, artificial neural networks (ANN), genetic algorithms (GA), fuzzy logic (FL) and various hybrid systems, which combine two or more techniques. AI systems can learn from examples, are fault tolerant, i.e., they are able to handle noisy and incomplete data, are able to deal with non-linear problems, and once trained can perform prediction and generalization at very high speed. They have been used in diverse applications in pattern recognition, forecasting, optimization, signal processing, medicine, power systems, manufacturing, control, robotics, social/psychological sciences and many more. They are particularly useful in system modeling such as in implementing complex mappings and system identification. The major objective of this introductory chapter is to outline an understanding of how artificial intelligence systems operate and how they are set up in order to be used in the different disciplines of energy and renewable energy engineering. Keywords:},
address = {London},
author = {Kalogirou, Soteris A.},
doi = {10.1007/978-0-85729-299-5},
file = {:home/abetan16/Dropbox/MendeleyV3/Kalogirou - 2005 - Introduction to Artificial Intelligence (Bridging).pdf:pdf},
isbn = {0-201-11945-5},
keywords = {artificial intelligence,fuzzy logic,genetic algorithms,hybrid,neural networks},
pages = {1--80},
publisher = {Springer London},
series = {Undergraduate Topics in Computer Science},
title = {{Introduction to Artificial Intelligence (Bridging)}},
url = {http://usir.salford.ac.uk/8332/},
year = {2005}
}
@article{Kanade2012,
abstract = {For understanding the behavior, intent, and environment of a person, the surveillance metaphor is traditional; that is, install cameras and observe the subject, and his/her interaction with other people and the environment. Instead, we argue that the first-person vision (FPV), which senses the environment and the subject's activities from a wearable sensor, is more advantageous with images about the subject's environment as taken from his/her view points, and with readily available information about head motion and gaze through eye tracking. In this paper, we review key research challenges that need to be addressed to develop such FPV systems, and describe our ongoing work to address them using examples from our prototype systems.},
author = {Kanade, Takeo and Hebert, Martial},
doi = {10.1109/JPROC.2012.2200554},
file = {:home/abetan16/Dropbox/MendeleyV3/Kanade, Hebert - 2012 - First-person Vision.pdf:pdf},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Computer vision,eye tracking,object recognition,quality of life technologies},
month = {aug},
number = {8},
pages = {2442--2453},
title = {{First-person vision}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6232429},
volume = {100},
year = {2012}
}
@inproceedings{Hebert2011,
abstract = {We propose an approach to identify and segment objects from scenes that a person (or robot) encounters in Activities of Daily Living (ADL). Images collected in those cluttered scenes contain multiple objects. Each image provides only a partial, possibly very different view of each object. An object instance discovery program must be able to link pieces of visual information from multiple images and extract the consistent patterns.},
author = {Kang, Hongwen and Hebert, Martial and Kanade, Takeo},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2011.6126314},
file = {:home/abetan16/Dropbox/MendeleyV3/Kang, Hebert, Kanade - 2011 - Discovering Object Instances from Scenes of Daily Living.pdf:pdf},
isbn = {9781457711015},
issn = {1550-5499},
month = {nov},
number = {Iccv},
pages = {762--769},
publisher = {Ieee},
title = {{Discovering object instances from scenes of Daily Living}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6126314},
year = {2011}
}
@inproceedings{Kapoor2001,
abstract = {This paper reports work in progress to build a Learning Companion, a computerized system sensitive to the affective aspects of learning, which facilitates the childs own efforts at learning. Learning related to science, math, engineering, and technology naturally involves failure and a host of associated affective responses. This article describes techniques and tools being developed to recognize affective states important in learning. the interplay between emotions and},
author = {Kapoor, Ashish and Mota, Selene and Picard, Rosalind W},
booktitle = {AAAI Fall symposium},
file = {:home/abetan16/Dropbox/MendeleyV3/Kapoor, Mota, Picard - 2001 - Towards a Learning Companion that Recognizes Affect.pdf:pdf},
number = {543},
pages = {2--4},
publisher = {AAAI Press},
title = {{Towards a Learning Companion that Recognizes Affect}},
url = {http://www.aaai.org/Papers/Symposia/Fall/2001/FS-01-02/FS01-02-015.pdf},
year = {2001}
}
@inproceedings{Karaman2010,
abstract = {Our research focuses on analysing human activities according to a known behaviorist scenario, in case of noisy and high dimensional collected data. The data come from the monitoring of patients with dementia diseases by wearable cameras. We define a structural model of video recordings based on a Hidden Markov Model. New spatio-temporal features, color features and localization features are proposed as observations. First results in recognition of activities are promising. 1.},
author = {Karaman, Svebor and Benois-Pineau, Jenny and Megret, Remi and Dovgalecs, Vladislavs and Dartigues, Jean-Francois and Gaestel, Yann},
booktitle = {2010 20th International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2010.999},
file = {:home/abetan16/Dropbox/MendeleyV3/Karaman et al. - 2010 - Human Daily Activities Indexing in Videos from Wearable Cameras for Monitoring of Patients with Dementia Disease.pdf:pdf},
isbn = {978-1-4244-7542-1},
month = {aug},
pages = {4113--4116},
publisher = {Ieee},
title = {{Human Daily Activities Indexing in Videos from Wearable Cameras for Monitoring of Patients with Dementia Diseases}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5597705},
year = {2010}
}
@inproceedings{Liu2014b,
abstract = {This paper presents a driver assistance system to monitor the driver driving behavior by applying the so-called “First-Person Vision” (FPV) technology. It consists of two modules: the scene classification and the driver viewing angle estimation. First, we use “bag of words” image classification approach based on FAST and BRIEF feature descriptor in the dataset. Second, we establish the “vocabulary dictionary” to encode an input image as a feature vector. Third, we apply SVM classifier to detect whether the driver's view is inside or outside scene of a vehicle. Finally, we estimate the driver viewing angle estimation based on FPV and the windshield-mounted camera. In the experiments, we illustrate the effectiveness of our system.},
author = {Keep, Lane and System, Assist},
booktitle = {Round Table, The},
file = {:home/abetan16/Dropbox/MendeleyV3/Liu, Hsu, Huang - 2014 - First-person-vision-based Driver Assistance System.pdf:pdf},
keywords = {- arm cortex m0,{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_},c language,ir proximity sensor,nuvoton development board,ultrasonic sensor},
number = {4},
pages = {3543--3545},
title = {{Driver Assistance System}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:No+Title{\#}0 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=7009793},
volume = {2},
year = {2004}
}
@inproceedings{Khan2010,
abstract = {Skin detection is used in applications ranging from face de- tection, tracking body parts and hand gesture analysis, to retrieval and blocking objectionable content. For robust skin segmentation and detection, we investigate color classifica- tion based on random forest. A random forest is a statistical framework with a very high generalization accuracy and quick training times. The random forest approach is used with the IHLS color space for raw pixel based skin detection. We evaluate random forest based skin detection and com- pare it to Bayesian network, Multilayer Perceptron, SVM, AdaBoost, Naive Bayes and RBF network. Results on a database of 8991 images with manually annotated pixel-level ground truth showthat with the IHLS color space, the random forest approach outperforms other approaches. We also show the effect of increasing the number of trees grown for random forest. With fewer trees we get faster training times and with 10 trees we get the highest F-score.},
author = {Khan, Rehanullah and Hanbury, Allan and Stoettinger, Julian},
booktitle = {Ieee Icip},
doi = {10.1109/ICIP.2010.5651638},
file = {:home/abetan16/Dropbox/MendeleyV3/Khan, Hanbury, Stoettinger - 2010 - Skin Detection a Random Forest Approach.pdf:pdf},
isbn = {9781424479948},
issn = {1522-4880},
pages = {4613--4616},
title = {{SKIN DETECTION : A RANDOM FOREST APPROACH IRF , Vienna , Austria}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5651638},
year = {2010}
}
@inproceedings{Kim2003,
abstract = {Generating vehicle trajectories from video data is an important application of ITS (intelligent transportation systems). We introduce a new tracking approach which uses model-based 3-D vehicle detection and description algorithm. Our vehicle detection and description algorithm is based on a probabilistic line feature grouping, and it is faster (by up to an order of magnitude) and more flexible than previous image-based algorithms. We present the system implementation and the vehicle detection and tracking results.},
address = {Nice, France},
author = {Kim, Z. and Malik, J.},
booktitle = {Proceedings Ninth IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2003.1238392},
file = {:home/abetan16/Dropbox/MendeleyV3/Kim, Malik - 2003 - Fast Vehicle Detection with Probabilistic Feature Grouping and Its Application to Vehicle Tracking.pdf:pdf},
isbn = {0-7695-1950-4},
number = {Iccv},
pages = {524 -- 531},
publisher = {IEEE},
title = {{Fast vehicle detection with probabilistic feature grouping and its application to vehicle tracking}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1238392},
year = {2003}
}
@article{Kitani2012,
abstract = {A new algorithm enables a fully automatic real-time video segmentation solution for dynamic first-person sports videos. The proposed approach leverages the latest in robust vision-based ego-motion estimation and unsupervised learning using nonparametric Bayesian modeling.},
author = {Kitani, K},
doi = {10.1109/MPRV.2012.28},
file = {:home/abetan16/Dropbox/MendeleyV3/Kitani - 2012 - Ego-Action Analysis for First-Person Sports Videos.pdf:pdf},
isbn = {1536-1268 VO  - 11},
issn = {1536-1268},
journal = {Pervasive Computing, IEEE},
keywords = {Algorithm design and analysis,Bayes methods,Cameras,Computer vision,Inference algorithms,Pervasive computing,Real time systems,Videos,ego action analysis,ego motion estimation,ego-action,ego-motion,first-person point-of-view video,image segmentation,nonparametric Bayesian modeling,pervasive computing,robust vision,sport,sports video analysis,sports videos,unsupervised learning,video segmentation,wearable computing},
number = {2},
pages = {92--95},
title = {{Ego-Action Analysis for First-Person Sports Videos}},
volume = {11},
year = {2012}
}
@inproceedings{Kitani2011,
abstract = {Portable high-quality sports cameras (e.g. head or helmet mounted) built for recording dynamic first-person video footage are becoming a common item among many sports enthusiasts. We address the novel task of discovering first-person action categories (which we call ego-actions) which can be useful for such tasks as video indexing and retrieval. In order to learn ego-action categories, we investigate the use of motion-based histograms and unsupervised learning algorithms to quickly cluster video content. Our approach assumes a completely unsupervised scenario, where labeled training videos are not available, videos are not pre-segmented and the number of ego-action categories are unknown. In our proposed framework we show that a stacked Dirichlet process mixture model can be used to automatically learn a motion histogram codebook and the set of ego-action categories. We quantitatively evaluate our approach on both in-house and public YouTube videos and demonstrate robust ego-action categorization across several sports genres. Comparative analysis shows that our approach outperforms other state-of-the-art topic models with respect to both classification accuracy and computational speed. Preliminary results indicate that on average, the categorical content of a 10 minute video sequence can be indexed in under 5 seconds.},
address = {Providence, RI},
author = {Kitani, K and Okabe, T and Sato, Y and Sugimoto, A},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2011.5995406},
file = {:home/abetan16/Dropbox/MendeleyV3//Kitani et al. - 2011 - Fast unsupervised ego-action learning for first-person sports videos.pdf:pdf},
isbn = {9781457703942},
issn = {10636919},
month = {jun},
pages = {3241--3248},
publisher = {IEEE},
title = {{Fast unsupervised ego-action learning for first-person sports videos}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5995406 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5995406},
year = {2011}
}
@inproceedings{Kleinsmith2005,
abstract = {Date: 24-29 July 2005 Abstract: The role of body posture in affect recognition, and the importance of emotion in the development and support of intelligent and social behavior have been accepted and researched within several fields. While posture is considered important, much research has focused on extracting emotion information from dance sequences. Instead, our focus is on creating an affective posture recognition system that incrementally learns to recognize and react to people's affective behaviors. In this paper, we examine a set of requirements for creating this system, and our proposed solutions. The first requirement is that the system is general and non-situation specific. Secondly, it should be able to handle explicit and implicit feedback. Finally, it must be able to incrementally learn the emotion categories without predefining them. We tested and compared the performance of our system using 182 standing postures described as a combination of form features and motion low level features, across several emotion categories, with a typical algorithm used for recognition, back-propagation, and with human observers in an aim to show the generalizability of the system. This initial testing showed positive results.},
author = {Kleinsmith, Andrea and Fushimi, Tsuyoshi and Bianchi-Berthouze, N.},
booktitle = {Context},
file = {:home/abetan16/Dropbox/MendeleyV3/Kleinsmith, Fushimi, Bianchi-Berthouze - 2005 - An incremental and interactive affective posture recognition system.pdf:pdf},
keywords = {affective posture,emotion recognition,explicit feedback,implicit feedback,incremental learning,incremental lexicon},
pages = {1--13},
title = {{An incremental and interactive affective posture recognition system}},
url = {http://www0.cs.ucl.ac.uk/staff/n.berthouze/paper/KleinsmithFushimi.pdf},
volume = {Internatio},
year = {2005}
}
@article{Klonoff2014,
author = {Klonoff, D. C.},
doi = {10.1177/1932296813518858},
file = {:home/abetan16/Dropbox/MendeleyV3/Klonoff - 2014 - New Wearable Computers Move Ahead Google Glass and Smart Wigs.pdf:pdf},
issn = {1932-2968},
journal = {Journal of Diabetes Science and Technology},
keywords = {and technology,computer,diabetes,google glass,journal of diabetes science,wearable,wig},
month = {jan},
number = {1},
pages = {3--5},
title = {{New Wearable Computers Move Ahead: Google Glass and Smart Wigs}},
url = {http://dst.sagepub.com/lookup/doi/10.1177/1932296813518858},
volume = {8},
year = {2014}
}
@article{Knaus2016,
author = {Knaus, T. a. and Kamps, J. and Foundas, a. L.},
doi = {10.1177/0031512516637021},
file = {:home/abetan16/Dropbox/MendeleyV3/Knaus, Kamps, Foundas - 2016 - Handedness in Children with Autism Spectrum Disorder.pdf:pdf},
issn = {0031-5125},
journal = {Perceptual and Motor Skills},
keywords = {at children,autism,brain and behavior program,corresponding author,department of neurology,handedness,knaus,language,laterality,s hospital,tracey a},
title = {{Handedness in Children with Autism Spectrum Disorder}},
url = {http://pms.sagepub.com/lookup/doi/10.1177/0031512516637021},
year = {2016}
}
@article{Kohonen1990,
abstract = {The self-organized map, an architecture suggested for artificial$\backslash$nneural networks, is explained by presenting simulation experiments and$\backslash$npractical applications. The self-organizing map has the property of$\backslash$neffectively creating spatially organized internal representations of$\backslash$nvarious features of input signals and their abstractions. One result of$\backslash$nthis is that the self-organization process can discover semantic$\backslash$nrelationships in sentences. Brain maps, semantic maps, and early work on$\backslash$ncompetitive learning are reviewed. The self-organizing map algorithm (an$\backslash$nalgorithm which order responses spatially) is reviewed, focusing on best$\backslash$nmatching cell selection and adaptation of the weight vectors.$\backslash$nSuggestions for applying the self-organizing map algorithm,$\backslash$ndemonstrations of the ordering process, and an example of hierarchical$\backslash$nclustering of data are presented. Fine tuning the map by learning vector$\backslash$nquantization is addressed. The use of self-organized maps in practical$\backslash$nspeech recognition and a simulation experiment on semantic mapping are$\backslash$ndiscussed},
author = {Kohonen, T.},
doi = {10.1109/5.58325},
file = {:home/abetan16/Dropbox/MendeleyV3/Kohonen - 1990 - The self-organizing map.pdf:pdf},
isbn = {0018-9219},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
number = {9},
pages = {1464--1480},
pmid = {21447162},
title = {{The self-organizing map}},
volume = {78},
year = {1990}
}
@inproceedings{Kojima2001,
abstract = {This paper proposes an interface for wearable computers using augmented reality (AR) environment which allows the user to handle virtual objects with his/her own hands intuitively. The proposed system is constructed with a wearable computer and a head mounted display (HMD). A pair of cameras is attached to HMD positioned as a pair of eyes. Using the stereo measurement, the finger's postures are captured and displayed together with virtual objects on a video-see-though HMD. The system tracks the fingertips motion of the index finger and the thumb so as the user can pick up, move and rotate the virtual object as if treating a real object with the hands in the real world. While the stereo measurement, range information is added to the region of the hand Visual occlusion is synthesized between the user's hand and the virtual objects including 2D based graphical user interfaces, 3D computer graphics models and so on},
author = {Kojima, Y and Yasumuro, Y},
booktitle = {Virtual Systems and Multimedia},
doi = {10.1109/VSMM.2001.969701},
file = {:home/abetan16/Dropbox/Mendeley/Kojima, Yasumuro - 2001 - Hand manipulation of virtual objects in wearable augmented reality.pdf:pdf},
isbn = {0769514022},
pages = {463 -- 469},
title = {{Hand Manipulation of Virtual Objects in Wearable Augmented Reality}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=969701},
year = {2001}
}
@article{Kolesnik2002,
abstract = {Although a number of conducting gesture analysis and following systems have been developed over the years, most of the projects either primarily concentrated on tracking tempo and amplitude indicating gestures while not taking expressive gestures into account, or implemented individual mapping techniques for expressive gestures that varied from research to research. There is a clear need for a uniform process that could be applied toward analysis of both indicative and expressive gestures. The conducting gesture recognition system is implemented on the basis of Hidden Markov Model (HMM) process. An external HMM object is developed for Max/MSP software. Training and recognition procedures are applied toward both right hand beat- and amplitude- indicative gestures, and left hand expressive gestures. Continuous recognition of right-hand gestures is incorporated into a real-time gesture analysis and performance system in Max/MSP/Jitter environment.},
author = {Kolesnik, Paul and Wanderley, Marcelo},
file = {:home/abetan16/Dropbox/MendeleyV3/Kolesnik, Wanderley - 2004 - Recognition, analysis and performance with expressive conducting gestures.pdf:pdf},
journal = {In Proceedings of the International Computer Music Conference},
pages = {1--4},
title = {{Recognition, analysis and performance with expressive conducting gestures}},
url = {http://idmil.org/{\_}media/wiki/icmc2004{\_}kolesnik.pdf?id=publications{\&}cache=cache},
year = {2004}
}
@article{Kollorz2008,
abstract = {This paper presents a new approach for gesture classifica- tion using x- and y-projections of the image and optional depth features. The system uses a 3-D time-of-flight (TOF) sensor which has the big advantage of simplifying hand segmentation. For the presented system, a Photonic-Mixer-Device (PMD) camera with a resolution of 160×120 pixels and a frame rate of 15 frames per second is used. The goal of our system is to recognise 12 different static hand gestures. The x- and y-projections and the depth features of the captured image are good enough to use a simple nearest neighbour classifier, resulting in a fast classification. To evaluate the system, a set of 408 images is recorded, 12 gestures from 34 persons. With a ‘Leave-One-Out' evaluation, the recognition rate of the system is 94.61{\%}, and classification time is about 30ms on a standard PC.},
author = {Kollorz, Eva and Penne, Jochen and Hornegger, Joachim and Barke, Alexander},
doi = {10.1504/IJISTA.2008.021296},
file = {:home/abetan16/Dropbox/Articles/Mendeley/Kollorz, Penne - 2008 - Gesture recognition with a time-of-flight camera.pdf:pdf},
issn = {1740-8865},
journal = {International Journal of Intelligent Systems Technologies and Applications},
keywords = {barke,e,follows,gesture recognition,gesture recognition with a,hornegger and a,j,kollorz,penne,projection features,reference to this paper,should be made as,time-of-,time-of-flight camera},
number = {3/4},
pages = {334},
title = {{Gesture recognition with a Time-Of-Flight camera}},
url = {http://inderscience.metapress.com/index/9515m16818538x43.pdf},
volume = {5},
year = {2008}
}
@article{Kolsch2006,
abstract = {Wearable computers and their novel applications demand more context-specific user interfaces than traditional desktop paradigms can offer. We describe a multimodal interface to a wearable computing system and explain how it enhances a mobile user's situational awareness and provides new functionality. Our mobile augmented reality system visualizes otherwise “invisible” information encountered in urban environments. A versatile filtering tool allows interactive display of occluded infrastructure and of dense data distributions such as room temperature or wireless network strength, with applications for building maintenance, emergency response, and reconnaissance missions. Multiple input modalities – vision-based hand gesture recognition, a onedimensional tool, and speech recognition – are combined with three late integration styles to provide intuitive and effective means to operate the complex application functionality. Users can select visualizations for various semantic information types with spoken commands and manipulate virtual objects through gestural input. The system is demonstrated and evaluated in a realistic indoor and outdoor task environment.},
author = {K{\"{o}}lsch, M and Bane, R and H{\"{o}}llerer, T and Turk, M},
file = {:home/abetan16/Dropbox/MendeleyV3/K{\"{o}}lsch et al. - 2006 - Touching the Visualized Invisible Wearable Ar with a Multimodal Interface.pdf:pdf},
journal = {IEEE Computer Graphics and Applications},
keywords = {augmented reality,hand gesture,information visualization,multimodal interface,recognition,wearable computing},
number = {1},
pages = {1--56},
title = {{Touching the visualized invisible: Wearable ar with a multimodal interface}},
url = {http://www.cs.ucsb.edu/{~}holl/pubs/kolsch-2004-mmj.pdf},
volume = {Jun},
year = {2006}
}
@inproceedings{Kolsch2004a,
abstract = {This paper introduces "Flocks of Features," a fast tracking method for non-rigid and highly articulated objects such as hands. It combines KLT features and a learned foreground color distribution to facilitate 2D position tracking from a monocular view. The tracker's benefits lie in its speed, its robustness against background noise, and its ability to track objects that undergo arbitrary rotations and vast and rapid deformations. We demonstrate tracker performance on hand tracking with a non-stationary camera in unconstrained indoor and outdoor environments. The tracker yields over threefold improvement over a CamShift tracker in terms of the number of frames tracked before the target was lost, and often more than one order of magnitude improvement in terms of the fractions of particular test sequences tracked successfully.},
author = {Kolsch, M and Turk, M},
booktitle = {2004 Conference on Computer Vision and Pattern Recognition Workshop},
doi = {10.1109/CVPR.2004.345},
file = {:home/abetan16/Dropbox/MendeleyV3/Kolsch, Turk - 2004 - Fast 2D Hand Tracking with Flocks of Features and Multi-Cue Integration.pdf:pdf},
isbn = {0-7695-2158-4},
issn = {21607516},
keywords = {flock of trackers,multi-cue,tracking},
number = {C},
pages = {158--158},
publisher = {IEEE Comput. Soc},
title = {{Fast 2D Hand Tracking with Flocks of Features and Multi-Cue Integration}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1384956},
volume = {10},
year = {2004}
}
@inproceedings{Kolsch2004b,
author = {Kolsch, M. and Turk, M.},
booktitle = {Sixth IEEE International Conference on Automatic Face and Gesture Recognition, 2004. Proceedings.},
doi = {10.1109/AFGR.2004.1301601},
file = {:home/abetan16/Dropbox/MendeleyV3/Kolsch, Turk - 2004 - Robust hand detection.pdf:pdf},
isbn = {0-7695-2122-3},
pages = {614--619},
publisher = {IEEE},
title = {{Robust hand detection}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1301601},
year = {2004}
}
@inproceedings{Kolsch2004,
abstract = {Vision-based user interfaces are a feasible and advanta- geous modality for wearable computers. To substanti- ate this claim, we present a robust real-time hand gesture recognition system that is capable of being the sole input provider for a demonstration application. It achieves us- ability and interactivity even when both the head-worn cam- era and the object of interest are in motion. We describe a set of general gesture-based interaction styles and ex- plore their characteristics in terms of task suitability and the computer vision algorithms required for their recogni- tion. Preliminary evaluation of our prototype system leads to the conclusion that vision-based interfaces have achieved thematurity necessary to help overcome some limitations of more traditional mobile user interfaces.},
author = {Kolsch, M. and Turk, M. and Hollerer, T.},
booktitle = {Mobile and Ubiquitous Systems: Networking and Services, 2004. MOBIQUITOUS 2004. The First Annual International Conference on},
doi = {10.1109/MOBIQ.2004.1331713},
file = {:home/abetan16/Dropbox/MendeleyV3/Kolsch, Turk, Hollerer - 2004 - Vision-based Interfaces for Mobility.pdf:pdf},
isbn = {0769522084},
pages = {86--94},
publisher = {Ieee},
title = {{Vision-based interfaces for mobility}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1331713},
year = {2004}
}
@inproceedings{Krahnstoever2005,
abstract = {Computer vision-based articulated human motion tracking is attractive for many applications since it allows unobtrusive and passive estimation of people's activities. Although much progress has been made on human-only tracking, the visual tracking of people that interact with objects such as tools, products, packages, and devices is considerably more challenging. The wide variety of objects, their varying visual appearance, and their varying (and often small) size makes a vision-based understanding of person-object interactions very difficult. To alleviate this problem for at least some application domains, we propose a framework that combines visual human motion tracking with RFID based object tracking. We customized commonly available RFID technology to obtain orientation estimates of objects in the field of RFID emitter coils. The resulting fusion of visual human motion tracking and RFID-based object tracking enables the accurate estimation of high-level interactions between people and objects for application domains such as retail, home-care, workplace-safety, manufacturing and others},
address = {Breckenridge, CO},
author = {Krahnstoever, N and Rittscher, J and Tu, P and Chean, K and Tomlinson, T},
booktitle = {2005 Seventh IEEE Workshops on Applications of Computer Vision WACVMOTION05 Volume 1},
doi = {10.1109/ACVMOT.2005.17},
file = {:home/abetan16/Dropbox/MendeleyV3/Krahnstoever et al. - 2005 - Activity Recognition using Visual Tracking and RFID.pdf:pdf},
isbn = {0769522718},
month = {jan},
pages = {494--500},
publisher = {IEEE},
title = {{Activity Recognition using Visual Tracking and RFID}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4129523},
volume = {1},
year = {2005}
}
@inproceedings{Kurata2000,
abstract = {This paper describes a wearable input interface the functions of which is similar to the usual mouse. The Hand-mouse is realized by classifying each pixel as a hand pixel or a background pixel based on approximation of a color histogram with the Gaussian mixture model and by tting the simple model of hand shapes into the classi ed hand pixels.},
address = {Yokohama},
author = {Kurata, T and Okuma, T and Kourogi, M and Sakaue, K},
booktitle = {Symposium on Mixed Reality},
file = {:home/abetan16/Dropbox/MendeleyV3//Kurata et al. - 2000 - The Hand-mouse A Human Interface Suitable for Augmented Reality Environments Enabled by Visual Wearables.pdf:pdf},
keywords = {1 wearable augmented environments,human interface,real-time system,vironments,visual wearables,wearable augmented en-},
pages = {188--189},
title = {{The Hand-mouse: A Human Interface Suitable for Augmented Reality Environments Enabled by Visual Wearables}},
year = {2000}
}
@inproceedings{Kurata2001,
abstract = {This paper describes an algorithm to detect and track a hand in each image taken by a wearable camera. We primarily use color information, however, instead of pre-defined skin-color models, we dynamically construct hand- and background-color models by using a Gaussian mixture model (GMM) to approximate the color histogram. Not only to obtain the estimated mean of hand color necessary for the restricted EM algorithm that estimates the GMM but also to classify hand pixels based on the Bayes decision theory, we use a spatial probability distribution of hand pixels. Since the static distribution is inadequate for the hand-tracking stage, we translate the distribution with the hand motion based on the mean shift algorithm. Using the proposed method, we implemented the Hand Mouse that uses the wearer's hand as a pointing device, on our wearable vision system},
address = {Vancuver, Canada},
author = {Kurata, T. and Okuma, T. and Kourogi, M. and Sakaue, K.},
booktitle = {Proceedings IEEE ICCV Workshop on Recognition, Analysis, and Tracking of Faces and Gestures in Real-Time Systems},
doi = {10.1109/RATFG.2001.938920},
file = {:home/abetan16/Dropbox/MendeleyV3//Kurata et al. - 2001 - The Hand Mouse GMM hand-color classification and mean shift tracking.pdf:pdf},
isbn = {0-7695-1074-4},
pages = {119--124},
publisher = {IEEE},
title = {{The Hand Mouse: GMM hand-color classification and mean shift tracking}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=938920},
year = {2001}
}
@inproceedings{Laganiere2008,
abstract = {In this paper we present a video summarization method based on the study of spatio-temporal activity within the video. The visual activity is estimated by measuring the number of interest points, jointly obtained in the spatial and temporal domains. The proposed approach is composed of five steps. First, image features are collected using the spatio-temporal Hessian matrix. Then, these features are processed to retrieve the candidate video segments for the summary (denoted clips). Further on, two specific steps are designed to first detect the redundant clips, and second to eliminate the clapperboard images. The final step consists in the construction of the final summary which is performed by retaining the clips showing the highest level of activity. The proposed approach was tested on the BBC Rushes Summarization task within the TRECVID 2008 campaign.},
address = {New York, New York, USA},
author = {Lagani{\`{e}}re, Robert and Bacco, Raphael and Hocevar, Arnaud and Lambert, Patrick and Pa{\"{i}}s, Gr{\'{e}}gory and Ionescu, Bogdan E.},
booktitle = {Proceeding of the 2nd ACM workshop on Video summarization - TVS '08},
doi = {10.1145/1463563.1463590},
file = {:home/abetan16/Dropbox/MendeleyV3/Lagani{\`{e}}re et al. - 2008 - Video Summarization from Spatio-temporal Features.pdf:pdf},
isbn = {9781605583099},
keywords = {hessian-,spatio-temporal features,video abstract},
pages = {144--148},
publisher = {ACM Press},
title = {{Video summarization from spatio-temporal features}},
url = {http://portal.acm.org/citation.cfm?doid=1463563.1463590},
year = {2008}
}
@article{Land2001,
abstract = {Two recent studies have investigated the relations of eye and hand movements in extended food preparation tasks, and here the results are compared. The tasks could be divided into a series of actions performed on objects. The eyes usually reached the next object in the sequence before any sign of manipulative action, indicating that eye movements are planned into the motor pattern and lead each action. The eyes usually fixated the same object throughout the action upon it, although they often moved on to the next object in the sequence before completion of the preceding action. The specific roles of individual fixations could be identified as locating (establishing the locations of objects for future use), directing (establishing target direction prior to contact), guiding (supervising the relative movements of two or three objects) and checking (establishing whether some particular condition is met, prior to the termination of an action). It is argued that, at the beginning of each action, the oculomotor system is supplied with the identity of the required object, information about its location, and instructions about the nature of the monitoring required during the action. The eye movements during this kind of task are nearly all to task-relevant objects, and thus their control is seen as primarily 'top-down', and influenced very little by the 'intrinsic salience' of objects. ?? 2001 Elsevier Science Ltd. All rights reserved.},
author = {Land, Michael F. and Hayhoe, Mary},
doi = {10.1016/S0042-6989(01)00102-X},
file = {:home/abetan16/Dropbox/MendeleyV3/Land, Hayhoe - 2001 - In What Ways Do Eye Movements Contribute to Everyday Activities.pdf:pdf},
isbn = {0042-6989},
issn = {00426989},
journal = {Vision Research},
keywords = {Everyday settings,Eye movement,Vision},
month = {jan},
number = {25-26},
pages = {3559--3565},
pmid = {11718795},
title = {{In what ways do eye movements contribute to everyday activities?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/11718795},
volume = {41},
year = {2001}
}
@inproceedings{Laptev2008,
abstract = {The aim of this paper is to address recognition of natural human actions in diverse and realistic video settings. This challenging but important subject has mostly been ignored in the past due to several problems one of which is the lack of realistic and annotated video datasets. Our first contribution is to address this limitation and to investigate the use of movie scripts for automatic annotation of human actions in videos. We evaluate alternative methods for action retrieval from scripts and show benefits of a text-based classifier. Using the retrieved action samples for visual learning, we next turn to the problem of action classification in video. We present a new method for video classification that builds upon and extends several recent ideas including local space-time features, space-time pyramids and multi-channel non-linear SVMs. The method is shown to improve state-of-the-art results on the standard KTH action dataset by achieving 91.8{\%} accuracy. Given the inherent problem of noisy labels in automatic annotation, we particularly investigate and show high tolerance of our method to annotation errors in the training set. We finally apply the method to learning and classifying challenging action classes in movies and show promising results.},
address = {Anchorage, AK},
author = {Laptev, Ivan and Marsza{\l}ek, Marcin and Schmid, Cordelia and Rozenfeld, Benjamin},
booktitle = {26th IEEE Conference on Computer Vision and Pattern Recognition, CVPR},
doi = {10.1109/CVPR.2008.4587756},
file = {:home/abetan16/Dropbox/MendeleyV3/Laptev et al. - 2008 - Learning Realistic Human Actions from Movies.pdf:pdf},
isbn = {9781424422432},
issn = {1063-6919},
pages = {1--8},
pmid = {7622251722739293746},
publisher = {IEEE},
title = {{Learning realistic human actions from movies}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4587756$\backslash$nhttp://ieeexplore.ieee.org/ielx5/4558014/4587335/04587756.pdf?tp={\&}arnumber=4587756{\&}isnumber=4587335},
year = {2008}
}
@article{Lara2013,
abstract = {Providing accurate and opportune information on people's activities and behaviors is one of the most important tasks in pervasive computing. Innumerable applications can be visualized, for instance, in medical, security, entertainment, and tactical scenarios. Despite human activity recognition (HAR) being an active field for more than a decade, there are still key aspects that, if addressed, would constitute a significant turn in the way people interact with mobile devices. This paper surveys the state of the art in HAR based on wearable sensors. A general architecture is first presented along with a description of the main components of any HAR system. We also propose a two-level taxonomy in accordance to the learning approach (either supervised or semi-supervised) and the response time (either offline or online). Then, the principal issues and challenges are discussed, as well as the main solutions to each one of them. Twenty eight systems are qualitatively evaluated in terms of recognition performance, energy consumption, obtrusiveness, and flexibility, among others. Finally, we present some open problems and ideas that, due to their high relevance, should be addressed in future research.},
author = {Lara, Oscar D. and Labrador, Miguel A.},
doi = {10.1109/SURV.2012.110112.00192},
file = {:home/abetan16/Dropbox/MendeleyV3/Lara, Labrador - 2013 - A Survey on Human Activity Recognition using Wearable Sensors.pdf:pdf},
isbn = {1553-877x},
issn = {1553-877X},
journal = {IEEE Communications Surveys {\&} Tutorials},
keywords = {Acceleration,Accelerometers,Accuracy,Feature extraction,Human-centric sensing,Humans,Wearable sensors,context awareness,energy consumption,human activity recognition,image motion analysis,learning (artificial intelligence),machine learning,mobile applications,mobile computing,mobile devices,open problems,pervasive computing,recognition performance,response time,semi-supervised learning,two-level taxonomy,wearable computers},
number = {3},
pages = {1192--1209},
title = {{A Survey on Human Activity Recognition using Wearable Sensors}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6365160},
volume = {15},
year = {2013}
}
@inproceedings{Lazebnik2006,
abstract = {This paper presents a method for recognizing scene categories based on approximate global geometric correspondence. This technique works by partitioning the image into increasingly fine sub-regions and computing histograms of local features found inside each sub-region. The resulting "spatial pyramid" is a simple and computationally efficient extension of an orderless bag-of-features image representation, and it shows significantly improved performance on challenging scene categorization tasks. Specifically, our proposed method exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories. The spatial pyramid framework also offers insights into the success of several recently proposed image descriptions, including Torralba's "gist" and Lowe's SIFT descriptors.},
author = {Lazebnik, S. and Schmid, C. and Ponce, J.},
booktitle = {2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2 (CVPR'06)},
doi = {10.1109/CVPR.2006.68},
file = {:home/abetan16/Dropbox/MendeleyV3/Lazebnik, Schmid, Ponce - 2006 - Beyond Bags of Features Spatial Pyramid Matching for Recognizing Natural Scene Categories.pdf:pdf},
isbn = {0-7695-2597-0},
pages = {2169--2178},
publisher = {Ieee},
title = {{Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1641019},
volume = {2},
year = {2006}
}
@article{Lee2013,
abstract = {This paper presents a new approach for tracking hand rotation and various grasping gestures through an infrared camera. For the complexity and ambiguity of an observed hand shape, it is difficult to simultaneously estimate hand configuration and orientation from a silhouette image of a grasping hand gesture. This paper proposes a dynamic shape model for hand grasping gestures using cylindrical manifold embedding to analyze variations of hand shape in different hand configurations between two key hand poses and in simultaneous circular view change by hand rotation. An arbitrary hand shape between two key hand poses from any view can be generated using a cylindrical manifold embedding point after learning nonlinear generative models from the embedding space to the corresponding hand shape observed. The cylindrical manifold embedding model is extended to various grasping gestures by decomposing multiple cylindrical manifold embeddings through grasping style analysis. Grasping hand gestures with simultaneous hand rotation are tracked using particle filters on the manifold space with grasping style estimation. Experimental results for synthetic and real data indicate that the proposed model can accurately track various grasping gestures with hand rotation. The proposed approach may be applied to advanced user interfaces in dark environments by using images beyond the visible spectrum. ?? 2013 Elsevier Inc. All rights reserved.},
author = {Lee, Chan Su and Chun, Sungyong and Park, Shin Won},
doi = {10.1016/j.cviu.2013.08.006},
file = {:home/abetan16/Dropbox/MendeleyV3/Lee, Chun, Park - 2013 - Tracking hand rotation and various grasping gestures from an IR camera using extended cylindrical manifold embe.pdf:pdf},
isbn = {9780769541099},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {Conceptual manifold embedding,Hand gesture recognition,Inferred image,Manifold embedding,Style decomposition,Tracking},
number = {12},
pages = {1711--1723},
pmid = {5595995},
publisher = {Elsevier Inc.},
title = {{Tracking hand rotation and various grasping gestures from an IR camera using extended cylindrical manifold embedding}},
url = {http://dx.doi.org/10.1016/j.cviu.2013.08.006},
volume = {117},
year = {2013}
}
@article{Lee,
abstract = {Designing a conducting gesture analysis system for public spaces poses unique challenges. We present conga, a software framework that enables automatic recognition and interpretation of conducting gestures. conga is able to recognize multiple types of gestures with varying levels of difficulty for the user to perform, from a standard four-beat pattern, to simplified up-down conducting movements, to no pattern at all. conga provides an extendable library of feature detectors linked together into a directed acyclic graph; these graphs represent the various conducting patterns as gesture profiles. At run-time, conga searches for the best profile to match a user's gestures in real-time, and uses a beat prediction algorithm to provide results at the sub-beat level, in addition to output values such as tempo, gesture size, and the gesture's geometric center. Unlike some previous approaches, conga does not need to be trained with sample data before use. Our preliminary user tests show that conga has a beat recognition rate of over 90{\%}. conga is deployed as the gesture recognition system for Maestro!, an interactive conducting exhibit that opened in the Betty Brinn Children's Museum in Milwaukee, USA in March 2006.},
author = {Lee, Eric and Gr{\"{u}}ll, Ingo and Kiel, Henning and Borchers, Jan},
file = {:home/abetan16/Dropbox/MendeleyV3/Lee et al. - 2006 - Conga a Framework for Adaptive Conducting Gesture Analysis.pdf:pdf},
isbn = {2-84426-314-3},
journal = {Proceedings of the 2006 International Conference on New Interfaces for Musical Expression (NIME06)},
keywords = {conducting,gesture recognition,software gesture frameworks},
pages = {260--265},
title = {{Conga: a Framework for Adaptive Conducting Gesture Analysis}},
year = {2006}
}
@inproceedings{Lee2014,
abstract = {Egocentric cameras are becoming more popular, intro- ducing increasing volumes of video in which the biases and framing of traditional photography are replaced with those of natural viewing tendencies. This paradigm enables new applications, including novel studies of social interaction and human development. Recent work has focused on iden- tifying the camera wearer's hands as a first step towards more complex analysis. In this paper, we study how to dis- ambiguate and track not only the observer's hands but also those of social partners. We present a probabilistic frame- work for modeling paired interactions that incorporates the spatial, temporal, and appearance constraints inherent in egocentric video. We test our approach on a dataset of over 30 minutes of video from six pairs of subjects},
address = {Columbus, Ohio},
annote = {- Graphic Probabilistic Networks
- Skin
- Gibbs
- Viola Jones},
author = {Lee, S and Bambach, S and Crandall, D and Franchak, J and Yu, C},
booktitle = {Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPRW.2014.86},
file = {:home/abetan16/Dropbox/MendeleyV3/Lee et al. - 2014 - This Hand Is My Hand A Probabilistic Approach to Hand Disambiguation in Egocentric Video.pdf:pdf},
isbn = {9781479943081},
number = {Figure 2},
pages = {1--8},
publisher = {IEEE Computer Society},
title = {{This Hand Is My Hand: A Probabilistic Approach to Hand Disambiguation in Egocentric Video}},
url = {http://homes.soic.indiana.edu/steflee/pdfs/cvpr{\_}hands.pdf},
year = {2014}
}
@article{Lee2015,
abstract = {We present a video summarization approach for egocentric or “wearable” camera data. Given hours of video, the proposed method produces a compact storyboard summary of the camera wearer's day. In contrast to tradi- tional keyframe selection techniques, the resulting summary focuses on the most important objects and people with which the camera wearer interacts. To accomplish this, we develop region cues indicative of high-level saliency in egocentric video—such as the nearness to hands, gaze, and frequency of occurrence—and learn a regressor to predict the relative importance of any new region based on these cues. Using these predictions and a simple form of temporal event detec- tion, our method selects frames for the storyboard that reflect the key object-driven happenings. We adjust the compact- ness of the final summary given either an importance selec- tion criterion or a length budget; for the latter, we design an efficient dynamic programming solution that accounts for importance, visual uniqueness, and temporal displacement. Critically, the approach is neither camera-wearer-specific nor object-specific; that means the learned importance metric need not be trained for a given user or context, and it can Communicated by C. Schn{\"{o}}rr. Electronic supplementary material The online version of this article (doi:10.1007/s11263-014-0794-5) contains supplementary material, which is available to authorized users. Y. J. Lee (B) Department of Computer Science, University of California, Davis, CA, USA e-mail: yjlee@cs.ucdavis.edu K. Grauman Department of Computer Science, University of Texas at Austin, Austin, TX, USA e-mail: grauman@cs.utexas.edu predict the importance of objects and people that have never been seen previously. Our results on two egocentric video datasets showthe method's promise relative to existing tech- niques for saliency and summarization.},
author = {Lee, Yong Jae and Grauman, Kristen},
doi = {10.1007/s11263-014-0794-5},
file = {:home/abetan16/Dropbox/MendeleyV3/Lee, Grauman - 2015 - Predicting Important Objects for Egocentric Video Summarization.pdf:pdf},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
keywords = {category discovery,egocentric vision,saliency detection,video summarization},
month = {jan},
number = {1},
pages = {38--55},
title = {{Predicting Important Objects for Egocentric Video Summarization}},
url = {http://link.springer.com/10.1007/s11263-014-0794-5},
volume = {114},
year = {2015}
}
@article{Lee2011,
abstract = {We present an approach to discover and segment foreground object(s) in video. Given an unannotated video sequence, the method first identifies object-like regions in any frame according to both static and dynamic cues. We then compute a series of binary partitions among those candidate “key-segments” to discover hypothesis groups with persistent appearance and motion. Finally, using each ranked hypothesis in turn, we estimate a pixel-level object labeling across all frames, where (a) the foreground likelihood depends on both the hypothesis's appearance as well as a novel localization prior based on partial shape matching, and (b) the background likelihood depends on cues pulled from the key-segments' (possibly diverse) surroundings observed across the sequence. Compared to existing methods, our approach automatically focuses on the persistent foreground regions of interest while resisting oversegmentation. We apply our method to challenging benchmark videos, and show competitive or better results than the state-of-the-art.},
address = {Barcelona},
author = {Lee, Yong Jae and Kim, Jaechul and Grauman, K},
doi = {10.1109/ICCV.2011.6126471},
file = {:home/abetan16/Dropbox/MendeleyV3/Lee, Kim, Grauman - 2011 - Key-segments for Video Object Segmentation.pdf:pdf},
isbn = {1550-5499 VO  -},
issn = {1550-5499},
journal = {Computer Vision (ICCV), 2011 IEEE International Conference on},
keywords = {image matching;image segmentation;image sequences;},
month = {nov},
number = {Iccv},
pages = {1995--2002},
publisher = {IEEE},
title = {{Key-segments for video object segmentation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6126471 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6126471},
year = {2011}
}
@inproceedings{Lester2005,
abstract = {Accurate recognition and tracking of human activities is an important goal of ubiquitous computing. Recent advances in the development of multi-modal wearable sensors enable us to gather rich datasets of human activities. However, the problem of automatically identifying the most useful features for modeling such activities remains largely unsolved. In this paper we present a hybrid approach to recognizing activities, which combines boosting to discriminatively select useful features and learn an ensemble of static classifiers to recognize different activities, with hidden Markov models (HMMs) to capture the temporal regularities and smoothness of activities. We tested the activity recognition system using over 12 hours of wearable-sensor data collected by volunteers in natural unconstrained environments. The models succeeded in identifying a small set of maximally informative features, and were able identify ten different human activities with an accuracy of 95{\%}. 1},
author = {Lester, Jonathan and Choudhury, Tanzeem and Kern, Nicky},
booktitle = {In Proc. of the International Joint Conference on Artificial Intelligence (IJCAI)},
file = {:home/abetan16/Dropbox/MendeleyV3/Lester, Choudhury, Kern - 2005 - A Hybrid Discriminative Generative Approach for Modeling Human Activities.pdf:pdf},
pages = {766--772},
title = {{A Hybrid Discriminative Generative Approach for Modeling Human Activities.}},
url = {http://www.researchgate.net/publication/220815706{\_}A{\_}Hybrid{\_}DiscriminativeGenerative{\_}Approach{\_}for{\_}Modeling{\_}Human{\_}Activities/file/9fcfd5141f6973f1d9.pdf},
year = {2005}
}
@inproceedings{Li2013a,
abstract = {We address the task of pixel-level hand detection in the context of ego-centric cameras. Extracting hand regions in ego-centric videos is a critical step for understanding hand- object manipulation and analyzing hand-eye coordination. However, in contrast to traditional applications of hand de- tection, such as gesture interfaces or sign-language recog- nition, ego-centric videos present new challenges such as rapid changes in illuminations, significant camera motion and complex hand-object manipulations. To quantify the challenges and performance in this new domain, we present a fully labeled indoor/outdoor ego-centric hand detection benchmark dataset containing over 200 million labeled pix- els, which contains hand images taken under various illu- mination conditions. Using both our dataset and a pub- licly available ego-centric indoors dataset, we give exten- sive analysis of detection performance using a wide range of local appearance features. Our analysis highlights the effectiveness of sparse features and the importance of mod- eling global illumination. We propose a modeling strategy based on our findings and show that our model outperforms several baseline approaches.},
author = {Li, C and Kitani, K},
booktitle = {2013 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2013.458},
file = {:home/abetan16/Dropbox/MendeleyV3/Li, Kitani - 2013 - Pixel-Level Hand Detection in Ego-centric Videos.pdf:pdf},
isbn = {978-0-7695-4989-7},
month = {jun},
pages = {3570--3577},
publisher = {Ieee},
title = {{Pixel-Level Hand Detection in Ego-centric Videos}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6619302},
year = {2013}
}
@inproceedings{Li2013b,
abstract = {Egocentric cameras can be used to benefit such tasks as analyzing fine motor skills, recognizing gestures and learn- ing about hand-object manipulation. To enable such tech- nology, we believe that the hands must detected on the pixel- level to gain important information about the shape of the hands and fingers. We show that the problem of pixel-wise hand detection can be effectively solved, by posing the prob- lem as a model recommendation task. As such, the goal of a recommendation system is to recommend the n-best hand detectors based on the probe set – a small amount of la- beled data from the test distribution. This requirement of a probe set is a serious limitation in many applications, such as ego-centric hand detection, where the test distribution may be continually changing. To address this limitation, we propose the use of virtual probes which can be automati- cally extracted from the test distribution. The key idea is that many features, such as the color distribution or rela- tive performance between two detectors, can be used as a proxy to the probe set. In our experiments we show that the recommendation paradigm is well-equipped to handle complex changes in the appearance of the hands in first- person vision. In particular, we show how our system is able to generalize to new scenarios by testing our model across multiple users.},
address = {Sydney},
author = {Li, C and Kitani, K},
booktitle = {2013 IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2013.326},
file = {:home/abetan16/Dropbox/MendeleyV3/Li, Kitani - 2013 - Model Recommendation with Virtual Probes for Egocentric Hand Detection(2).pdf:pdf},
isbn = {978-1-4799-2840-8},
pages = {2624--2631},
publisher = {IEEE Computer Society},
title = {{Model Recommendation with Virtual Probes for Egocentric Hand Detection}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6751437},
year = {2013}
}
@inproceedings{Li2007,
abstract = {We propose a first attempt to classify events in static images by integrating scene and object categorizations. We define an event in a static image as a human activity taking place in a specific environment. In this paper, we use a number of sport games such as snow boarding, rock climbing or badminton to demonstrate event classification. Our goal is to classify the event in the image as well as to provide a number of semantic labels to the objects and scene environment within the image. For example, given a rowing scene, our algorithm recognizes the event as rowing by classifying the environment as a lake and recognizing the critical objects in the image as athletes, rowing boat, water, etc. We achieve this integrative and holistic recognition through a generative graphical model. We have assembled a highly challenging database of 8 widely varied sport events. We show that our system is capable of classifying these event classes at 73.4{\%} accuracy. While each component of the model contributes to the final recognition, using scene or objects alone cannot achieve this performance.},
address = {Rio de Janeiro},
author = {Li, Li Jia and Fei-Fei, Li},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2007.4408872},
file = {:home/abetan16/Dropbox/MendeleyV3/Li, Fei-Fei - 2007 - What, Where and Who Classifying Events by Scene and Object Recognition.pdf:pdf},
isbn = {978-1-4244-1631-8},
issn = {1550-5499},
pages = {1--8},
title = {{What, where and who? Classifying events by scene and object recognition}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4408872},
year = {2007}
}
@inproceedings{Li2011,
abstract = {Unsupervised learning can be used to extract image representations that are useful for various and diverse vision tasks. After noticing that most biological vision systems for interpreting static images are trained using disparity information, we developed an analogous framework for unsupervised learning. The output of our method is a model that can generate a vector representation or descriptor from any static image. However, the model is trained using pairs of consecutive video frames, which are used to find representations that are consistent with optical flow-derived objects, or `flobjects'. To demonstrate the flobject analysis framework, we extend the latent Dirichlet allocation bag-of-words model to account for real-valued word-specific flow vectors and image-specific probabilistic associations between flow clusters and topics. We show that the static image representations extracted using our method can be used to achieve higher classification rates and better generalization than standard topic models, spatial pyramid matching and gist descriptors.},
author = {Li, P S and Givoni, I E and Frey, B J},
booktitle = {Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on},
doi = {10.1109/CVPR.2011.5995649},
file = {:home/abetan16/Dropbox/MendeleyV3/Li, Givoni, Frey - 2011 - Learning Better Image Representations Using 'Flobject Analysis'.pdf:pdf},
isbn = {978-1-4577-0394-2},
issn = {1063-6919},
keywords = {computer vision;feature extraction;image represent},
month = {jun},
pages = {2721--2728},
publisher = {IEEE Computer Society},
title = {{Learning better image representations using {\#}x2018;flobject analysis {\#}x2019;}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5995649 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5995649},
year = {2011}
}
@inproceedings{Li2013,
abstract = {We present a model for gaze prediction in egocentric video by leveraging the implicit cues that exist in camera wearer's behaviors. Specifically, we compute the camera wearer's head motion and hand location from the video and combine them to estimate where the eyes look. We further model the dynamic behavior of the gaze, in particular fix- ations, as latent variables to improve the gaze prediction. Our gaze prediction results outperform the state-of-the-art algorithms by a large margin on publicly available egocen- tric vision datasets. In addition, we demonstrate that we get a significant performance boost in recognizing daily actions and segmenting foreground objects by plugging in our gaze predictions into state-of-the-art methods.},
author = {Li, Yin and Fathi, Alireza and Rehg, James M.},
booktitle = {IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2013.399},
file = {:home/abetan16/Dropbox/MendeleyV3/Li, Fathi, Rehg - 2013 - Learning to Predict Gaze in Egocentric Video.pdf:pdf},
isbn = {9781479928392},
issn = {1550-5499},
keywords = {Action Recognition,Egocentric Vision,Gaze Prediction,Object Segmentation},
pages = {3216--3223},
publisher = {Ieee},
title = {{Learning to Predict Gaze in Egocentric Video}},
year = {2013}
}
@article{Liu2010,
abstract = {We propose a novel method for removing irrelevant frames from a video given user-provided frame-level labeling for a very small number of frames. We first hypothesize a number of windows which possibly contain the object of interest, and then determine which window(s) truly contain the object of interest. Our method enjoys several favorable properties. First, compared to approaches where a single descriptor is used to describe a whole frame, each window's feature descriptor has the chance of genuinely describing the object of interest; hence it is less affected by background clutter. Second, by considering the temporal continuity of a video instead of treating frames as independent, we can hypothesize the location of the windows more accurately. Third, by infusing prior knowledge into the patch-level model, we can precisely follow the trajectory of the object of interest. This allows us to largely reduce the number of windows and hence reduce the chance of overfitting the data during learning. We demonstrate the effectiveness of the method by comparing it to several other semi-supervised learning approaches on challenging video clips.},
author = {Liu, David and Hua, Gang and Chen, Tsuhan},
doi = {10.1109/TPAMI.2010.31},
file = {:home/abetan16/Dropbox/Articles/Mendeley/Liu, Hua, Chen - 2010 - A Hierarchical Visual Model for Video Object Summarization.pdf:pdf},
isbn = {0162-8828},
issn = {0162-8828},
journal = {Analysis},
number = {Xx},
pages = {1--13},
title = {{for Video Object Summarization}},
volume = {32},
year = {2010}
}
@incollection{Liu2006,
abstract = {This paper presents a novel optimization-based approach for video key frame selection. We define key frames to be a temporally ordered subsequence of the original video sequence, and the optimal k key frames are the subsequence of length k that optimizes an energy function we define on all subsequences. These optimal key subsequences form a hierarchy, with one such subsequence for every k less than the length of the video n, and this hierarchy can be retrieved all at once using a dynamic programming process with polynomial (On 3) computation time. To further reduce computation, an approximate solution based on a greedy algorithm can compute the key frame hierarchy in O(n{\textperiodcentered}log(n)). We also present a hybrid method, which flexibly captures the virtues of both approaches. Our empirical comparisons between the optimal and greedy solutions indicate their results are very close. We show that the greedy algorithm is more appropriate for video streaming and network applications where compression ratios may change dynamically, and provide a method to compute the appropriate times to advance through key frames during video playback of the compressed stream. Additionally, we exploit the results of the greedy algorithm to devise an interactive video content browser. To quantify our algorithms' effectiveness, we propose a new evaluation measure, called “well-distributed” key frames. Our experimental results on several videos show that both the optimal and the greedy algorithms outperform several popular existing algorithms in terms of summarization quality, computational time, and guaranteed convergence.},
author = {Liu, T and Kender, J},
booktitle = {Internantional Conference on Computer Vision},
file = {:home/abetan16/Dropbox/Articles/Mendeley/Liu, Kender - 2006 - Optimization Algorithms for the Selection of Key Frame Sequences of Variable Length.pdf:pdf},
pages = {403--417},
publisher = {Springer Berlin Heidelberg},
title = {{Optimization Algorithms for the Selection of Key Frame Sequences of Variable Length}},
url = {http://link.springer.com/chapter/10.1007/3-540-47979-1{\_}27},
year = {2006}
}
@article{Liu2014,
abstract = {In this paper, the task of human hand trajectory tracking and gesture trajectory recognition based on synchronized color and depth video is considered. Toward this end, in the facet of hand tracking, a joint observation model with the hand cues of skin saliency, motion, depth are integrated into particle filter in order to move particles to local peak in the likelihood. The proposed hand tracking method, namely Salient Skin, Motion, Depth based particle filter (SSMD-PF), is capable to improve the tracking accuracy considerably, in the context of the signer performing the gesture toward the camera device and in front of moving, cluttered backgrounds. In the facet of gesture recognition, a shape-order context descriptor on the basis of shape context is introduced, which can describe the gesture in spatiotemporal domain. The efficient shape-order context descriptor can reveal the shape relationship and embed gesture sequence order information into descriptor. Moreover, the shape-order context leads to a robust score for gesture invariant. Our approach is complemented with experimental results on the settings of the challenging hand–signed digits datasets and American Sign Language dataset, which corroborate the performance of the novel techniques.},
author = {Liu, Weihua and Fan, Yangyu and ZhongZhang, ZL},
file = {:home/abetan16/Dropbox/MendeleyV3/Liu, Fan, ZhongZhang - 2014 - RGBD Video Based Human Hand Trajectory Tracking and Gesture Recognition System.pdf:pdf},
journal = {Mathematical Problems in Engineering},
title = {{RGBD Video Based Human Hand Trajectory Tracking and Gesture Recognition System}},
url = {http://downloads.hindawi.com/journals/mpe/aip/863732.pdf},
year = {2014}
}
@inproceedings{Liu2014a,
annote = {Object Recognition
- Bag of Words
- Feature Points
- NN
- SVM
- RF},
author = {Liu, Yang and Jang, Youngkyoon and Woo, Woontack and Kim, Tae-Kyun},
booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2014.83},
file = {:home/abetan16/Dropbox/MendeleyV3/Liu et al. - 2014 - Video-Based Object Recognition Using Novel Set-of-Sets Representations.pdf:pdf},
isbn = {978-1-4799-4308-1},
month = {jun},
pages = {533--540},
publisher = {Ieee},
title = {{Video-Based Object Recognition Using Novel Set-of-Sets Representations}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6910032},
year = {2014}
}
@article{Livanis2006,
author = {Livanis, G. and Moss, C. B. and Breneman, V. E. and Nehring, R. F.},
doi = {10.1111/j.1467-8276.2006.00906.x},
file = {:home/abetan16/Dropbox/MendeleyV3/Livanis et al. - 2006 - Urban Sprawl and Farmland Prices.pdf:pdf},
issn = {0002-9092},
journal = {American Journal of Agricultural Economics},
keywords = {hedonic determinants,jor policy issue since,land prices,ma-,of urban areas has,reduced farmland around,spatial econometrics,the 1980s,the expansion,urban sprawl,urban sprawl and land,use has become a},
month = {nov},
number = {4},
pages = {915--929},
title = {{Urban Sprawl and Farmland Prices}},
url = {http://ajae.oxfordjournals.org/cgi/doi/10.1111/j.1467-8276.2006.00906.x},
volume = {88},
year = {2006}
}
@inproceedings{Lowe1999,
abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds},
address = {Kerkyra},
author = {Lowe, D.G.},
booktitle = {Proceedings of the Seventh IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.1999.790410},
file = {:home/abetan16/Dropbox/MendeleyV3/Lowe - 1999 - Object recognition from local scale-invariant features.pdf:pdf},
isbn = {0-7695-0164-8},
pages = {1150--1157 vol.2},
publisher = {Ieee},
title = {{Object recognition from local scale-invariant features}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=790410},
year = {1999}
}
@article{Lu2015,
author = {Lu, Cewu and Liao, Renjie and Jia, Jiaya},
doi = {10.1109/TIP.2015.2487868},
file = {:home/abetan16/Dropbox/MendeleyV3/Lu, Liao, Jia - 2015 - Personal Objects Discovery in First-Person Video.pdf:pdf},
issn = {1057-7149},
journal = {IEEE Transactions on Image Processing},
number = {12},
pages = {1--1},
title = {{Personal Objects Discovery in First-Person Video}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7293645},
volume = {24},
year = {2015}
}
@article{Lu2007,
author = {Lu, D. and Weng, Q.},
doi = {10.1080/01431160600746456},
file = {:home/abetan16/Dropbox/MendeleyV3/Lu, Weng - 2007 - A Survey of Image Classification Methods and Techniques for Improving Classification Performance.pdf:pdf},
issn = {0143-1161},
journal = {International Journal of Remote Sensing},
month = {mar},
number = {5},
pages = {823--870},
title = {{A survey of image classification methods and techniques for improving classification performance}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01431160600746456},
volume = {28},
year = {2007}
}
@article{Liu2006a,
abstract = { Music mood describes the inherent emotional expression of a music clip. It is helpful in music understanding, music retrieval, and some other music-related applications. In this paper, a hierarchical framework is presented to automate the task of mood detection from acoustic music data, by following some music psychological theories in western cultures. The hierarchical framework has the advantage of emphasizing the most suitable features in different detection tasks. Three feature sets, including intensity, timbre, and rhythm are extracted to represent the characteristics of a music clip. The intensity feature set is represented by the energy in each subband, the timbre feature set is composed of the spectral shape features and spectral contrast features, and the rhythm feature set indicates three aspects that are closely related with an individual's mood response, including rhythm strength, rhythm regularity, and tempo. Furthermore, since mood is usually changeable in an entire piece of classical music, the approach to mood detection is extended to mood tracking for a music piece, by dividing the music into several independent segments, each of which contains a homogeneous emotional expression. Preliminary evaluations indicate that the proposed algorithms produce satisfactory results. On our testing database composed of 800 representative music clips, the average accuracy of mood detection achieves up to 86.3{\%}. We can also on average recall 84.1{\%} of the mood boundaries from nine testing music pieces.},
author = {Lu, Lie and Liu, Dan and Zhang, Hong Jiang},
doi = {10.1109/TSA.2005.860344},
file = {:home/abetan16/Dropbox/MendeleyV3/Lu, Liu, Zhang - 2006 - Automatic mood detection and tracking of music audio signals.pdf:pdf},
isbn = {1558-7916},
issn = {15587916},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
keywords = {Affective computing,Hierarchical framework,Mood detection,Mood tracking,Music emotion,Music information retrieval,Music mood},
month = {jan},
number = {1},
pages = {5--18},
title = {{Automatic mood detection and tracking of music audio signals}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1561259},
volume = {14},
year = {2006}
}
@inproceedings{Lu2013,
abstract = {We present a video summarization approach that discovers the story of an egocentric video. Given a long input video, our method selects a short chain of video sub shots depicting the essential events. Inspired by work in text analysis that links news articles over time, we define a random-walk based metric of influence between sub shots that reflects how visual objects contribute to the progression of events. Using this influence metric, we define an objective for the optimal k-subs hot summary. Whereas traditional methods optimize a summary's diversity or representative ness, ours explicitly accounts for how one sub-event "leads to" another-which, critically, captures event connectivity beyond simple object co-occurrence. As a result, our summaries provide a better sense of story. We apply our approach to over 12 hours of daily activity video taken from 23 unique camera wearers, and systematically evaluate its quality compared to multiple baselines with 34 human subjects.},
address = {Portland, OR, USA},
author = {Lu, Zheng and Grauman, Kristen},
booktitle = {2013 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2013.350},
file = {:home/abetan16/Dropbox/MendeleyV3/Lu, Grauman - 2013 - Story-Driven Summarization for Egocentric Video.pdf:pdf},
isbn = {978-0-7695-4989-7},
month = {jun},
pages = {2714--2721},
publisher = {IEEE},
title = {{Story-Driven Summarization for Egocentric Video}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6619194},
year = {2013}
}
@article{Press2015,
abstract = {Previous work suggests that the perception of a visual beat in conductors' gestures is related to certain physical characteristics of the movements they produce, most notably to periods of negative acceleration, and low position in the vertical axis. These findings are based on studies that have presented participants with somewhat simple gestures, and in which participants have been required to simply tap in time with the beat. Thus, it is not clear how generalizable these findings are to real-world conducting situations, in which a conductor uses considerably more complex gestures to direct an ensemble of musicians playing actual instruments. The aims of the present study were to examine the features of conductors' gestures with which ensemble musicians synchronize their performance in an ecologically valid setting and to develop automatic feature extraction methods for the analysis of audio and movement data. An optical motion capture system was used to record the gestures of an expert conductor directing an ensemble of expert musicians over a 20-minute period. A simultaneous audio recording of the performance of the ensemble was also made and synchronized with the motion capture data. Four short excerpts were selected for analysis, two in which the conductor communicated the beat with high clarity, and two in which the beat was communicated with low clarity. Twelve movement variables were computationally extracted from the movement data and cross-correlated with the pulse of the ensemble's performance, the latter based on the spectral flux of the audio signal. Results of the analysis indicated that the ensemble's performance tended to be most highly synchronized with periods of maximal deceleration along the trajectory, followed by periods of high vertical velocity (a higher correlation than deceleration but a longer delay).},
author = {Luck, Geoff and Toiviainen, Petri},
doi = {MP.2006.24.2.189},
file = {:home/abetan16/Dropbox/MendeleyV3/Luck, Toiviainen - 2006 - Ensemble musicians' synchronization with conductors' gestures An automated feature-extraction analysis.pdf:pdf},
isbn = {07307829},
issn = {0730-7829},
journal = {Music Perception: An Interdisciplinary Journal},
number = {2},
pages = {189--200},
title = {{Ensemble Musicians' Synchronization With Conductors' Gestures: An Automated Feature-Extraction Analysis}},
url = {http://www.jstor.org/stable/10.1525/mp.2006.24.2.189},
volume = {24},
year = {2006}
}
@article{Maes2013,
abstract = {Research in the field of embodied music cognition has shown the importance of coupled processes of body activity (action) and multimodal representations of these actions (perception) in how music is processed. Technologies in the field of human?computer interaction (HCI) provide excellent means to intervene into, and extend, these coupled action-perception processes. In this article this model is applied to a concrete HCI application, called the ?Conducting Master.? The application facilitates multiple users to interact in real time with the system in order to explore and learn how musical meter can be articulated into body movements (i.e., meter-mimicking gestures). Techniques are provided to model and automatically recognize these gestures in order to provide multimodal feedback streams back to the users. These techniques are based on template-based methods that allow approaching meter-mimicking gestures explicitly from a spatiotemporal account. To conclude, some concrete setups are presented in which the functionality of the Conducting Master was evaluated.},
author = {Maes, Pieter-jan and Amelynck, Denis and Lesaffre, Micheline and Leman, Marc and Arvind, DK},
doi = {10.1080/10447318.2012.720197},
file = {:home/abetan16/Dropbox/MendeleyV3/Maes et al. - 2012 - The “Conducting Master” An Interactive, Real-Time Gesture Monitoring System Based on Spatiotemporal Motion Temp.pdf:pdf},
isbn = {http://dx.doi.org/10.1080/10447318.2012.720197},
issn = {1044-7318},
journal = {International Journal of Human-Computer Interaction},
month = {jul},
number = {March 2015},
pages = {120919075414004},
title = {{The “Conducting Master”: An interactive, real-time gesture monitoring system based on spatiotemporal motion templates}},
url = {http://www.tandfonline.com/doi/abs/10.1080/10447318.2012.720197},
volume = {29},
year = {2012}
}
@inproceedings{Makita2009,
abstract = {In annotation overlay applications using augmented reality (AR), view management is widely used for improving readability and intelligibility of the annotations. In order to recognize the visible portions of objects in the user's view, the positions, orientations, and shapes of the objects should be known in the case of conventional view management methods. However, it is difficult for a wearable AR system to obtain the positions, orientations and shapes of objects because the target object is usually moving or non-rigid. In this paper, we propose a view management method to overlay annotations of moving or non-rigid objects for networked wearable AR. The proposed method obtains positions and shapes of target objects via a network in order to estimate the visible portions of the target objects in the user's view. Annotations are located by minimizing penalties related to the overlap of an annotation, occlusion of target objects, length of a line between the annotation and the target object, and distance of the annotation in sequential frames. Through experiments, we have proven that the prototype system can correctly provide each user with annotations on multiple users of wearable AR systems.},
author = {Makita, K. and Kanbara, M. and Yokoya, N.},
booktitle = {2009 IEEE International Conference on Multimedia and Expo},
doi = {10.1109/ICME.2009.5202661},
file = {:home/abetan16/Dropbox/MendeleyV3/Makita, Kanbara, Yokoya - 2009 - View Management of Annotations for Wearable Augmented Reality.pdf:pdf},
isbn = {978-1-4244-4290-4},
issn = {1945-7871},
month = {jun},
pages = {982--985},
publisher = {Ieee},
title = {{View management of annotations for wearable augmented reality}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5202661},
year = {2009}
}
@inproceedings{Mann2013,
abstract = {This paper presents “FreeGlass”, a hands-free user-buildable wearable computer system based on Mann's “Digital Eye Glass” (also known as EyeTap) concept developed 35 years ago. Like Mann's “Eye Glass”, FreeGlass is based on free and Open Source principles consistent with a free and open i-Society. FreeGlass is suitable for researchers and creators of AR (Augmediated Reality) Glass, LifeGlogging, etc. in the information society. Two example applications of FreeGlass are also presented: Reality User Interface; and “ALIBEye”, a digital alibi through sousveillance (inverse surveillance). We note the potential for FreeGlass to bring about a transition from a surveillance society (cameras affixed to land and buildings), to a sousveillance-society (cameras held, carried, or worn by individuals). Important elements of a balanced sur/sous-veillance society are also presented with a view toward a middle-ground between a surveillance-only society where large entities record interactions but forbid individuals from keeping their own record of their sensory information, and the coming sousveillance society in which individuals are also equipped with veillance capacity, and the capacity to capture AND authenticate their own recordings as digital alibis.},
author = {Mann, S. and Ali, M.a. and Lo, R. and {Han Wu}},
booktitle = {2013 International Conference on Information Society (i-Society)},
file = {:home/abetan16/Dropbox/MendeleyV3/Mann et al. - 2013 - Freeglass for Developers,“haccessibility”, and Ar Glass Lifeglogging Research in a (Sursous) Veillance Society.pdf:pdf},
isbn = {VO  -},
keywords = {alibi sousveillance,ality,augmediated reality,augmented re-,digital eye glass,eyetap,freeglass,glogging,life-,mannglas,mediated reality,sousveillance,surveillance,veillance,wearable computing},
pages = {48--53},
title = {{FreeGlass for developers, “haccessibility”, and Digital Eye Glass + Lifeglogging research in a (sur/sous)veillance society}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:FreeGlass+for+developers,+“haccessibility”,+and+Digital+Eye+Glass+++Lifeglogging+Research+in+a+(Sur/Sous)Veillance+Society{\#}0},
year = {2013}
}
@inproceedings{Mann2004,
abstract = {I begin with the argument that continuous archival of personal experience requires certain criteria to be met. In particular, for continuous usage, it is essential that each ray of light entering the eye be collinear with a corresponding ray of light entering the device, in at least one mode of operation. This is called the EyeTap criterion, and devices meeting this criterion are called EyeTap devices. Secondly, I outline Mediated Reality as a necessary framework for continuous archival and retrieval of personal experience. Thirdly, I show some examples of personalized experience capture (i.e. visual art). Finally, I outline the social issues of such devices, in particular, the accidentally discovered inverse to surveillance that I call "sosuveillance". It is argued that an equilibrium between surveillance and sousveillance is implicit in the archival of personal experiences.},
address = {New York, New York, USA},
author = {Mann, Steve},
booktitle = {Proceedings of the the 1st ACM workshop on Continuous archival and retrieval of personal experiences - CARPE'04},
doi = {10.1145/1026653.1026654},
file = {:home/abetan6/Dropbox/Mendeley/Mann - 2004 - Continuous lifelong capture of personal experience with EyeTap.pdf:pdf},
isbn = {1581139322},
keywords = {computer mediated reality,computer vision,cyborglog,equiveillance,inverse surveillance,oversight,perveillance,sousveillance,sousvey,sur-,surveillance,undersight,vey,weblog},
pages = {1--21},
publisher = {ACM Press},
title = {{Continuous lifelong capture of personal experience with EyeTap}},
url = {http://portal.acm.org/citation.cfm?doid=1026653.1026654},
year = {2004}
}
@inproceedings{Mann1998,
abstract = {The purpose of this paper is to disclose the operational principles of 'WearCam', the basis for wearable tetherless computer-mediated reality both in its idealized form, as well as in some practical embodiments of the invention, including one of its latest embodiments. The specific inner workings of WearCam, in particular; details of its optical arrangement, have not previously been disclosed, other than by allowing a small number of individuals to look inside the glasses. General considerations, background, and relevant findings, in the area of long-term use of wearable, tetherless computer-mediated reality are also presented. Some general insight (arising from having designed and built more than 100 different kinds of personal imaging systems over the last 20 years) is also provided. Unlike the artificiality of many controlled laboratory experiments, much of the insight gained from these experiences relates to the natural complexity of real-life situations.},
address = {Pittsburgh},
author = {Mann, Steve},
booktitle = {Iswc},
doi = {10.1109/ISWC.1998.729538},
file = {:home/abetan16/Dropbox/MendeleyV3/Mann - 1998 - ``WearCam'' (Wearable Camera) Personal Imaging Systems for Long-term Use in Wearable Tetherless Computer-mediated Reality.pdf:pdf},
isbn = {0-8186-9074-7},
pages = {124--131},
publisher = {IEEE Computer Society},
title = {{'WearCam' (The wearable camera): personal imaging systems for long-term use in wearable tetherless computer-mediated reality and personal photo/videographic memory prosthesis}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=729538},
year = {1998}
}
@article{Mann1997,
abstract = {Miniaturization of components has enabled systems that are$\backslash$nwearable and nearly invisible, so that individuals can move about and$\backslash$ninteract freely, supported by their personal information domain. To$\backslash$nexplore such new concepts in imaging and lighting, I designed and built$\backslash$nthe wearable personal imaging system. My invention differed from$\backslash$npresent-day laptops and personal digital assistants in that I could keep$\backslash$nan eye on the screen while walking around and doing other things. Just$\backslash$nas computers have come to serve as organizational and personal$\backslash$ninformation repositories, computer clothing, when worn regularly, could$\backslash$nbecome a visual memory prosthetic and perception enhancer},
author = {Mann, Steve},
doi = {10.1109/2.566147},
file = {:home/abetan16/Dropbox/MendeleyV3/Mann - 1997 - Wearable Computing a First Step Toward Personal Imaging.pdf:pdf},
isbn = {0018-9162 VO  - 30},
issn = {00189162},
journal = {Computer},
number = {2},
pages = {25--32},
title = {{Wearable computing: A first step toward personal imaging}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=566147},
volume = {30},
year = {1997}
}
@inproceedings{Mann2011,
abstract = {We present a way finding system that uses a range camera and an array of vibrotactile elements we built into a helmet. The range camera is a Kinect 3D sensor from Microsoft that is meant to be kept stationary, and used to watch the user (i.e., to detect the person's gestures). Rather than using the camera to look at the user, we reverse the situation, by putting the Kinect range camera on a helmet for being worn by the user. In our case, the Kinect is in motion rather than stationary. Whereas stationary cameras have previously been used for gesture recognition, which the Kinect does very well, in our new modality, we take advantage of the Kinect's re- silience against rapidly changing background scenery, where the background in our case is now in motion (i.e., a con- ventional wearable camera would be presented with a con- stantly changing background that is difficult to manage by mere background subtraction). The goal of our project is collision avoidance for blind or visually impaired individuals, and for workers in harsh environments such as industrial environments with signifi- cant 3-dimensional obstacles, as well as for use in low-light environments. Categories},
address = {New York, New York, USA},
author = {Mann, Steve and Huang, Jason and Janzen, Ryan and Lo, Raymond and Rampersad, Valmiki and Chen, Alexander and Doha, Taqveer},
booktitle = {Proceedings of the 19th ACM international conference on Multimedia - MM '11},
doi = {10.1145/2072298.2072005},
file = {:home/abetan16/Dropbox/MendeleyV3/Mann et al. - 2011 - Blind navigation with a wearable range camera and vibrotactile helmet.pdf:pdf},
isbn = {9781450306164},
keywords = {blind navigation,collision avoidance,depth sensor,microsoft kinect,personal safety devices,vibrotactile array,vibrotactile helmet},
pages = {1325},
publisher = {ACM Press},
title = {{Blind navigation with a wearable range camera and vibrotactile helmet}},
url = {http://dl.acm.org/citation.cfm?id=2072298.2072005},
year = {2011}
}
@article{Mann2003,
abstract = {This paper describes using wearable computing devices to perform "sousveillance" (inverse surveillance) as a counter to organizational surveillance. A variety of wearable computing devices generated different kinds of responses, and allowed for the collection of data in different situations. Visible sousveillance often evoked counter-performances by front-line surveillance workers. The juxtaposition of sousveillance with surveillance generates new kinds of information in a social surveillance situation},
author = {Mann, Steve and Nolan, Jason and Wellman, Barry},
file = {:home/abetan16/Dropbox/MendeleyV3/Mann, Nolan, Wellman - 2003 - Sousveillance Inventing and Using Wearable Computing Devices for Data Collection in Surveillance Environme.pdf:pdf},
isbn = {1477-7487},
issn = {1477-7487},
journal = {Surveillance and Society},
keywords = {CCTV and video surveillanceFolder - interaction te},
number = {3},
pages = {331--355},
title = {{Sousveillance : Inventing and Using Wearable Computing Devices for Data Collection in Surveillance Environments ∗}},
url = {http://www.surveillance-and-society.org/articles1(3)/sousveillance.pdf},
volume = {1},
year = {2003}
}
@article{Manresa2005,
abstract = {The proposed work is part of a project that aims for the control of a videogame based on hand gesture recognition. This goal implies the restriction of real-time response and unconstrained environments. In this paper we present a real-time algorithm to track and recognise hand gestures for interacting with the videogame. This algorithm is based on three main steps: hand segmentation, hand tracking and gesture recognition from hand features. For the hand segmentation step we use the colour cue due to the characteristic colour values of human skin, its invariant properties and its computational simplicity. To prevent errors from hand segmentation we add a second step, hand tracking. Tracking is performed assuming a constant velocity model and using a pixel labeling approach. From the tracking process we extract several hand features that are fed to a finite state classifier which identifies the hand configuration. The hand can be classified into one of the four gesture classes or one of the four different movement directions. Finally, using the system's performance evaluation results we show the usability of the algorithm in a videogame environment. keywords: Hand Tracking, Gesture Recognition, Human-Computer Interaction.},
author = {Manresa, Cristina and Varona, Javier and Mas, Ramon and Perales, Francisco J.},
doi = {10.5565/rev/elcvia.109},
file = {:home/abetan16/Dropbox/MendeleyV3/Manresa, Varona - 2005 - Hand Tracking and Gesture Recognition for Human-computer Interaction.pdf:pdf},
issn = {1577-5097},
journal = {Electronic Letters on Computer Vision and Image Analysis},
keywords = {Gesture Recognition,Hand Tracking,Human-Computer Interaction,Perceptual User Interfaces},
number = {3},
pages = {96 -- 104},
title = {{Hand Tracking and Gesture Recognition for Human-Computer Interaction}},
url = {http://elcvia.cvc.uab.es/article/view/109},
volume = {5},
year = {2005}
}
@article{Marchetti1996,
abstract = {Decrease or growth of population comes from the interplay of death and birth (and locally, migration). We revive the logistic model, which was tested and found wanting in early-20th-century studies of aggregate human populations, and apply it instead to life expectancy (death) and fertility (birth), the key factors totaling population. For death, once an individual has legally entered society, the logistic portrays the situation crisply. Human life expectancy is reaching the culmination of a two-hundred year-process that forestalls death until about 80 for men and the mid-80's for women. No breakthroughs in longevity are in sight unless genetic engineering comes to help. For birth, the logistic covers quantitatively its actual morphology. However, because we have not been able to model this essential parameter in a predictive way over long periods, we cannot say whether the future of human population is runaway growth or slow implosion. Thus, we revisit the logistic analysis of aggregate human numbers. From a niche point of view, resources are the limits to numbers, and access to resources depends on technologies. The logistic makes clear that for homo faber, the limits to numbers keep shifting. These moving edges may most confound forecasting the long-run size of humanity.},
author = {Marchetti, C. and Meyer, P. S. and Ausubel, J. H.},
doi = {10.1016/0040-1625(96)00001-7},
issn = {03939375},
journal = {Human Evolution},
keywords = {Human population},
month = {may},
number = {4},
pages = {247--286},
pmid = {12292026},
title = {{Human population dynamics revisited with the logistic model: How much can be modeled and predicted?}},
url = {http://linkinghub.elsevier.com/retrieve/pii/0040162596000017},
volume = {19},
year = {2004}
}
@inproceedings{Marszalek2009,
abstract = {This paper exploits the context of natural dynamic scenes for human action recognition in video. Human actions are frequently constrained by the purpose and the physical properties of scenes and demonstrate high correlation with particular scene classes. For example, eating often happens in a kitchen while running is more common outdoors. The contribution of this paper is three-fold: (a) we automatically discover relevant scene classes and their correlation with human actions, (b) we show how to learn selected scene classes from video without manual supervision and (c) we develop a joint framework for action and scene recognition and demonstrate improved recognition of both in natural video. We use movie scripts as a means of automatic supervision for training. For selected action classes we identify correlated scene classes in text and then retrieve video samples of actions and scenes for training using script-to-video alignment. Our visual models for scenes and actions are formulated within the bag-of-features framework and are combined in a joint scene-action SVM-based classifier. We report experimental results and validate the method on a new large dataset with twelve action classes and ten scene classes acquired from 69 movies.},
address = {Miami, Fl},
author = {Marszalek, M. and Laptev, Ivan and Schmid, Cordelia and Marsza{\l}ek, Marcin and Laptev, Ivan and Schmid, Cordelia},
booktitle = {2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2009},
doi = {10.1109/CVPRW.2009.5206557},
file = {:home/abetan16/Dropbox/MendeleyV3/Marszalek et al. - 2009 - Actions in Context.pdf:pdf},
isbn = {9781424439935},
issn = {1063-6919},
keywords = {Humans,Layout,Motion pictures,Roads,SVM-based classifier,Scalability,Support vector machine classification,Support vector machines,Surveillance,Testing,Text mining,automatic supervision,gesture recognition,human action recognition,image classification,manual supervision,movie scripts,natural dynamic scenes,natural scenes,natural video,scene recognition,script-to-video alignment,support vector machines,video signal processing},
number = {i},
pages = {2929--2936},
publisher = {IEEE},
title = {{Actions in context}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=5206557},
year = {2009}
}
@article{Martinez2013,
abstract = {This paper presents a method to recognize attentional behaviors from a head-mounted binocular eye tracker in triadic interactions. By taking advantage of the first-person view, we simultaneously estimate the first-person and third-person gaze. The first-person gaze is computed using an appearance-based method relying on local features. In parallel, head pose tracking allows determining the coarse gaze of people in the scene camera. Finally, knowing the first- and third-person gaze direction, scores are computed which permit to assign attention patterns to each frame. Our contributions are the followings: (i) head pose estimation based on localized regression, (ii) attention analysis, in particular mutual and shared gaze, including the first-person gaze, (iii) experiments conducted using a head-mounted appearance-based gaze tracker. Experiments on recorded data show encouraging results.},
author = {Martinez, Francis and Carbone, Andrea and Pissaloux, Edwige},
doi = {10.1109/FG.2013.6553735},
file = {:home/abetan16/Dropbox/MendeleyV3/Martinez, Carbone, Pissaloux - 2013 - Combining First-person and Third-person Gaze for Attention Recognition.pdf:pdf},
isbn = {978-1-4673-5546-9},
journal = {2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)},
keywords = {Calibration,Estimation,Face,Feature extraction,Kernel,Pattern recognition,appearance-based method,attention analysis,attention pattern,attentional behavior recognition,behavioural sciences,cameras,coarse gaze,first-person gaze,first-person view,head pose estimation,head pose tracking,head-mounted appearance-based gaze tracker,head-mounted binocular eye tracker,helmet mounted displays,local feature,localized regression,mutual gaze,object tracking,pose estimation,regression analysis,scene camera,shared gaze,third-person gaze,triadic interaction},
month = {apr},
pages = {1--6},
publisher = {Ieee},
title = {{Combining first-person and third-person gaze for attention recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6553735},
year = {2013}
}
@inproceedings{Matsuo2014,
abstract = {In this paper, we propose a human activity recognition method from first-person videos, which provides a supplementary method to improve the recognition accuracy. Conventional methods detect objects and derive a user's behavior based on their taxonomy. One of the recent works has achieved accuracy improvement by determining key objects based on hand manipulation. However, such manipulation-based approach has a restriction on applicable scenes and object types because the user's hands don't always present significant information. In contrast, our proposed attention-based approach provides a solution to detect visually salient objects as key objects in a non-contact manner. Experimental results show that the proposed method classifies first-person actions more accurately than the previous method by 6.4 percentage points and its average accuracy reaches 43.3{\%}.},
annote = {Activity Recognition
- Object Detection
- Visual Saliency
- Temporal Pyramid
- SVM
- LFP
- User Head Movement
- Temporal Pyramids
-},
author = {Matsuo, Kenji and Yamada, Kentaro and Ueno, Satoshi and Naito, Sei},
booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2014.87},
file = {:home/abetan16/Dropbox/MendeleyV3/Matsuo et al. - 2014 - An Attention-Based Activity Recognition for Egocentric Video.pdf:pdf},
isbn = {978-1-4799-4308-1},
month = {jun},
pages = {565--570},
publisher = {Ieee},
title = {{An Attention-Based Activity Recognition for Egocentric Video}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6910036},
year = {2014}
}
@inproceedings{Lin2015,
abstract = {Quantitative measures of the space an individual can reach is essential for tracking the progression of a disease and the effects of therapeutic intervention. The reachable workspace can be used to track an individuals' ability to perform activities of daily living, such as feeding and grooming. There are few methods for quantifying upper limb performance, none of which are able to generate a reachable workspace volume from motion capture data. We introduce a method to estimate the reachable workspace volume for an individual by capturing their observed joint limits using a low cost depth camera. This method is then tested on seven individuals with varying upper limb performance. Based on these initial trials, we found that the reachable workspace volume decreased as muscular impairment increased. This shows the potential for this method to be used as a quantitative clinical assessment tool.},
address = {Cham},
author = {Matthew, Robert Peter and Kurillo, Gregorij and Han, Jay J. and Bajcsy, Ruzena},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-16199-0},
editor = {Agapito, Lourdes and Bronstein, Michael M. and Rother, Carsten},
file = {:home/abetan16/Dropbox/MendeleyV3/Matthew et al. - 2015 - Egocentric Object Recognition Leveraging the 3D Shape of the Grasping Hand.pdf:pdf},
isbn = {978-3-319-16198-3},
issn = {16113349},
keywords = {Assessment,Diagnosis,Functional workspace,Goniometry,Kinect,Muscular dystrophy,Rehabilitation,Skeletal Modelling},
pages = {570--583},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Computer Vision - ECCV 2014 Workshops}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84928819309{\&}partnerID=tZOtx3y1},
volume = {8927},
year = {2015}
}
@article{Mayo2001,
abstract = {We propose and demonstrate a methodology for building tractable normative intelligent tutoring systems (ITSs). A normative ITS uses a Bayesian network for long-term student modelling and decision theory to select the next tutorial action. Because normative theories are a general framework for rational behaviour, they can be used to both define and apply learning theories in a rational, and therefore optimal, way. This contrasts to the more traditional approach of using an ad-hoc scheme to implement the learning theory. A key step of the methodology is the induction and the continual adaptation of the Bayesian network student model from student performance data, a step that is distinct from other recent Bayesian net approaches in which the network structure and probabilities are either chosen beforehand by an expert, or by efficiency considerations. The methodology is demonstrated by a description and evaluation of CAPIT, a normative constraint-based tutor for English capitalisation and punctuation. Our evaluation results show that a class using the full normative version of CAPIT learned the domain rules at a faster rate than the class that used a non-normative version of the same system.},
author = {Mayo, Michael and Mitrovic, Antonija},
doi = {http://www.ijaied.org/iaied/ijaied/},
file = {:home/abetan16/Dropbox/MendeleyV3/Mayo, Mitrovic - 2001 - Optimising ITS Behaviour With Bayesian Networks and Decision Theory.pdf:pdf},
issn = {0302-9743},
journal = {International Journal of Artificial Intelligence in Education},
number = {1},
pages = {124--153},
title = {{Optimising ITS behaviour with Bayesian networks and decision theory}},
url = {http://researchcommons.waikato.ac.nz/handle/10289/1997},
volume = {12},
year = {2001}
}
@inproceedings{Mayol2005,
abstract = { In this paper we develop a first step towards the recognition of hand activity by detecting objects subject to manipulation, and use the results to build a visual summary of events. The motivation is to extract information from hand activity without requiring that the wearer is explicit as in gesture-based interaction. Our method uses simple image measurements within a probabilistic framework and allows real-time implementation.},
author = {Mayol, W. W. and Murray, D. W.},
booktitle = {Proceedings - International Symposium on Wearable Computers, ISWC},
doi = {10.1109/ISWC.2005.57},
file = {:home/abetan16/Dropbox/MendeleyV3/Mayol, Murray - 2005 - Wearable Hand Activity Recognition for Event Summarization.pdf:pdf},
isbn = {0769524192},
issn = {15504816},
pages = {122--129},
publisher = {IEEE},
title = {{Wearable hand activity recognition for event summarization}},
volume = {2005},
year = {2005}
}
@inproceedings{Mayol2000,
abstract = {This paper presents a wearable active visual sensor which is able$\backslash$nto achieve a level of decoupling of camera movement from the wearer's$\backslash$nposture and motions. This decoupling is achieved by a combination of$\backslash$nactive sensing, inertial information and visual sensor feedback. The$\backslash$nissues of sensor placement, robot kinematics and their relation to$\backslash$nwearability are discussed. The performance of the prototype robot is$\backslash$nevaluated for some essential visual tasks. The paper also discusses$\backslash$npotential application scenarios for this kind of wearable robot},
address = {Atlanta},
author = {Mayol, W. W. and Tordoff, B. J. and Murray, D. W.},
booktitle = {Personal and Ubiquitous Computing},
doi = {10.1007/s007790200004},
file = {:home/abetan16/Dropbox/MendeleyV3/Mayol, Tordoff, Murray - 2002 - Wearable visual robots.pdf:pdf},
isbn = {0-7695-0795-6},
issn = {16174909},
keywords = {Active vision,Person oriented robotics,Wearable computing},
number = {1},
pages = {37--48},
publisher = {IEEE Computer Society},
title = {{Wearable visual robots}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=888470},
volume = {6},
year = {2002}
}
@article{Mcmanus2009,
author = {Mcmanus, I C},
doi = {10.1017/CBO9780511576744},
file = {:home/abetan16/Dropbox/MendeleyV3/Mcmanus - 2009 - The history and geography of human handedness.pdf:pdf},
isbn = {9780511576744},
journal = {Language Lateralization and Psychosis},
pages = {37--57},
title = {{The history and geography of human handedness}},
url = {http://ebooks.cambridge.org/ref/id/CBO9780511576744},
year = {2009}
}
@inproceedings{Merler2007,
abstract = {The problem of using pictures of objects captured under ideal imaging conditions (here referred to as in vitro) to recognize objects in natural environments (in situ) is an emerging area of interest in computer vision and pattern recognition. Examples of tasks in this vein include assistive vision systems for the blind and object recognition for mobile robots; the proliferation of image databases on the web is bound to lead to more examples in the near future. Despite its importance, there is still a need for a freely available database to facilitate study of this kind of training/testing dichotomy. In this work one of our contributions is a new multimedia database of 120 grocery products, GroZi-120. For every product, two different recordings are available: in vitro images extracted from the web, and in situ images extracted from camcorder video collected inside a grocery store. As an additional contribution, we present the results of applying three commonly used object recognition/detection algorithms (color histogram matching, SIFT matching, and boosted Haar-like features) to the dataset. Finally, we analyze the successes and failures of these algorithms against product type and imaging conditions, both in terms of recognition rate and localization accuracy, in order to suggest ways forward for further research in this domain.},
address = {Minneapolis, MN},
author = {Merler, Michele and Galleguillos, Carolina and Belongie, Serge},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2007.383486},
file = {:home/abetan16/Dropbox/MendeleyV3/Merler, Galleguillos, Belongie - 2007 - Recognizing Groceries in Situ Using in Vitro Training Data.pdf:pdf},
isbn = {1424411807},
issn = {10636919},
month = {jun},
pages = {1--8},
publisher = {Ieee},
title = {{Recognizing groceries in situ using in vitro training data}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4270484},
year = {2007}
}
@misc{Miller2013,
author = {Miller, Claire C},
booktitle = {The New York Times},
file = {:home/abetan16/Dropbox/MendeleyV3/Miller - 2013 - Google Searches for Style.pdf:pdf},
pages = {1--3},
title = {{Google Searches for Style}},
url = {http://www.nytimes.com/2013/02/21/technology/google-looks-to-make-its-computer-glasses-stylish.html?{\_}r=1{\&}},
year = {2013}
}
@inproceedings{Min2014,
abstract = {We propose representing one's visual experiences (captured as a series of ego-centric videos) as a sparse-graph, where each node is an individual frame in the video, and nodes are connected if there exists a geometric transform between them. Such a graph is massive and contains millions of edges. Autobiographical egocentric visual data are highly redundant, and we show how the graph representation and graph clustering can be used to exploit redundancy in the data. We show that popular global clustering methods like spectral clustering and multi-level graph partitioning perform poorly for clustering egocentric visual data. We propose using local density clustering algorithms for clustering the data, and provide detailed qualitative and quantitative comparisons between the two approaches. The graph-representation and clustering are used to aggressively prune the database. By retaining only representative nodes from dense sub graphs, we achieve 90{\%} of peak recall by retaining only 1{\%} of data, with a significant 18{\%} improvement in absolute recall over naive uniform subsampling of the egocentric video data.},
annote = {Clustering
Graphs
Difference of Gaussiam
SIFT
Bag of Words
RANSAC
Spectral Clustering},
author = {Min, Wu and Li, Xiao and Tan, Cheston and Mandal, Bappaditya and Li, Liyuan and Lim, Joo Hwee},
booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2014.84},
file = {:home/abetan16/Dropbox/MendeleyV3/Min et al. - 2014 - Efficient Retrieval from Large-Scale Egocentric Visual Data Using a Sparse Graph Representation.pdf:pdf},
isbn = {978-1-4799-4308-1},
month = {jun},
pages = {541--548},
publisher = {Ieee},
title = {{Efficient Retrieval from Large-Scale Egocentric Visual Data Using a Sparse Graph Representation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6910033},
year = {2014}
}
@inproceedings{Sheikh2009,
abstract = {Background subtraction algorithms define the background as parts of a scene that are at rest. Traditionally, these algorithms assume a stationary camera, and identify moving objects by detecting areas in a video that change over time. In this paper, we extend the concept of `subtracting' areas at rest to apply to video captured from a freely moving camera. We do not assume that the background is well-approximated by a plane or that the camera center remains stationary during motion. The method operates entirely using 2D image measurements without requiring an explicit 3D reconstruction of the scene. A sparse model of background is built by robustly estimating a compact trajectory basis from trajectories of salient features across the video, and the background is `subtracted' by removing trajectories that lie within the space spanned by the basis. Foreground and background appearance models are then built, and an optimal pixel-wise foreground/background labeling is obtained by efficiently maximizing a posterior function.},
author = {Mnb, J J B and Ob, N B and Lb, L R B and Lqdg, J L B and Conference, International and Vision, Computer},
booktitle = {Internantional Conference on Computer Vision},
file = {:home/abetan16/Dropbox/MendeleyV3/Sheikh, Javed, Kanade - 2009 - Background Subtraction for Freely Moving Cameras.pdf:pdf},
isbn = {9781424444199},
number = {Iccv},
pages = {1219 -- 1225},
title = {{Background Subtraction for Freely Moving Cameras}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5459334},
year = {2009}
}
@inproceedings{Moore1999,
abstract = {Our goal is to exploit human motion and object context to perform action recognition and object classification. Towards this end, we introduce a framework for recognizing actions and objects by measuring image-, object- and action-based information from video. Hidden Markov models are combined with object context to classify hand actions, which are aggregated by a Bayesian classifier to summarize activities. We also use Bayesian methods to differentiate the class of unknown objects by evaluating detected actions along with low-level, extracted object features. Our approach is appropriate for locating and classifying objects under a variety of conditions including full occlusion. We show experiments where both familiar and previously unseen objects are recognized using action and context information},
address = {Kerkyra},
author = {Moore, D.J. and Essa, I.A. and Hayes, M.H.},
booktitle = {Proceedings of the Seventh IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.1999.791201},
file = {:home/abetan16/Dropbox/MendeleyV3/Moore, Essa, Hayes - 1999 - Exploiting Human Actions and Object Context for Recognition Tasks.pdf:pdf},
isbn = {0-7695-0164-8},
pages = {80--86 vol.1},
publisher = {IEEE},
title = {{Exploiting human actions and object context for recognition tasks}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=791201},
volume = {1},
year = {1999}
}
@article{Morerio2015,
abstract = {In this work, we propose a strategy for optimizing a superpixel algorithm for video signals, in order to get closer to real time performances which are on the one hand needed for egocentric vision applications and on the other must be bearable by wearable technologies. Instead of applying the algorithm frame by frame, we propose a technique inspired to Bayesian filtering and to video coding which allows to re-initialize superpixels using the information from the previous frame. This results in faster convergence and demonstrates how performances improve with respect to the standard application of the algorithm from scratch at each frame.},
author = {Morerio, Pietro and Georgiu, Gabriel Claudiu and Marcenaro, Lucio and Regazzoni, Carlo},
doi = {10.1109/LSP.2014.2362852},
file = {:home/abetan16/Dropbox/MendeleyV3/Morerio et al. - 2015 - Optimizing Superpixel Clustering for Real-Time Egocentric-Vision Applications.pdf:pdf},
issn = {1070-9908},
journal = {IEEE Signal Processing Letters},
keywords = {Algorithm design and analysis,Bayes methods,Bayesian Filtering,Bayesian filtering,Clustering algorithms,Convergence,Image segmentation,Real-time systems,Signal processing algorithms,belief networks,computer vision,egocentric vision,filtering theory,first-person vision,image resolution,optimization,real-time egocentric-vision applications,superpixel,superpixel clustering algorithm,video analysis,video coding,video signals,wearable technologies},
month = {apr},
number = {4},
pages = {469--473},
title = {{Optimizing Superpixel Clustering for Real-Time Egocentric-Vision Applications}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6920066},
volume = {22},
year = {2015}
}
@inproceedings{Morerio2013,
abstract = {The emergence of new pervasive wearable technolo- gies (e.g. action cameras and smart glasses) calls attention to the so called First Person Vision (FPV). In the future, more and more everyday-life videos will be shot from a first-person point of view, overturning the classical fixed-camera understanding of Vision, specializing the existing knowledge of moving cameras and bringing new challenges in the field of video processing. The trend in research is going to be oriented towards a new type of computer vision, centred on moving sensors and driven by the need for new applications for wearable devices. We identify hand tracking and gesture recognition as an essential topic in this field, motivated by the simple realization that we often look at our hand, even while performing the simplest tasks in everyday life. In addition, the next frontier in user interfaces are hands-free devices.},
address = {Istanbul},
author = {Morerio, Pietro and Marcenaro, Lucio and Regazzoni, Cs},
booktitle = {Isip40.It},
file = {:home/abetan16/Dropbox/MendeleyV3/Morerio, Marcenaro, Regazzoni - 2013 - Hand Detection in First Person Vision.pdf:pdf},
isbn = {9786058631113},
organization = {University of Genoa},
pages = {0--6},
title = {{Hand Detection in First Person Vision}},
url = {http://www.isip40.it/resources/papers/2013/fpv{\_}FUSION2013.pdf},
year = {2013}
}
@article{Morshidi2014,
abstract = {Abstract This paper presents a gravity optimised particle filter (GOPF) where the magnitude of the gravitational force for every particle is proportional to its weight. GOPF attracts nearby particles and replicates new particles as if moving the particles towards the peak of the likelihood distribution, improving the sampling efficiency. GOPF is incorporated into a technique for hand features tracking. A fast approach to hand features detection and labelling using convexity defects is also presented. Experimental results show that GOPF outperforms the standard particle filter and its variants, as well as state-of-the-art CamShift guided particle filter using a significantly reduced number of particles.},
author = {Morshidi, Malik and Tjahjadi, Tardi},
doi = {http://dx.doi.org/10.1016/j.patcog.2013.06.032},
file = {:home/abetan16/Dropbox/MendeleyV3/Morshidi, Tjahjadi - 2014 - Gravity Optimised Particle Filter for Hand Tracking.pdf:pdf},
isbn = {0031-3203},
issn = {0031-3203},
journal = {Pattern Recognition},
keywords = {Articulated hand tracking,CamShift,Convexity defects,Finger movement,Gravity,Particle filter},
number = {1},
pages = {194--207},
title = {{Gravity optimised particle filter for hand tracking}},
url = {http://www.sciencedirect.com/science/article/pii/S0031320313002872},
volume = {47},
year = {2014}
}
@article{Mount2011,
abstract = {In this study the impact of a planar and toroidal self-organizing map (SOM) configuration are investigated with respect to their impact on SOM trajectories. Such trajectories are an encoding of processes within an n-dimensional input data set and offer an important means of visualizing and analyzing process complexity in large n-dimensional problem domains. However, discontinuity associated with boundaries in the standard, planar SOM results in error that limits their analytical use. Previous studies have recommended the use of a toroidal SOM to reduce these errors, but fall short of a fully quantified analysis of the benefits that result. In this study, the comparative analysis of fifteen pairs of identically initiated and trained SOMs, of planar and toroidal configuration, allows the error in trajectory magnitude to be quantified and visualized; both within the SOM and data space. This offers an important insight into the impact of planar SOM boundaries that goes beyond the general, statistical measures of clustering efficacy associated with previous work. The adoption of a toroidal SOM can be seen to improve the distribution of error in the trajectory sets, with the specific spatial configuration of SOM neurons associated with the largest errors changing from those at the corners of the planar SOM to a more complex and less predictable pattern in the toroidal SOM. However, this improvement is limited to the smallest 60{\%} of errors, with torus and planar SOMs performing similarly for the largest 40{\%}.},
author = {Mount, N. J. and Weaver, D.},
doi = {10.1007/s10044-011-0210-5},
file = {:home/abetan16/Dropbox/MendeleyV3/Mount, Weaver - 2011 - Self-organizing maps and boundary effects Quantifying the benefits of torus wrapping for mapping SOM trajectories.pdf:pdf},
isbn = {1433-7541},
issn = {14337541},
journal = {Pattern Analysis and Applications},
keywords = {Cluster analysis,Neural nets,Self-organizing maps},
number = {2},
pages = {139--148},
title = {{Self-organizing maps and boundary effects: Quantifying the benefits of torus wrapping for mapping SOM trajectories}},
volume = {14},
year = {2011}
}
@article{Mukhopadhyay2015,
abstract = {An increase in world population along with a significant aging portion is forcing rapid rises in healthcare costs. The healthcare system is going through a transformation in which continuous monitoring of inhabitants is possible even without hospitalization. The advancement of sensing technologies, embedded systems, wireless communication technologies, nano technologies, and miniaturization makes it possible to develop smart systems to monitor activities of human beings continuously. Wearable sensors detect abnormal and/or unforeseen situations by monitoring physiological parameters along with other symptoms. Therefore, necessary help can be provided in times of dire need. This paper reviews the latest reported systems on activity monitoring of humans based on wearable sensors and issues to be addressed to tackle the challenges.},
author = {Mukhopadhyay, Subhas Chandra},
doi = {10.1109/JSEN.2014.2370945},
file = {:home/abetan16/Dropbox/MendeleyV3/Mukhopadhyay - 2015 - Wearable Sensors for Human Activity Monitoring A Review.pdf:pdf},
isbn = {1530-437X VO - 15},
issn = {1530-437X},
journal = {IEEE Sensors Journal},
keywords = {Biomedical monitoring,Monitoring,Temperature measurement,Wearable sensors,Wireless communication,Wireless sensor networks,activity monitoring,assisted living,body area networks,body sensor networks,physiological parameters monitoring,sensor networks,smart home,smart sensors,wearable sensors,wireless sensor networks},
number = {3},
pages = {1321--1330},
title = {{Wearable Sensors for Human Activity Monitoring: A Review}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6974987},
volume = {15},
year = {2015}
}
@article{Murphy2003,
abstract = {This paper presents a system to control the playback of audio files by means of the standard classical conducting technique. Computer vision techniques are developed to track a conductor's baton, and the gesture is subsequently analysed. Audio parameters are extracted from the sound-file and are further processed for audio beat tracking. The sound-file playback speed is adjusted in order to bring the audio beat points into alignment with the gesture beat points. The complete system forms all parts necessary to simulate an orchestra reacting to a conductor's baton.},
author = {Murphy, Declan and Andersen, Tue Haste and Jensen, Kristoffer},
doi = {10.1007/978-3-540-24598-8_49},
file = {:home/abetan16/Dropbox/MendeleyV3/Murphy, Andersen, Jensen - 2004 - Conducting Audio Files via Computer Vision.pdf:pdf},
issn = {03029743},
journal = {Gesture-Based Communication In Human-Computer Interaction},
pages = {101--102},
title = {{Conducting Audio Files via Computer Vision}},
url = {http://www.springerlink.com/content/w5amk8uw3uuygcyv/},
volume = {2915/2004},
year = {2004}
}
@phdthesis{Murphy2002,
abstract = {Chair-Stuart Russell},
author = {Murphy, K P},
booktitle = {Annals of Physics},
doi = {10.1.1.129.7714},
file = {:home/abetan16/Dropbox/Articles/Mendeley//Murphy - 2002 - Dynamic Bayesian Networks Representation, Inference and Learning.pdf:pdf},
pages = {225},
school = {University of California, Berkeley},
title = {{Dynamic Bayesian Networks: Representation, Inference and Learning}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.93.778{\&}rep=rep1{\&}type=pdf},
volume = {Ph. D.},
year = {2002}
}
@article{Murphy2006,
abstract = {Traditional approaches to object detection only look at local pieces of the image, whether it be within a sliding window or the regions around an interest point detector. However, such local pieces can be ambiguous, especially when the object of interest is small, or imaging conditions are otherwise unfavorable. This ambiguity can be reduced by using global features of the image — which we call the “gist” of the scene — as an additional source of evidence. We show that by combining local and global features, we get significantly improved detection rates. In addition, since the gist is much cheaper to compute than most local detectors, we can potentially gain a large increase in speed as well.},
author = {Murphy, Kevin and Torralba, Antonio and Eaton, Daniel and Freeman, William},
doi = {10.1007/11957959_20},
file = {:home/abetan16/Dropbox/MendeleyV3/Murphy et al. - 2006 - Object Detection and Localization Using Local and Global Features.pdf:pdf},
journal = {Toward Category-Level Object Recognition - Lecture Notes in Computer Science},
pages = {382--400},
title = {{Object detection and localization using local and global features}},
url = {http://link.springer.com/chapter/10.1007/11957959{\_}20},
volume = {4170},
year = {2006}
}
@inproceedings{Castle2007a,
abstract = {This paper presents a system which combines single-camera SLAM (simultaneous localization and mapping) with established methods for feature recognition. Besides using standard salient image features to build an on-line map of the camera's environment, this system is capable of identifying and localizing known planar objects in the scene, and incorporating their geometry into the world map. Continued measurement of these mapped objects improves both the accuracy of estimated maps and the robustness of the tracking system. In the context of hand-held or wearable vision, the system's ability to enhance generated maps with known objects increases the map's value to human operators, and also enables meaningful automatic annotation of the user's surroundings. The presented solution lies between the high order enriching of maps such as scene classification, and the efforts to introduce higher geometric primitives such as lines into probabilistic maps.},
author = {Murray, D.W. and Gawley, D.J. and Klein, G.},
booktitle = {International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2007.364109},
file = {:home/abetan16/Dropbox/Mendeley/Castle et al. - 2007 - Towards simultaneous recognition, localization and mapping for hand-held and wearable cameras.pdf:pdf},
isbn = {1424406021},
issn = {10504729},
month = {apr},
pages = {4102--4107},
publisher = {Ieee},
title = {{DW Towards simultaneous recognition, localization and mapping for hand-held and wearable cameras}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Towards+simultaneous+recognition,+localization+and+mapping+for+hand-held+and+wearable+cameras{\#}0},
volume = {10},
year = {2007}
}
@incollection{Gauthier2000,
abstract = {DT Tutor uses a decision-theoretic approach to select tutorial actions for coached problem solving that are optimal given the tutor's beliefs and objectives. It employs a model of learning to predict the possible outcomes of each action, weighs the utility of each outcome by the tutor's belief that it will occur, and selects the action with highest expected utility. For each tutor and student action, an updated student model is added to a dynamic decision network to reflect the changing student state. The tutor considers multiple objectives, including the student's problem-related knowledge, focus of attention, independence, and morale, as well as action relevance and dialog coherence. Evaluation in a calculus domain shows that DT Tutor can select rational and interesting tutorial actions for real-world-sized problems in satisfactory response time. The tutor does not yet have a suitable user interface, so it has not been evaluated with human students.},
author = {Murray, RC and VanLehn, K},
booktitle = {Intelligent Tutoring Systems},
editor = {Gauthier, Gilles and Frasson, Claude and VanLehn, Kurt},
file = {:home/abetan16/Dropbox/MendeleyV3/Murray, VanLehn - 2000 - DT Tutor A Decision-TheoreticDynamic Approach for Optimal Selection of Tutorial Actions.pdf:pdf},
pages = {153--162},
publisher = {Springer Berlin Heidelberg},
title = {{DT Tutor: A Decision-TheoreticDynamic Approach for Optimal Selection of Tutorial Actions}},
url = {http://link.springer.com/chapter/10.1007/3-540-45108-0{\_}19},
year = {2000}
}
@article{Nam2002,
abstract = {n this paper, we propose a new video summarization procedure that produces a dynamic (video) abstract of the original video sequence. Our technique compactly summarizes a video data by preserving its original temporal characteristics (visual activity) and semantically essential information. It relies on an adaptive nonlinear sampling. The local sampling rate is directly proportional to the amount of visual activity in localized sub-shot units of the video. To get very short, yet semantically meaningful summaries, we also present an event-oriented abstraction scheme, in which two semantic events; emotional dialogue and violent action, are characterized and abstracted into the video summary before all other events. If the length of the summary permits, other non key events are then added. The resulting video abstract is highly compact.},
author = {Nam, J and Tewfik, A},
doi = {10.1023/A:1013241718521},
file = {:home/abetan16/Dropbox/MendeleyV3/Nam, Tewfik - 2002 - Event-Driven Video Abstraction and Visualization.pdf:pdf},
issn = {1380-7501},
journal = {Multimedia Tools and Applications},
keywords = {dynamic summarization,key-event,video abstract,visualization},
number = {1-2},
pages = {55--77},
title = {{Event-Driven Video Abstraction and Visualization}},
url = {http://link.springer.com/article/10.1023/A:1013241718521},
volume = {16},
year = {2002}
}
@inproceedings{Narayan2014,
abstract = {In this work, we evaluate the performance of the popular dense trajectories approach on first-person action recog- nition datasets. A person moving around with a wearable camera will actively interact with humans and objects and also passively observe others interacting. Hence, in order to represent real-world scenarios, the dataset must con- tain actions from first-person perspective as well as third- person perspective. For this purpose, we introduce a new dataset which contains actions from both the perspectives captured using a head-mounted camera. We employ a mo- tion pyramidal structure for grouping the dense trajectory features. The relative strengths of motion along the trajecto- ries are used to compute different bag-of-words descriptors and concatenated to form a single descriptor for the ac- tion. The motion pyramidal approach performs better than the baseline improved trajectory descriptors. The method achieves 96.7{\%} on the JPL interaction dataset and 61.8{\%} on our NUS interaction dataset. The same is used to de- tect actions in long video sequences and achieves average precision of 0.79 on JPL interaction dataset.},
annote = {Activity recognition
- Optical Flow
- FP
- Bag of words
- K-means
- SVM
- Fisher vector
- GMM

        

      },
author = {Narayan, Sanath and Kankanhalli, Mohan S. and Ramakrishnan, Kalpathi R.},
booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2014.82},
file = {:home/abetan16/Dropbox/MendeleyV3/Narayan, Kankanhalli, Ramakrishnan - 2014 - Action and Interaction Recognition in First-Person Videos.pdf:pdf},
isbn = {978-1-4799-4308-1},
issn = {21607516},
month = {jun},
pages = {526--532},
publisher = {Ieee},
title = {{Action and Interaction Recognition in First-Person Videos}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6910031},
year = {2014}
}
@article{McMillan2007,
address = {Cambridge},
author = {Necaise, Rance D.},
doi = {10.1017/CBO9780511547010},
file = {:home/abetan16/Dropbox/MendeleyV3/Necaise - 2011 - Data Structures and Algorithms Using Python.pdf:pdf},
isbn = {9780470618295},
pages = {520},
publisher = {Cambridge University Press},
title = {{Data Structures and Algorithms Using Python}},
url = {http://ebooks.cambridge.org/ref/id/CBO9780511804793},
year = {2011}
}
@article{Needham2005,
abstract = {This paper presents a cognitive vision system capable of autonomously learning protocols from perceptual observations of dynamic scenes. The work is motivated by the aim of creating a synthetic agent that can observe a scene containing interactions between unknown objects and agents, and learn models of these sufficient to act in accordance with the implicit protocols present in the scene. Discrete concepts (utterances and object properties), and temporal protocols involving these concepts, are learned in an unsupervised manner from continuous sensor input alone. Crucial to this learning process are methods for spatio-temporal attention applied to the audio and visual sensor data. These identify subsets of the sensor data relating to discrete concepts. Clustering within continuous feature spaces is used to learn object property and utterance models from processed sensor data, forming a symbolic description. The progol Inductive Logic Programming system is subsequently used to learn symbolic models of the temporal protocols presented in the presence of noise and over-representation in the symbolic data input to it. The models learned are used to drive a synthetic agent that can interact with the world in a semi-natural way. The system has been evaluated in the domain of table-top game playing and has been shown to be successful at learning protocol behaviours in such real-world audio-visual environments.},
author = {Needham, Chris J. and Santos, Paulo E. and Magee, Derek R. and Devin, Vincent and Hogg, David C. and Cohn, Anthony G.},
doi = {10.1016/j.artint.2005.04.006},
file = {:home/abetan16/Dropbox/MendeleyV3/Needham et al. - 2005 - Protocols from perceptual observations.pdf:pdf},
issn = {00043702},
journal = {Artificial Intelligence},
month = {sep},
number = {1-2},
pages = {103--136},
title = {{Protocols from perceptual observations}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0004370205000986},
volume = {167},
year = {2005}
}
@article{Nelder1965,
abstract = {A method is described for the minimization of a function of n variables, which depends on the comparison of function values at the (n 4- 1) vertices of a general simplex, followed by the replacement of the vertex with the highest value by another point. The simplex adapts itself to the local landscape, and contracts on to the final minimum. The method is shown to be effective and computationally compact. A procedure is given for the estimation of the Hessian matrix in the neighbourhood of the minimum, needed in statistical estimation problems.},
author = {Nelder, J. a. and Mead, R.},
doi = {10.1093/comjnl/7.4.308},
file = {:home/abetan16/Dropbox/MendeleyV3/Nelder, Mead - 1965 - A Simplex Method for Function Minimization.pdf:pdf},
isbn = {9781605580852},
issn = {0010-4620},
journal = {The Computer Journal},
number = {4},
pages = {308--313},
title = {{A simplex method for function minimization}},
url = {http://comjnl.oxfordjournals.org/content/7/4/308.abstract},
volume = {7},
year = {1964}
}
@inproceedings{Sawahata2002,
author = {Ng, Hw and Sawahata, Y. and Aizawa, K.},
booktitle = {Multimedia and Expo, 2002. {\ldots}},
doi = {10.1109/ICME.2002.1035784},
file = {:home/abetan16/Dropbox/MendeleyV3/Ng, Sawahata, Aizawa - 2002 - Summarization of Wearable Videos Using Support Vector Machine.pdf:pdf},
isbn = {0-7803-7304-9},
number = {1},
pages = {325--328},
publisher = {IEEE},
title = {{Summarization of wearable videos using support vector machine}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1035784$\backslash$nhttp://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1035784},
volume = {Aug},
year = {2002}
}
@inproceedings{Ngo2003,
abstract = {We propose a unified approach for summarization based on the analysis of video structures and video highlights. Our approach emphasizes both the content balance and perceptual quality of a summary. Normalized cut algorithm is employed to globally and optimally partition a video into clusters. A motion attention model based on human perception is employed to compute the perceptual quality of shots and clusters. The clusters, together with the computed attention values, form a temporal graph similar to Markov chain that inherently describes the evolution and perceptual importance of video clusters. In our application, the flow of a temporal graph is utilized to group similar clusters into scenes, while the attention values are used as guidelines to select appropriate subshots in scenes for summarization.},
address = {Nice, France},
author = {Ngo, Chong-Wah Ngo Chong-Wah and Ma, Yu-Fei Ma Yu-Fei and Zhang, Hong-Jiang Zhang Hong-Jiang},
booktitle = {Proceedings Ninth IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2003.1238320},
file = {:home/abetan16/Dropbox/MendeleyV3/Ngo, Ma, Zhang - 2003 - Automatic Video Summarization by Graph Modeling.pdf:pdf},
isbn = {0-7695-1950-4},
issn = {10518215},
number = {Iccv},
pages = {0--5},
pmid = {4043228420150458867},
publisher = {IEEE},
title = {{Automatic video summarization by graph modeling}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1238320},
volume = {1},
year = {2003}
}
@inproceedings{Nguyen2009,
abstract = {In this paper, we present a study of responses to the idea of being recorded by a ubicomp recording technology called SenseCam. This study focused on real-life situations in two North American and two European locations. We present the findings of this study and their implications, specifically how those who might be recorded perceive and react to SenseCam. We describe what system parameters, social processes, and policies are required to meet the needs of both the primary users and these secondary stakeholders and how being situated within a particular locale can influence responses. Our results indicate that people would tolerate potential incursions from SenseCam for particular purposes. Furthermore, they would typically prefer to be informed about and to consent to recording as well as to grant permission before any data is shared. These preferences, however, are unlikely to instigate a request for deletion or other action on their part. These results inform future design of recording technologies like SenseCam and provide a broader understanding of how ubicomp technologies might be taken up across different cultural and political regions.},
author = {Nguyen, David H. and Marcu, Gabriela and Hayes, Gillian R. and Truong, Khai N. and Scott, James and Langheinrich, Marc and Roduner, Christof},
booktitle = {Proceedings of the 11th international conference on Ubiquitous computing - Ubicomp '09},
doi = {10.1145/1620545.1620571},
file = {:home/abetan16/Dropbox/MendeleyV3/Nguyen et al. - 2009 - Encountering Sensecam Personal Recording Technologies in Everyday Life.pdf:pdf},
isbn = {9781605584317},
pages = {165},
title = {{Encountering SenseCam}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.147.9295{\&}rep=rep1{\&}type=pdf},
year = {2009}
}
@article{Nguyen2016,
author = {Nguyen, Thi-Hoa-Cuc and Nebel, Jean-Christophe and Florez-Revuelta, Francisco},
doi = {10.3390/s16010072},
file = {:home/abetan16/Dropbox/MendeleyV3/Nguyen, Nebel, Florez-Revuelta - 2016 - Recognition of Activities of Daily Living with Egocentric Vision A Review.pdf:pdf},
isbn = {4477568266},
issn = {1424-8220},
journal = {Sensors},
keywords = {activity recognition,ambient assisted living,egocentric vision,wearable cameras},
number = {1},
pages = {72},
title = {{Recognition of Activities of Daily Living with Egocentric Vision: A Review}},
url = {http://www.mdpi.com/1424-8220/16/1/72},
volume = {16},
year = {2016}
}
@inproceedings{Rogez2014,
abstract = {We focus on the task of everyday hand pose estimation from egocentric viewpoints. For this task, we show that depth sensors are particularly informative for extracting near-field interactions of the camera wearer with his/her environment. Despite the recent advances in full-body pose estimation using Kinect-like sensors, reliable monocular hand pose estimation in RGB-D images is still an unsolved problem. The problem is considerably exacerbated when analyzing hands performing daily activities from a first-person viewpoint, due to severe occlusions arising from object manipulations and a limited field-of-view. Our system addresses these difficulties by exploiting strong priors over viewpoint and pose in a discriminative tracking-by-detection framework. Our priors are operationalized through a photorealistic synthetic model of egocentric scenes, which is used to generate training data for learning depth-based pose classifiers. We evaluate our approach on an annotated dataset of real egocentric object manipulation scenes and compare to both commercial and academic approaches. Our method provides state-of-the-art performance for both hand detection and pose estimation in egocentric RGB-D images.},
address = {Zurich, Switzerland},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.0065v1},
author = {Nov, C V},
booktitle = {ECCV Workshop on Consumer Depth Camera for Computer Vision},
eprint = {arXiv:1412.0065v1},
file = {:home/abetan16/Dropbox/MendeleyV3/Nov - 2014 - 3D Hand Pose Detection in Egocentric RGB-D Images.pdf:pdf},
month = {nov},
number = {1},
pages = {1--14},
publisher = {Springer},
title = {{3D Hand Pose Detection in Egocentric RGB-D Images}},
volume = {Sep},
year = {2014}
}
@inproceedings{Ogaki2012,
abstract = {We focus on the use of first-person eye movement and ego-motion as a means of understanding and recognizing indoor activities from an “inside-out” camera system. We show that when eye movement captured by an inside look- ing camera is used in tandem with ego-motion features ex- tracted from an outside looking camera, the classification accuracy of first-person actions can be improved. We also present a dataset of over two hours of realistic indoor desk- top actions, including both eye tracking information and a high quality outside camera video. We run experiments and show that our joint feature is effective and robust over mul- tiple users. 1.},
author = {Ogaki, K and Kitani, K and Sugano, Y and Sato, Y},
booktitle = {2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2012.6239188},
file = {:home/abetan16/Dropbox/MendeleyV3/Ogaki et al. - 2012 - Coupling Eye-motion and Ego-motion Features for First-person Activity Recognition.pdf:pdf},
isbn = {978-1-4673-1612-5},
issn = {2160-7508},
month = {jun},
pages = {1--7},
publisher = {Ieee},
title = {{Coupling eye-motion and ego-motion features for first-person activity recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6239188},
year = {2012}
}
@inproceedings{Okamoto2014,
abstract = {In this paper, we propose a method to summarize an egocentric moving video (a video recorded by a moving wearable camera) for generating a walking route guidance video. To summarize an egocentric video, we analyze it by applying pedestrian crosswalk detection as well as ego-motion classification, and estimate an importance score of each section of the given video. Based on the estimated importance scores, we dynamically control video playing speed instead of generating a summarized video file in advance. In the experiments, we prepared an egocentric moving video dataset including more than one-hour-long videos totally, and evaluated crosswalk detection and ego-motion classification methods. Evaluation of the whole system by user study has been proved that the proposed method is much better than a simple baseline summarization method without video analysis.},
annote = {Video Segmentation
- crosswalk detection
Geometric context
Bag of Features
SVM
SIFT
- egomotion - Activity recognition
optical flow
SVM},
author = {Okamoto, M and Yanai, K},
booktitle = {Image and Video Technology},
doi = {10.1007/978-3-642-53842-1-37},
file = {:home/abetan16/Dropbox/MendeleyV3/Okamoto, Yanai - 2014 - Summarization of Egocentric Moving Videos for Generating Walking Route Guidance.pdf:pdf},
isbn = {9783642538414},
issn = {03029743},
keywords = {egocentric vision,video summarization,walking route},
pages = {431--442},
title = {{Summarization of Egocentric Moving Videos for Generating Walking Route Guidance}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-53842-1{\_}37},
year = {2014}
}
@inproceedings{Olier2015,
address = {Palma de Mallorca},
author = {Olier, J.S. and Regazzoni, C.S. and Marcenaro, L. and Rauterberg, M.},
booktitle = {International Work Conference on Artificial Neural Networks},
title = {{Convolutional Neural Networks for Detecting and Mapping Crowds in First Person Vision Applications}},
year = {2015}
}
@incollection{Oliva2005,
abstract = {Studies in scene perception have shown that observers recognize a real-world scene at a single glance. During this expeditious process of seeing, the visual system forms a spatial representation of the outside world that is rich enough to grasp the meaning of the scene, to recognize a few objects and other salient information in the image, and to facilitate object detection and the deployment of attention. This representation refers to the gist of a scene that includes all levels of processing, from low-level features (for example, color, spatial frequencies) to intermediate image properties (for example, surface, volume) and high-level information (for example, objects, activation of semantic knowledge). Therefore, gist can be studied at both perceptual and conceptual levels. ?? 2005 Elsevier Inc. All rights reserved.},
author = {Oliva, Aude},
booktitle = {Neurobiology of Attention},
doi = {10.1016/B978-012375731-9/50045-8},
file = {:home/abetan16/Dropbox/MendeleyV3/Oliva - 2005 - Gist of the scene.pdf:pdf},
isbn = {9780123757319},
issn = {15205126},
pages = {251--256},
pmid = {21114312},
publisher = {Elsevier Inc.},
title = {{Gist of the scene}},
url = {http://cvcl.mit.edu/papers/oliva04.pdf},
year = {2005}
}
@article{Oliva2001,
abstract = {In this paper, we propose a computational model of the recognition of real world scenes that bypasses the segmentation and the processing of individual objects or regions. The procedure is based on a very low dimen- sional representation of the scene, that we term the Spatial Envelope. We propose a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. Then, we show that these dimensions may be reliably estimated using spectral and coarsely localized information. The model generates a multidimensional space in which scenes sharing membership in semantic categories (e.g., streets, highways, coasts) are projected closed together. The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a holistic representation of the scene informs about its probable semantic category.},
annote = {If one day is required to understan the concepts behind spectrum analysis of an image this is a good start point.},
author = {Oliva, Aude and Torralba, Antonio},
doi = {10.1023/A:1011139631724},
file = {:home/abetan16/Dropbox/MendeleyV3/Oliva, Torralba - 2001 - Modeling the shape of the scene A holistic representation of the spatial envelope.pdf:pdf},
isbn = {0920-5691},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {Energy spectrum,Natural images,Principal components,Scene recognition,Spatial layout},
number = {3},
pages = {145--175},
pmid = {1113597138},
title = {{Modeling the shape of the scene: A holistic representation of the spatial envelope}},
url = {http://link.springer.com/article/10.1023/A:1011139631724},
volume = {42},
year = {2001}
}
@book{Ortony1988,
abstract = {What causes us to experience emotions? What makes emotions vary in intensity? How are different emotions related to one another and to the language used to talk about them? What are the information processing mechanisms and structures that underlie the elicitation and intensification of emotions? Despite an abundance of psychological research on emotions, many fundamental questions like these have yet to be answered. The Cognitive Structure of Emotions addresses such questions by presenting a systematic and detailed account of the cognitive antecedents of emotions. The authors propose three aspects of the world to which people can react emotionally. People can react to events of concern to them, to the actions of those they consider responsible for such events, and to objects. It is argued that these three classes of reactions lead to three classes of emotions, each based on evaluations in terms of different kinds of knowledge representations. The authors characterize a wide range of emotions, offering concrete proposals about the factors that influence the intensity of each. In doing so, they forge a clear separation between emotions themselves and the language of emotion, and offer the first systematic, comprehensive, and computationally tractable account of the cognitions that underlie distinct types of human emotions.},
address = {CAMBRIDGE, ENGLAND, UK},
annote = {1. Is an interesting but long book. I will try to find a review which sumarize it.
2. Is a seminal text of emotion categorization that has to be taken into account and cited in each article.},
author = {Ortony, Andrew and Clore, Gerald L. and Collins, Allan},
booktitle = {Cambridge University Press},
doi = {10.2307/2074241},
edition = {1},
file = {:home/abetan16/Dropbox/MendeleyV3/Ortony, Clore, Collins - 1990 - The Cognitive Structure of Emotions.pdf:pdf},
isbn = {0521353645},
issn = {00943061},
pmid = {2896677},
publisher = {CAMBRIDGE UNIVERSITY PRESS},
title = {{The Cognitive Structure of Emotions}},
year = {1990}
}
@article{Ozcan2013,
author = {Ozcan, Koray and Member, Student and Mahabalagiri, Anvith Katte and Member, Student},
file = {:home/abetan16/Dropbox/MendeleyV3/Ozcan et al. - 2013 - Automatic Fall Detection and Activity Classi fi cation by a Wearable Embedded Smart Camera.pdf:pdf},
journal = {IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
number = {2},
pages = {125--136},
title = {{Automatic Fall Detection and Activity Classi fi cation by a Wearable Embedded Smart Camera}},
volume = {3},
year = {2013}
}
@inproceedings{Pai2010,
abstract = {Skin colour is an important feature for face detection and recognition in colour images. To obtain the possible face regions in colour images, skin colour models are always constructed by statistical analysis. Owing to low accuracy of the static models, researchers have discussed several dynamic models to correct input images. Unfortunately, it is possible that some objects whose colour is the same as the definition exist, and the previous methods cannot separate real skin item from skin colour background. Thus, this paper presents a honeycomb model to recognise the real human skin colour. The performance of the new skin colour detector technique has been tested under complex lighting source and background environments. It is observed that the proposed model can effectively improve the segmentation results. Especially, the honeycomb model is capable of separating the human face which connected with other face or skin colour background.},
address = {Auckland},
author = {Pai, Yu-Ting and Lee, Li-Te and Ruan, Shanq-Jang and Chen, Yen-Hsiang and Mohanty, Saraju and Kougianos, Elias},
booktitle = {International Journal of Computer Applications in Technology},
doi = {10.1504/ijcat.2010.034736},
file = {:home/abetan16/Dropbox/MendeleyV3/Pai et al. - 2010 - Honeycomb model based skin colour detector for face detection.pdf:pdf},
isbn = {9780473135324},
issn = {09528091},
number = {1},
pages = {93--100},
publisher = {IEEE Computer Society},
title = {{Honeycomb model based skin colour detector for face detection}},
url = {http://dx.doi.org/10.1504/IJCAT.2010.034736},
volume = {39},
year = {2010}
}
@article{Paradiso1997,
abstract = {This paper describes three different types of real-time optical tracking systems developed at the MIT Media Laboratory for use as expressive human-computer interfaces in music, dance, and interactive multimedia performances. Two of these, a multimodal conducing baton and a scanning laser rangefinder, are essentially hardware-based, while the third is a computer vision system that can identify and track different segments of the performers body. We discuss the technical concepts behind these devices and outline their applications in music and dance environments.  1.},
author = {Paradiso, Joseph A. and Sparacino, F.},
file = {:home/abetan16/Dropbox/MendeleyV3/Paradiso, Sparacino - 1997 - Optical Tracking for Music and Dance Performance.pdf:pdf},
journal = {Proceedings of the Fourth Conference on Optical 3D Measurement Techniques, ETH, Zurich, September 1997},
number = {September},
pages = {11--18},
title = {{Optical Tracking for Music and Dance Performance}},
year = {1997}
}
@article{Park2012,
abstract = {A gaze concurrence is a point in 3D where the gaze directions of two or more people intersect. It is a strong indicator of social saliency because the attention of the participating group is focused on that point. In scenes occupied by large groups of people, multiple concurrences may occur and transition over time. In this paper, we present a method to construct a 3D social saliency field and lo- cate multiple gaze concurrences that occur in a social scene from videos taken by head-mounted cameras. We model the gaze as a cone-shaped distribution emanat- ing from the center of the eyes, capturing the variation of eye-in-head motion. We calibrate the parameters of this distribution by exploiting the fixed relationship between the primary gaze ray and the head-mounted camera pose. The result- ing gaze model enables us to build a social saliency field in 3D. We estimate the number and 3D locations of the gaze concurrences via provably convergent mode- seeking in the social saliency field. Our algorithm is applied to reconstruct mul- tiple gaze concurrences in several real world scenes and evaluated quantitatively against motion-captured ground truth.},
annote = {Environment Mapping
- Attention estimation
- Least squares
- Mean Shift

        

      },
author = {Park, Hyon Soo and Jain, Eakta and Sheikh, Yaser},
file = {:home/abetan16/Dropbox/MendeleyV3/Park, Jain, Sheikh - 2012 - 3d Social Saliency from Head-mounted Cameras.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {422--430},
title = {{3D Social Saliency from Head-mounted Cameras}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2012{\_}0212.pdf},
volume = {25},
year = {2012}
}
@inproceedings{Patterson2005,
abstract = {In this paper we present results related to achieving finegrained activity recognition for context-aware computing applications. We examine the advantages and challenges of reasoning with globally unique object instances detected by an RFID glove. We present a sequence of increasingly powerful probabilistic graphical models for activity recognition. We show the advantages of adding additional complexity and conclude with a model that can reason tractably about aggregated object instances and gracefully generalizes from object instances to their classes by using abstraction smoothing. We apply these models to data collected from a morning household routine.},
author = {Patterson, D.J. and Fox, D. and Kautz, H. and Philipose, M.},
booktitle = {Ninth IEEE International Symposium on Wearable Computers (ISWC'05)},
doi = {10.1109/ISWC.2005.22},
file = {:home/abetan16/Dropbox/MendeleyV3/Patterson et al. - 2005 - Fine-Grained Activity Recognition by Aggregating Abstract Object Usage.pdf:pdf},
isbn = {0-7695-2419-2},
pages = {44--51},
publisher = {IEEE},
title = {{Fine-Grained Activity Recognition by Aggregating Abstract Object Usage}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1550785},
year = {2005}
}
@article{Pavlovic2000,
author = {Pavlovic, V and Rehg, JM and MacCormick, J},
file = {:home/abetan16/Dropbox/MendeleyV3/Pavlovic, Rehg, MacCormick - 2001 - Learning switching linear models of human motion.pdf:pdf},
journal = {Nips},
title = {{Learning switching linear models of human motion}},
url = {http://scholar.google.com/scholar?q=intitle:Learning+Switching+Linear+Models+of+Human+Motion{\#}0},
year = {2001}
}
@article{Pearce1974,
author = {Pearce, C. E. M.},
doi = {10.1287/trsc.8.2.142},
issn = {0041-1655},
journal = {Transportation Science},
month = {may},
number = {2},
pages = {142--168},
title = {{Locating Concentric Ring Roads in a City}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/trsc.8.2.142},
volume = {8},
year = {1974}
}
@article{Pedregosa2011,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algo-rithms for medium-scale supervised and unsupervised problems. This package focuses on bring-ing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependen-cies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1201.0490},
author = {{Pedregosa FABIANPEDREGOSA}, Fabian and {Alexandre Gramfort}, Normalesuporg and Michel, Vincent and {Thirion BERTRANDTHIRION}, Bertrand and Grisel, Olivier and Blondel, Mathieu and {Prettenhofer PETERPRETTENHOFER}, Peter and Weiss, Ron and Dubourg, Vincent and {Vanderplas VANDERPLAS}, Jake and Passos, Alexandre and Cournapeau, David and Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Thirion, Bertrand and Prettenhofer, Peter and Vanderplas, Jake and Brucher, Matthieu and {Perrot an Edouard Duchesnay PEDREGOSA}, Matthieu and {Matthieu Brucher MATTHIEUBRUCHER}, Al and {Perrot MATTHIEUPERROT}, Matthieu and {Edouard Duchesnay EDOUARDDUCHESNAY}, Cea F},
eprint = {1201.0490},
journal = {Journal of Machine Learning Research},
keywords = {Python,model selection,supervised learning,unsupervised learning},
pages = {2825--2830},
title = {{Scikit-learn: Machine Learning in Python Ga{\"{e}}l Varoquaux}},
volume = {12},
year = {2011}
}
@article{Pentland2000,
abstract = {The research topic of looking at people, that is, giving machines$\backslash$nthe ability to detect, track, and identify people and more generally, to$\backslash$ninterpret human behavior, has become a central topic in machine vision$\backslash$nresearch. Initially thought to be the research problem that would be$\backslash$nhardest to solve, it has proven remarkably tractable and has even$\backslash$nspawned several thriving commercial enterprises. The principle driving$\backslash$napplication for this technology is {\&}ldquo;fourth generation{\&}rdquo;$\backslash$nembedded computing: {\&}ldquo;smart{\&}rdquo; environments and portable or$\backslash$nwearable devices. The key technical goals are to determine the$\backslash$ncomputer's context with respect to nearby humans (e.g., who, what, when,$\backslash$nwhere, and why) so that the computer can act or respond appropriately$\backslash$nwithout detailed instructions. The paper examines the mathematical tools$\backslash$nthat have proven successful, provides a taxonomy of the problem domain,$\backslash$nand then examines the state of the art. Four areas receive particular$\backslash$nattention: person identification, surveillance/monitoring, 3D methods,$\backslash$nand smart rooms/perceptual user interfaces. Finally, the paper discusses$\backslash$nsome of the research challenges and opportunities},
author = {Pentland, Alex},
doi = {10.1109/34.824823},
file = {:home/abetan16/Dropbox/Articles/Mendeley/Pentland - 2000 - Looking at People Sensing for Ubiquitous and Wearable Computing.pdf:pdf},
isbn = {0162-8828 VO - 22},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Appearance-based vision,Face recognition,Gesture recognition,Looking at people,Ubiquitious,Visual interlace,Wearable computing},
number = {1},
pages = {107--110},
title = {{Looking at people: Sensing for ubiquitous and wearable computing}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=824823},
volume = {22},
year = {2000}
}
@article{Philipose2004,
abstract = {A key aspect of pervasive computing is using computers and sensor networks to effectively and unobtrusively infer users' behavior in their environment. This includes inferring which activity users are performing, how they're performing it, and its current stage. Recognizing and recording activities of daily living is a significant problem in elder care. A new paradigm for ADL inferencing leverages radio-frequency-identification technology, data mining, and a probabilistic inference engine to recognize ADLs, based on the objects people use. We propose an approach that addresses these challenges and shows promise in automating some types of ADL monitoring. Our key observation is that the sequence of objects a person uses while performing an ADL robustly characterizes both the ADL's identity and the quality of its execution. So, we have developed Proactive Activity Toolkit (PROACT).},
author = {Philipose, Matthai and Fishkin, Kenneth P. and Perkowitz, Mike and Patterson, Donald J. and Fox, Dieter and Kautz, Henry and H{\"{a}}hnel, Dirk},
doi = {10.1109/MPRV.2004.7},
file = {:home/abetan16/Dropbox/MendeleyV3/Philipose et al. - 2004 - Inferring Activities from Interactions with Objects.pdf:pdf},
isbn = {1536-1268},
issn = {15361268},
journal = {IEEE Pervasive Computing},
number = {4},
pages = {50--57},
title = {{Inferring activities from interactions with objects}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1369161},
volume = {3},
year = {2004}
}
@inproceedings{Pirsiavash2012,
abstract = {We present a novel dataset and novel algorithms for the problem of detecting activities of daily living (ADL) in firstperson camera views. We have collected a dataset of 1 million frames of dozens of people performing unscripted, everyday activities. The dataset is annotated with activities, object tracks, hand positions, and interaction events. ADLs differ from typical actions in that they can involve long-scale temporal structure (making tea can take a few minutes) and complex object interactions (a fridge looks different when its door is open). We develop novel representations including (1) temporal pyramids, which generalize the well-known spatial pyramid to approximate temporal correspondence when scoring a model and (2) composite object models that exploit the fact that objects look different when being interacted with. We perform an extensive empirical evaluation and demonstrate that our novel representations produce a two-fold improvement over traditional approaches. Our analysis suggests that real-world ADL recognition is “all about the objects,” and in particular, “all about the objects being interacted with.”},
author = {Pirsiavash, H. and Ramanan, D.},
booktitle = {2012 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2012.6248010},
file = {:home/abetan16/Dropbox/MendeleyV3/Pirsiavash, Ramanan - 2012 - Detecting Activities of Daily Living in First-Person Camera Views.pdf:pdf},
isbn = {978-1-4673-1228-8},
issn = {1063-6919},
keywords = {Biomedical monitoring,Cameras,Detectors,Face,Hidden Markov models,Taxonomy,Visualization,activities of daily living detection,approximate temporal correspondence,cameras,complex object interactions,composite object models,extensive empirical evaluation,first-person camera views,gesture recognition,image motion analysis,long-scale temporal structure,real-world ADL recognition,spatial pyramid,temporal pyramids},
month = {jun},
pages = {2847--2854},
publisher = {IEEE},
title = {{Detecting activities of daily living in first-person camera views}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6248010},
year = {2012}
}
@inproceedings{Poleg2014a,
abstract = {The proliferation of surveillance cameras has created new privacy concerns as people are captured daily without explicit consent, and the video is kept in databases for a very long time. With the increas- ing popularity of wearable cameras like Google Glass the problem is set to increase substantially. An important computer vision task is to enable a person (“subject”) to query the video database (“observer”) whether he/she has been captured on the video. Following a positive answer, the subject may request a copy of the video, or ask to be “forgotten” by erasing this video from the database. Two properties such queries should have are: (i) The query should not reveal more information about the subject, further breaching his privacy. (ii) The query should certify that the subject is indeed the captured person before sending him the video or erasing it. This paper presents a possible solution when the subject has a head mounted camera, e.g. Google Glass. We propose to create a unique signature, based on pattern of head motion, that could identify that the subject is indeed the person seen in a video. Unlike traditional biometric methods (face, gait recognition etc.), the proposed signature is temporally volatile, and can identify the subject only at a particular time. It is of no use for any other place or time.},
address = {Singapore},
author = {Poleg, Y and Arora, C and Peleg, S},
booktitle = {Asian Conference on Computer Vision},
file = {:home/abetan16/Dropbox/MendeleyV3/Poleg, Arora, Peleg - 2014 - Head Motion Signatures from Egocentric Videos.pdf:pdf},
number = {1},
pages = {1--15},
publisher = {Springer},
title = {{Head Motion Signatures from Egocentric Videos}},
volume = {Nov},
year = {2014}
}
@inproceedings{Poleg2014,
abstract = {The use of wearable cameras makes it possible to record life logging egocentric videos. Browsing such long unstruc- tured videos is time consuming and tedious. Segmenta- tion into meaningful chapters is an important first step to- wards adding structure to egocentric videos, enabling ef- ficient browsing, indexing and summarization of the long videos. Two sources of information for video segmentation are (i) the motion of the camera wearer, and (ii) the objects and activities recorded in the video. In this paper we ad- dress the motion cues for video segmentation. Motion based segmentation is especially difficult in ego- centric videos when the camera is constantly moving due to natural head movement of the wearer. We propose a robust temporal segmentation of egocentric videos into a hierar- chy of motion classes using a newCumulative Displacement Curves. Unlike instantaneous motion vectors, segmentation using integrated motion vectors performs well even in dy- namic and crowded scenes. No assumptions are made on the underlying scene structure and the method works in in- door as well as outdoor situations. We demonstrate the ef- fectiveness of our approach using publicly available videos as well as choreographed videos. We also suggest an ap- proach to detect the fixation of wearer's gaze in the walking portion of the egocentric videos. 1.},
author = {Poleg, Yair and Arora, Chetan and Peleg, Shmuel},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.325},
file = {:home/abetan16/Dropbox/MendeleyV3/Poleg, Arora, Peleg - 2014 - Temporal Segmentation of Egocentric Videos.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
month = {jun},
pages = {2537--2544},
publisher = {Ieee},
title = {{Temporal Segmentation of Egocentric Videos Yair Poleg}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909721},
year = {2014}
}
@article{Pollick2001,
abstract = {We examined the visual perception of affect from point-light displays of arm movements. Two actors were instructed to perform drinking and knocking movements with ten different affects while the three-dimensional positions of their arms were recorded. Point-light animations of these natural movements and phase-scrambled, upside-down versions of the same knocking movements were shown to participants who were asked to categorize the affect of the display. In both cases the resulting confusion matrices were analyzed using multidimensional scaling. For the natural movements the resulting two-dimensional psychological space was similar to a circumplex with the first dimension appearing as activation and the second dimension as pleasantness. For the scrambled displays the first dimension was similar in structure to that obtained for the natural movements but the second dimension was not. With both natural and scrambled movements Dimension 1 of the psychological space was highly correlated to the kinematics of the movement. These results suggest that the corresponding activation of perceived affect is a formless cue that relates directly to the movement kinematics while the pleasantness of the movement appears to be carried in the phase relations between the different limb segments. Copyright ?? 2001 Elsevier Science B.V.},
author = {Pollick, Frank E. and Paterson, Helena M. and Bruderlin, Armin and Sanford, Anthony J.},
doi = {10.1016/S0010-0277(01)00147-0},
file = {:home/abetan16/Dropbox/MendeleyV3/Pollick et al. - 2001 - Perceiving Affect from Arm Movement.pdf:pdf},
isbn = {0010-0277 (Print)},
issn = {00100277},
journal = {Cognition},
keywords = {Affect,Biological motion,Categorization,Kinematics,Multidimensional scaling},
month = {dec},
number = {2},
pages = {51--61},
pmid = {11716834},
title = {{Perceiving affect from arm movement}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/11716834},
volume = {82},
year = {2001}
}
@article{Popoola2012,
abstract = {Modeling human behaviors and activity patterns for recognition or$\backslash$ndetection of special event has attracted significant research interest$\backslash$nin recent years. Diverse methods that are abound for building intelligent$\backslash$nvision systems aimed at scene understanding and making correct semantic$\backslash$ninference from the observed dynamics of moving targets. Most applications$\backslash$nare in surveillance, video content retrieval, and human-computer$\backslash$ninterfaces. This paper presents not only an update extending previous$\backslash$nrelated surveys, but also a focus on contextual abnormal human behavior$\backslash$ndetection especially in video surveillance applications. The main$\backslash$npurpose of this survey is to extensively identify existing methods$\backslash$nand characterize the literature in a manner that brings key challenges$\backslash$nto attention.},
author = {Popoola, O P and Wang, Kejun},
doi = {10.1109/TSMCC.2011.2178594},
file = {:home/abetan16/Dropbox/MendeleyV3//Popoola, Wang - 2012 - Video-Based Abnormal Human Behavior Recognition A Review.pdf:pdf},
isbn = {1094-6977},
issn = {1094-6977},
journal = {Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on},
keywords = {Behavioral science;Feature extraction;Hidden Marko},
month = {nov},
number = {6},
pages = {865--878},
title = {{Video-Based Abnormal Human Behavior Recognition: A Review}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6129539},
volume = {42},
year = {2012}
}
@article{Rajamani2009,
abstract = {Designing a state estimator for a linear state-space model requires knowledge of the characteristics of the disturbances entering the states and the measurements. In [Odelson, B. J., Rajamani, M. R., {\&} Rawlings, J. B. (2006). A new autocovariance least squares method for estimating noise covariances. Automatica, 42(2), 303-308], the correlations between the innovations data were used to form a least-squares problem to determine the covariances for the disturbances. In this paper we present new and simpler necessary and sufficient conditions for the uniqueness of the covariance estimates. We also formulate the optimal weighting to be used in the least-squares objective in the covariance estimation problem to ensure minimum variance in the estimates. A modification to the above technique is then presented to estimate the number of independent stochastic disturbances affecting the states. This minimum number of disturbances is usually unknown and must be determined from data. A semidefinite optimization problem is solved to estimate the number of independent disturbances entering the system and their covariances. ?? 2008 Elsevier Ltd. All rights reserved.},
author = {Rajamani, Murali R. and Rawlings, James B.},
doi = {10.1016/j.automatica.2008.05.032},
file = {:home/abetan16/Dropbox/MendeleyV3/Rajamani, Rawlings - 2009 - Estimation of the Disturbance Structure from Data Using Semidefinite Programming and Optimal Weighting.pdf:pdf},
isbn = {0005-1098},
issn = {00051098},
journal = {Automatica},
keywords = {Covariance estimation,Minimum independent disturbances,Minimum variance estimation,Semidefinite programming,State estimation},
number = {1},
pages = {142--148},
title = {{Estimation of the disturbance structure from data using semidefinite programming and optimal weighting}},
url = {http://www.sciencedirect.com/science/article/pii/S000510980800366X},
volume = {45},
year = {2009}
}
@article{Ramirez-amaro,
author = {Ramirez-Amaro, K and Beetz, Michael and Cheng, Gordon},
file = {:home/abetan16/Dropbox/MendeleyV3/Ramirez-Amaro, Beetz, Cheng - Unknown - Understanding Human Activities from Observation via Semantic Reasoning for Humanoid Robots.pdf:pdf},
journal = {Ics.Ei.Tum.De},
pages = {2--3},
title = {{Understanding Human Activities from Observation via Semantic Reasoning for Humanoid Robots}},
url = {http://web.ics.ei.tum.de/{~}karinne/Pdfs/14{\_}iros{\_}workshop-ramirezK.pdf}
}
@article{Ramirez-Amaro2013,
abstract = {— In this paper, we present a two-stage framework that deal with the problem of automatically extract human activities from videos. First, for action recognition we employ an unsupervised state-of-the-art learning algorithm based on Independent Subspace Analysis (ISA). This learning algorithm extracts spatio-temporal features directly from video data and it is computationally more efficient and robust than other unsupervised methods. Nevertheless, when applying this one-stage state-of-the-art action recognition technique on the ob-servations of human everyday activities, it can only reach an accuracy rate of approximately 25{\%}. Hence, we propose to enhance this process with a second stage, which define a new method to automatically generate semantic rules that can reason about human activities. The obtained semantic rules enhance the human activity recognition by reducing the com-plexity of the perception system and they allow the possibility of domain change, which can great improve the synthesis of robot behaviors. The proposed method was evaluated under two complex and challenging scenarios: making a pancake and making a sandwich. The difficulty of these scenarios is that they contain finer and more complex activities than the well known data sets (Hollywood2, KTH, etc). The results show benefits of two stages method, the accuracy of action recognition was significantly improved compared to a single-stage method (above 87{\%} compared to human expert). This indicates the improvement of the framework using the reasoning engine for the automatic extraction of human activities from observations, thus, providing a rich mechanism for transferring a wide range of human skills to humanoid robots.},
author = {Ramirez-amaro, Karinne and Kim, Eun-sol and Kim, Jiseob and Zhang, Byoung-tak and Beetz, Michael and Cheng, Gordon},
doi = {10.1109/HUMANOIDS.2013.7030014},
file = {:home/abetan16/Dropbox/MendeleyV3/Ramirez-amaro et al. - 2013 - Enhancing Human Action Recognition through Spatio-temporal Feature Learning and Semantic Rules.pdf:pdf},
isbn = {9781479926183},
journal = {IEEE-RAS International Conference on Humanoid Robots},
month = {oct},
pages = {456--461},
publisher = {Ieee},
title = {{Enhancing Human Action Recognition through Spatio-temporal Feature Learning and Semantic Rules}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7030014},
year = {2013}
}
@article{Rautaray2012,
abstract = {As computers become more pervasive in society, facilitating natural human–computer interaction (HCI) will have a positive impact on their use. Hence, there has been growing interest in the development of new approaches and technologies for bridging the human–computer barrier. The ultimate aim is to bring HCI to a regime where interactions with computers will be as natural as an interaction between humans, and to this end, incorporating gestures in HCI is an important research area. Gestures have long been considered as an interaction technique that can potentially deliver more natural, creative and intuitive methods for communicating with our computers. This paper provides an analysis of comparative surveys done in this area. The use of hand gestures as a natural interface serves as a motivating force for research in gesture taxonomies, its representations and recognition techniques, software platforms and frameworks which is discussed briefly in this paper. It focuses on the three main phases of hand gesture recognition i.e. detection, tracking and recognition. Different application which employs hand gestures for efficient interaction has been discussed under core and advanced application domains. This paper also provides an analysis of existing literature related to gesture recognition systems for human computer interaction by categorizing it under different key parameters. It further discusses the advances that are needed to further improvise the present hand gesture recognition systems for future perspective that can be widely used for efficient human computer interaction. The main goal of this survey is to provide researchers in the field of gesture based HCI with a summary of progress achieved to date and to help identify areas where further research is needed.},
author = {Rautaray, Siddharth S. and Agrawal, Anupam},
doi = {10.1007/s10462-012-9356-9},
file = {:home/abetan16/Dropbox/MendeleyV3/Rautaray, Agrawal - 2012 - Vision based hand gesture recognition for human computer interaction a survey.pdf:pdf},
isbn = {02692821 (ISSN)},
issn = {02692821},
journal = {Artificial Intelligence Review},
keywords = {Gesture recognition,Hand,Human computer interaction,Natural interfaces,Recognition,Representations},
number = {1},
pages = {1--54},
title = {{Vision based hand gesture recognition for human computer interaction: a survey}},
volume = {43},
year = {2012}
}
@inproceedings{Rav-Acha2006,
abstract = {The power of video over still images is the ability to represent dynamic activities. But video browsing and retrieval are inconvenient due to inherent spatio-temporal redundancies, where some time intervals may have no activity, or have activities that occur in a small image region. Video synopsis aims to provide a compact video representation, while preserving the essential activities of the original video. We present dynamic video synopsis, where most of the activity in the video is condensed by simultaneously showing several actions, even when they originally occurred at different times. For example, we can create a "stroboscopic movie", where multiple dynamic instances of a moving object are played simultaneously. This is an extension of the still stroboscopic picture. Previous approaches for video abstraction addressed mostly the temporal redundancy by selecting representative key-frames or time intervals. In dynamic video synopsis the activity is shifted into a significantly shorter period, in which the activity is much denser. Video examples can be found online in http://www.vision.huji.ac.il/synopsis},
author = {Rav-Acha, A. and Pritch, Y. and Peleg, S.},
booktitle = {2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 1 (CVPR'06)},
doi = {10.1109/CVPR.2006.179},
file = {:home/abetan16/Dropbox/MendeleyV3/Rav-Acha, Pritch, Peleg - 2006 - Making a Long Video Short Dynamic Video Synopsis.pdf:pdf},
isbn = {0-7695-2597-0},
pages = {435--441},
publisher = {Ieee},
title = {{Making a Long Video Short: Dynamic Video Synopsis}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1640790},
volume = {1},
year = {2006}
}
@inproceedings{Rehg1994,
abstract = {Computer sensing of hand and limb motion is an important problem for applications in HumanComputer Interaction (HCI), virtual reality, and athletic performance measurement. Commercially available sensors are invasive, and require the user to wear gloves or targets. We have developed a noninvasive vision-based hand tracking system, called DigitEyes. Employing a kinematic hand model, the DigitEyes system has demonstrated tracking performance at speeds of up to 10 Hz, using line and point features extracted from gray scale images of unadorned, unmarked hands. We describe an application of our sensor to a 3D mouse user-interface problem. 1 Introduction A "human sensor" capable of tracking a person's spatial motion using techniques from Computer Vision would be a powerful tool for human-computer interfaces. Such a sensor could be located in the user's environment (rather than on their person) and could operate under natural conditions of lighting and dress, providing a degree of convenien...},
author = {Rehg, James M},
booktitle = {Workshop on Motion of Non-Rigid and Articulated Bodies},
file = {:home/abetan16/Dropbox/MendeleyV3/Rehg - 1994 - DigitEyes Vision-Based Hand Tracking for Human-Computer Interaction.pdf:pdf},
number = {November},
pages = {16--22},
publisher = {IEEE Comput. Soc},
title = {{DigitEyes : Vision-Based Hand Tracking for Human-Computer Interaction}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=346260},
year = {1994}
}
@techreport{Ren2011,
abstract = {We introduce a parallel implementation of the Simple Linear Iterative Clustering (SLIC) superpixel segmentation. Our implementation uses GPU and the NVIDIA CUDA frame- work. Using a single graphic card, our implementation achieves speedups of 10x∼20x from the sequential implementation. This allow us to use the superpixel segmentation method in real-time performance. Our implementation is compatible with the standard sequential implementation. Finally, the software is now online and is open source.},
address = {Oxford, UK},
author = {Ren, Carl Yuheng and Reid, Ian},
booktitle = {University of Oxford, Department of Engineering Science},
file = {:home/abetan16/Dropbox/MendeleyV3/Ren, Reid - 2011 - gSLIC A Real-time Implementation of SLIC Superpixel Segmentation.pdf:pdf},
institution = {University of Oxford, Department of Engineering Science},
pages = {1--6},
title = {{gSLIC: a real-time implementation of SLIC superpixel segmentation}},
url = {http://mfile.narotama.ac.id/files/Umum/JURNAL OXFORD/gSLIC- a real-time implementation of SLIC superpixel segmentation.pdf},
year = {2011}
}
@inproceedings{Ren2010,
abstract = {Identifying handled objects, i.e. objects being manipulated by a user, is essential for recognizing the person's activities. An egocentric camera as worn on the body enjoys many advantages such as having a natural first-person view and not needing to instrument the environment. It is also a challenging setting, where background clutter is known to be a major source of problems and is difficult to handle with the camera constantly and arbitrarily moving. In this work we develop a bottom-up motion-based approach to robustly segment out foreground objects in egocentric video and show that it greatly improves object recognition accuracy. Our key insight is that egocentric video of object manipulation is a special domain and many domain-specific cues can readily help. We compute dense optical flow and fit it into multiple affine layers. We then use a max-margin classifier to combine motion with empirical knowledge of object location and background movement as well as temporal cues of support region and color appearance. We evaluate our segmentation algorithm on the large Intel Egocentric Object Recognition dataset with 42 objects and 100K frames. We show that, when combined with temporal integration, figure-ground segmentation improves the accuracy of a SIFT-based recognition system from 33{\%} to 60{\%}, and that of a latent-HOG system from 64{\%} to 86{\%}.},
author = {Ren, Xiaofeng and Gu, Chunhui},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2010.5540074},
file = {:home/abetan16/Dropbox/MendeleyV3/Ren, Gu - 2010 - Figure-ground Segmentation Improves Handled Object Recognition in Egocentric Video.pdf:pdf},
isbn = {9781424469840},
issn = {10636919},
month = {jun},
number = {1},
pages = {3137--3144},
publisher = {IEEE},
title = {{Figure-ground segmentation improves handled object recognition in egocentric video}},
year = {2010}
}
@inproceedings{Philipose2009,
abstract = {Recognizing objects being manipulated in hands can provide essential information about a person's activities and have far-reaching impacts on the application of vision in everyday life. The egocentric viewpoint from a wearable camera has unique advantages in recognizing handled objects, such as having a close view and seeing objects in their natural positions. We collect a comprehensive dataset and analyze the feasibilities and challenges of the egocentric recognition of handled objects. We use a lapel-worn camera and record uncompressed video streams as human subjects manipulate objects in daily activities. We use 42 day-to-day objects that vary in size, shape, color and textureness. 10 video sequences are shot for each object under different illuminations and backgrounds. We use this dataset and a SIFT-based recognition system to analyze and quantitatively characterize the main challenges in egocentric object recognition, such as motion blur and hand occlusion, along with its unique constraints, such as hand color, location prior and temporal consistency. SIFT-based recognition has an average recognition rate of 12{\%}, and reaches 20{\%} through enforcing temporal consistency. We use simulations to estimate the upper bound for SIFT-based recognition at 64{\%}, the loss of accuracy due to background clutter at 20{\%}, and that of hand occlusion at 13{\%}. Our quantitative evaluations show that the egocentric recognition of handled objects is a challenging but feasible problem with many unique characteristics and many opportunities for future research.},
address = {Miami, FL},
author = {Ren, Xiaofeng and Philipose, Matthai and Xiaofengren},
booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2009},
doi = {10.1109/CVPR.2009.5204360},
file = {:home/abetan16/Dropbox/MendeleyV3/Philipose - 2009 - Egocentric Recognition of Handled Objects Benchmark and Analysis.pdf:pdf},
isbn = {9781424439911},
issn = {1063-6919},
month = {jun},
pages = {49--56},
publisher = {IEEE},
title = {{Egocentric recognition of handled objects: Benchmark and analysis}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5204360},
year = {2009}
}
@article{Renteria2012,
abstract = {The longitudinal fissure separates the human brain into two hemispheres that remain connected through the corpus callosum. The left and the right halves of the brain resemble each other, and almost every structure present in one side has an equivalent structure in the other. Despite this exceptional correspondence, the two hemispheres also display important anatomical differences and there is marked lateralization of certain cognitive and motor functions such as language and handedness. However, the mechanisms that underlie the establishment of these hemispheric specializations, as well as their physiological and behavioral implications, remain largely unknown. Thanks to recent advances in neuroimaging, a series of studies documenting variation in symmetry and asymmetry as a function of age, gender, brain region, and pathological state, have been published in the past decade. Here, we review evidence of normal and atypical cerebral asymmetry, and the factors that influence it at the macrostructural level. Given the prominent role that cerebral asymmetry plays in the organization of the brain, and its possible implication in neurodevelopmental and psychiatric conditions, further research in this area is anticipated.},
author = {Renter{\'{i}}a, Miguel E.},
doi = {10.1017/thg.2012.13},
file = {:home/abetan16/Dropbox/MendeleyV3/Renter{\'{i}}a - 2012 - Cerebral Asymmetry A Quantitative, Multifactorial, and Plastic Brain Phenotype.pdf:pdf},
isbn = {1832427412000},
issn = {1832-4274},
journal = {Twin Research and Human Genetics},
keywords = {at the macrostructural level,brain asymmetry features,brain lateralization,can be investigated with,cerebral asymmetry,gyri or subcortical structures,lobes,magnetic resonance imaging,mri,shape and size of,such as differences in,sulci,the volume},
number = {03},
pages = {401--413},
pmid = {22856374},
title = {{Cerebral Asymmetry: A Quantitative, Multifactorial, and Plastic Brain Phenotype}},
volume = {15},
year = {2012}
}
@article{Riboni2011,
author = {Riboni, Daniele and Bettini, Claudio},
doi = {10.1007/s00779-010-0331-7},
file = {:home/abetan16/Dropbox/MendeleyV3/Riboni, Bettini - 2011 - COSAR hybrid reasoning for context-aware activity recognition.pdf:pdf},
isbn = {0077901003317},
issn = {1617-4909},
journal = {Personal and Ubiquitous Computing},
keywords = {a special interest is,activity,activity recognition {\'{a}} context,among many applications of,awareness {\'{a}},in the pervasive e-health,ontological reasoning,recognition,the circumstances},
number = {3},
pages = {271--289},
title = {{COSAR: hybrid reasoning for context-aware activity recognition}},
url = {http://link.springer.com/10.1007/s00779-010-0331-7},
volume = {15},
year = {2011}
}
@article{Rimrott2013,
abstract = {This paper presents a study in which we examined spelling mistakes made by 34 learners of German in an online CALL exercise. We analyzed a total of 374 spell- ing errors that occurred in 341 words and subsequently classified them along four dimensions: (a) competence versus performance, (b) linguistic subsystem, (c) language influence, and (d) target deviation. We also evaluated the perfor- mance of a generic spell checker, one that is not specifically designed for second language learners, to determine the kinds and frequencies of errors it can suc- cessfully correct. Results indicate that 80{\%} of the spelling errors in our study are systematic competence errors rather than accidental typographical mistakes. The study further reveals that MS Word 2003, the spell checker used in our study, fails to detect or provide a correction for 48{\%} of the spelling mistakes made by our language learners. Our study offers explanations for the spell checkerʼs failure to correct many of the misspellings and makes several computational and peda- gogical suggestions to overcome some of the shortcomings of a generic spell checker in the CALL classroom.},
author = {Rimrott, Anne and Heift, Trude},
file = {:home/abetan16/Dropbox/MendeleyV3/Rimrott, Heift - 2005 - Language learners and generic spell checkers in CALL.pdf:pdf},
issn = {0742-7778},
journal = {Calico Journal},
keywords = {error classification,german as a foreign,language,misspellings,spell checkers,spell checking,spelling errors},
number = {1},
pages = {17--48},
title = {{Language Learners and Generic Spell Checkers in CALL}},
url = {http://journals.sfu.ca/CALICO/index.php/calico/article/view/717},
volume = {23},
year = {2005}
}
@article{Ristic2013,
abstract = {Bernoulli filters are a class of exact Bayesian filters for non-linear/non-Gaussian recursive estimation of dynamic systems, recently emerged from the random set theoretical framework. The common feature of Bernoulli filters is that they are designed for stochastic dynamic systems which randomly switch on and off. The applications are primarily in target tracking, where the switching process models target appearance or disappearance from the surveillance volume. The concept, however, is applicable to a range of dynamic phenomena, such as epidemics, pollution, social trends, etc. Bernoulli filters in general have no analytic solution and are implemented as particle filters or Gaussian sum filters. This tutorial paper reviews the theory of Bernoulli filters as well as their implementation for different measurement models. The theory is backed up by applications in sensor networks, bearings-only tracking, passive radar/sonar surveillance, visual tracking, monitoring/prediction of an epidemic and tracking using natural language statements. More advanced topics of smoothing, multi-target detection/tracking, parameter estimation and sensor control are briefly reviewed with pointers for further reading.},
author = {Ristic, Branko and Vo, Ba Tuong and Vo, Ba Ngu and Farina, Alfonso},
doi = {10.1109/TSP.2013.2257765},
file = {:home/abetan16/Dropbox/MendeleyV3/Ristic et al. - 2013 - A Tutorial on Bernoulli Filters Theory, Implementation and Applications.pdf:pdf},
isbn = {1053-587X VO - 61},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
keywords = {Particle filters,Random sets,Sequential Bayesian estimation,Target tracking},
month = {jul},
number = {13},
pages = {3406--3430},
title = {{A tutorial on Bernoulli filters: Theory, implementation and applications}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6497685},
volume = {61},
year = {2013}
}
@article{NataliaDiazManuelPegalajarJohanLilius2014,
abstract = {Describing user activity plays an essential role in ambient intelligence. In this work, we review different methods for human activity recognition, classified as data-driven and knowledge-based techniques. We focus on context ontologies whose ultimate goal is the tracking of human behavior. After studying upper and domain ontologies, both useful for human activity representation and inference, we establish an evaluation criterion to assess the suitability of the different candidate ontologies for this purpose. As a result, any missing features, which are relevant for modeling daily human behaviors, are identified as future challenges.},
author = {Rodr{\'{i}}guez, Natalia D{\'{i}}az and Cu{\'{e}}llar, M. P. and Lilius, Johan and Calvo-Flores, Miguel Delgado},
doi = {10.1145/2523819},
file = {:home/abetan16/Dropbox/MendeleyV3/Rodr{\'{i}}guez et al. - 2014 - A survey on ontologies for human behavior recognition.pdf:pdf},
isbn = {0360-0300},
issn = {03600300},
journal = {ACM Computing Surveys},
number = {4},
pages = {1--33},
title = {{A survey on ontologies for human behavior recognition}},
url = {http://dl.acm.org/citation.cfm?doid=2597757.2523819},
volume = {46},
year = {2014}
}
@inproceedings{Rogez2014a,
abstract = {We tackle the problem of estimating the 3D pose of an individual's upper limbs (arms+hands) from a chest mounted depth-camera. Importantly, we consider pose estimation during everyday interactions with objects. Past work shows that strong pose+viewpoint priors and depth-based features are crucial for robust performance. In egocentric views, hands and arms are observable within a well defined volume in front of the camera. We call this volume an egocentric workspace. A notable property is that hand appearance correlates with workspace location. To exploit this correlation, we classify arm+hand configurations in a global egocentric coordinate frame, rather than a local scanning window. This greatly simplify the architecture and improves performance. We propose an efficient pipeline which 1) generates synthetic workspace exemplars for training using a virtual chest-mounted camera whose intrinsic parameters match our physical camera, 2) computes perspective-aware depth features on this entire volume and 3) recognizes discrete arm+hand pose classes through a sparse multi-class SVM. Our method provides state-of-the-art hand pose recognition performance from egocentric RGB-D images in real-time.},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.0060v1},
author = {Rogez, Gregory and Iii, James S Supancic and Ramanan, Deva},
booktitle = {ArXiv e-prints},
eprint = {arXiv:1412.0060v1},
file = {:home/abetan16/Dropbox/MendeleyV3/Rogez, Iii, Ramanan - 2014 - Egocentric Pose Recognition in Four Lines of Code.pdf:pdf},
month = {nov},
number = {1},
pages = {1--9},
title = {{Egocentric Pose Recognition in Four Lines of Code}},
volume = {Jun},
year = {2014}
}
@inproceedings{Rosales-Perez2013,
abstract = {Support vector machines (SVMs) are among the most used methods for pattern recognition. Acceptable results have been obtained with such methods in many domains and applications. However, as most learning algorithms, SVMs have hyperparameters that influence the effectiveness of the generated model. Thus, choosing adequate values for such hyperparameters is critical in order to obtain satisfactory results for a given classification task, a problem known as model selection. This paper introduces a novel model selection approach for SVMs based on multi-objective optimization and on the bias and variance definition. We propose an evolutionary algorithm that aims to select the configuration of hyperparameters that optimizes a trade-off between estimates of bias and variance; two factors that are closely related to the model accuracy and complexity. The proposed technique is evaluated using a suite of benchmark data sets for classification. Experimental results show the validity of our approach. We found that the model selection criteria resulted very helpful for selecting highly effective classification models.},
author = {Rosales-P{\'{e}}rez, a and Escalante, Hj},
booktitle = {The Twenty-Sixth International FLAIRS Conference},
file = {:home/abetan16/Dropbox/MendeleyV3/Rosales-P{\'{e}}rez, Escalante - 2013 - Bias and Variance Optimization for SVMs Model Selection.pdf:pdf},
isbn = {9781577356059},
pages = {136--141},
publisher = {AAAI Publications},
title = {{Bias and Variance Optimization for SVMs Model Selection}},
url = {http://www.aaai.org/ocs/index.php/FLAIRS/FLAIRS13/paper/download/5890/6055},
year = {2013}
}
@article{Roweis2008,
abstract = {Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text.},
author = {Roweis, S T and Saul, L K},
doi = {10.1126/science.290.5500.2323},
file = {:home/abetan16/Dropbox/MendeleyV3/Roweis, Saul - 2000 - Nonlinear dimensionality reduction by locally linear embedding.pdf:pdf},
isbn = {00368075},
issn = {0036-8075},
journal = {Science},
number = {5500},
pages = {2323--2326},
pmid = {11125150},
title = {{Nonlinear dimensionality reduction by locally linear embedding.}},
volume = {290},
year = {2000}
}
@article{Russell2008,
abstract = {We seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web.},
author = {Russell, Bryan C. and Torralba, Antonio and Murphy, Kevin P. and Freeman, William T.},
doi = {10.1007/s11263-007-0090-8},
file = {:home/abetan16/Dropbox/Articles/Mendeley/Russell, Torralba - 2008 - LabelMe A Database and Web-Based Tool for Image Annotation.pdf:pdf},
isbn = {0920-5691},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {Annotation tool,Database,Object detection,Object recognition},
number = {1-3},
pages = {157--173},
title = {{LabelMe: A database and web-based tool for image annotation}},
url = {http://link.springer.com/article/10.1007/s11263-007-0090-8},
volume = {77},
year = {2008}
}
@inproceedings{Ryoo2007,
abstract = {The paper presents a system that recognizes humans interacting with objects. We delineate a new framework that integrates object recognition, motion estimation, and semantic-level recognition for the reliable recognition of hierarchical human-object interactions. The framework is designed to integrate recognition decisions made by each component, and to probabilistically compensate for the failure of the components with the use of the decisions made by the other components. As a result, human-object interactions in an airport-like environment, such as a person carrying a baggage, a person leaving his/her baggage, or a person snatching another's baggage, are recognized. The experimental results show that not only the performance of the final activity recognition is superior to that of previous approaches, but also the accuracy of the object recognition and the motion estimation increases using feedback from the semantic layer. Several real examples illustrate the superior performance in recognition and semantic description of occurring events.},
address = {Minneapolis, MN},
author = {Ryoo, M S and Aggarwal, J K},
booktitle = {Framework},
file = {:home/abetan16/Dropbox/MendeleyV3/Ryoo, Aggarwal - 2007 - Hierarchical Recognition of Human Activities Interacting with Objects.pdf:pdf},
isbn = {1424411807},
issn = {1063-6919},
number = {June},
pages = {1--8},
publisher = {IEEE},
title = {{Hierarchical Recognition of Human Activities Interacting with Objects The University of Texas at Austin O}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4270485},
volume = {2007},
year = {2007}
}
@inproceedings{Ryoo2013,
abstract = {This paper discusses the problem of recognizing interaction-level human activities from a first-person viewpoint. The goal is to enable an observer (e.g., a robot or a wearable camera) to understand 'what activity others are performing to it' from continuous video inputs. These include friendly interactions such as 'a person hugging the observer' as well as hostile interactions like 'punching the observer' or 'throwing objects to the observer', whose videos involve a large amount of camera ego-motion caused by physical interactions. The paper investigates multi-channel kernels to integrate global and local motion information, and presents a new activity learning/recognition methodology that explicitly considers temporal structures displayed in first-person activity videos. In our experiments, we not only show classification results with segmented videos, but also confirm that our new approach is able to detect activities from continuous videos reliably.},
address = {Portland, OR, US},
author = {Ryoo, M S and Matthies, L},
booktitle = {Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on},
doi = {10.1109/CVPR.2013.352},
file = {:home/abetan16/Dropbox/MendeleyV3/Ryoo, Matthies - 2013 - First-Person Activity Recognition What Are They Doing to Me.pdf:pdf},
isbn = {1063-6919 VO  -},
issn = {1063-6919},
keywords = {Cameras,Histograms,Kernel,Observers,Positron emission tomography,Robots,Visualization,activity learning,camera ego-motion,first person activity video,first-person computer vision,human activity recognition,image classification,image segmentation,interaction level human activity recognition,interactive video,learning (artificial intelligence),motion information integration,multichannel kernel,object recognition,observer,person activity recognition,physical interaction,temporal structure,video cameras,video classification,video segmentation},
pages = {2730--2737},
publisher = {IEEE Comput. Soc},
title = {{First-Person Activity Recognition: What Are They Doing to Me?}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6619196},
year = {2013}
}
@article{Ryoo2015,
abstract = {In this paper, we present a new feature representation for first-person videos. In first-person video understanding (e.g., activity recognition), it is very important to capture both entire scene dynamics (i.e., egomotion) and salient local motion observed in videos. We describe a representation framework based on time series pooling, which is designed to abstract short-term/long-term changes in feature descriptor elements. The idea is to keep track of how descriptor values are changing over time and summarize them to represent motion in the activity video. The framework is general, handling any types of per-frame feature descriptors including conventional motion descriptors like histogram of optical flows (HOF) as well as appearance descriptors from more recent convolutional neural networks (CNN). We experimentally confirm that our approach clearly outperforms previous feature representations including bag-of-visual-words and improved Fisher vector (IFV) when using identical underlying feature descriptors. We also confirm that our feature representation has superior performance to existing state-of-the-art features like local spatio-temporal features and Improved Trajectory Features (originally developed for 3rd-person videos) when handling first-person videos. Multiple first-person activity datasets were tested under various settings to confirm these findings.},
archivePrefix = {arXiv},
arxivId = {1412.6505},
author = {Ryoo, M. S. and Rothrock, Brandon and Matthies, Larry},
eprint = {1412.6505},
file = {:home/abetan16/Dropbox/MendeleyV3/Ryoo, Rothrock, Matthies - 2014 - Pooled Motion Features for First-Person Videos.pdf:pdf},
journal = {arXiv preprint},
month = {dec},
title = {{Pooled Motion Features for First-Person Videos}},
url = {http://arxiv.org/abs/1412.6505},
year = {2014}
}
@inproceedings{Sadeghi2011,
abstract = {In this paper we introduce visual phrases, complex visual composites like “a person riding a horse”. Visual phrases often display significantly reduced visual complexity compared to their component objects, because the appearance of those objects can change profoundly when they participate in relations. We introduce a dataset suitable for phrasal recognition that uses familiar PASCAL object categories, and demonstrate significant experimental gains resulting from exploiting visual phrases. We show that a visual phrase detector significantly outperforms a baseline which detects component objects and reasons about relations, even though visual phrase training sets tend to be smaller than those for objects. We argue that any multi-class detection system must decode detector outputs to produce final results; this is usually done with non-maximum suppression. We describe a novel decoding procedure that can account accurately for local context without solving difficult inference problems. We show this decoding procedure outperforms the state of the art. Finally, we show that decoding a combination of phrasal and object detectors produces real improvements in detector results.},
author = {Sadeghi, Mohammad Amin and Farhadi, Ali},
booktitle = {Cvpr 2011},
doi = {10.1109/CVPR.2011.5995711},
file = {:home/abetan16/Dropbox/MendeleyV3/Sadeghi, Farhadi - 2011 - Recognition Using Visual Phrases.pdf:pdf},
isbn = {978-1-4577-0394-2},
month = {jun},
pages = {1745--1752},
publisher = {IEEE},
title = {{Recognition using visual phrases}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5995711},
year = {2011}
}
@article{Guaus,
abstract = {Many musical interfaces have used the musical conductor metaphor, allowing users to control the expressive aspects of a performance by imitating the gestures of conductors. In most of them, the rules to control these expressive aspects are predefined and users have to adapt to them. Other works have studied conductors' gestures in relation to the performance of the orchestra. The goal of this study is to analyze, following the path initiated by this latter kind of works, how simple motion capture descriptors can explain the relationship between the loudness of a given performance and the way in which different subjects move when asked to impersonate the conductor of that performance. Twenty-five subjects were asked to impersonate the conductor of three classical music fragments while listening to them. The results of different linear regression models with motion capture descriptors as explanatory variables show that, by studying how descriptors correlate to loudness differently among subjects, different tendencies can be found and exploited to design models that better adjust to their expectations.},
author = {Saras{\'{u}}a, Alvaro and Guaus, Enric},
file = {:home/abetan16/Dropbox/MendeleyV3/Saras{\'{u}}a, Guaus - 2014 - Dynamics in Music Conducting A Computational Comparative Study Among Subjects.pdf:pdf},
journal = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {classical music,conducting,expressive performance,motion},
pages = {195--200},
title = {{Dynamics in Music Conducting: A Computational Comparative Study Among Subjects}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}464.pdf},
year = {2014}
}
@incollection{Savva2012,
abstract = {This study aims at recognizing the affective states of players from non-acted, non-repeated body movements in the context of a video game scenario. A motion capture system was used to collect the movements of the participants while playing a Nintendo Wii tennis game. Then, a combination of body movement features along with a machine learning technique was used in order to automatically recognize emotional states from body movements. Our system was then tested for its ability to generalize to new participants and to new body motion data using a sub-sampling validation technique. To train and evaluate our system, online evaluation surveys were created using the body movements collected from the motion capture system and human observers were recruited to classify them into affective categories. The results showed that observer agreement levels are above chance level and the automatic recognition system achieved recognition rates comparable to the observers' benchmark.},
author = {Savva, Nikolaos and Bianchi-Berthouze, Nadia},
booktitle = {Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering},
doi = {10.1007/978-3-642-30214-5_17},
file = {:home/abetan16/Dropbox/MendeleyV3/Savva, Bianchi-Berthouze - 2012 - Automatic Recognition of Affective Body Movement in a Video Game Scenario.pdf:pdf},
isbn = {9783642302138},
issn = {18678211},
keywords = {Body movement,automatic emotion recognition,exertion game},
pages = {149--159},
publisher = {Springer Berlin Heidelberg},
title = {{Automatic recognition of affective body movement in a video game scenario}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-30214-5{\_}17},
volume = {78 LNICST},
year = {2012}
}
@article{Sawahata2003,
abstract = {Digitization of lengthy personal experiences would be made possible by constant recording using wearable video cameras. It is conceivable that the resulting amount of video content would be extraordinarily large. In order to retrieve and browse the desired scenes, a vast amount of video would need to be organized with structural information. In this paper, we attempt to develop a "wearable imaging system" that is capable of constantly capturing data, not only from a wearable video camera, but also from various kinds of sensors, such as a GPS, an accelerometer and a gyro sensor. The data from these sensors are appropriately extracted and processed by hidden Markov model (HMM) to achieve efficient video retrieval and browsing.},
author = {Sawahata, Y and Aizawa, K},
doi = {10.1109/ICME.2003.1220850},
file = {:home/abetan16/Dropbox/MendeleyV3/Sawahata, Aizawa - 2003 - Wearable Imaging System for Summarizing Personal Experiences.pdf:pdf},
isbn = {0-7803-7965-9},
issn = {1945788X},
journal = {Multimedia and Expo},
number = {1},
pages = {1--45},
publisher = {Ieee},
title = {{Wearable Imaging System for Summarizing Personal Experiences}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1220850},
volume = {Jul},
year = {2003}
}
@article{Saxena2004,
author = {Saxena, Ashutosh and Gupta, Abhinav},
doi = {10.1007/978-3-540-30499-9_161},
file = {:home/abetan16/Dropbox/MendeleyV3/Saxena, Gupta - 2004 - Non-linear dimensionality reduction by locally linear isomaps.pdf:pdf},
issn = {03029743},
journal = {Neural Information Processing},
pages = {1038--1043},
title = {{Non-linear dimensionality reduction by locally linear isomaps}},
url = {http://www.springerlink.com/index/14754g3k7gp4lt2y.pdf},
year = {2004}
}
@article{Scheck2014,
author = {Scheck, Anne},
file = {:home/abetan16/Dropbox/MendeleyV3/Scheck - 2014 - Seeing the (Google) Glass as Half Full.pdf:pdf},
journal = {Emergency Mediine News},
number = {2},
pages = {20--21},
title = {{Seeing the (Google) glass as half full}},
volume = {36},
year = {2014}
}
@inproceedings{Schiele1999a,
abstract = {The importance of context in communication and interface can not be overstated. Physical environment, time of day, mental state, and the model each conversant has of the other participants can be critical in conveying necessary information and mood. An anecdote from Nicholas Negroponte's book "Being Digital" Negroponte, 1995 illustrates this point: Before dinner, we walked around Mr. Shikanai's famous outdoor art collection, which during the daytime doubles as the Hakone Open Air Museum. At dinner with Mr. and Mrs. Shikanai, we were joined by Mr. Shikanai's private male secretary who, quite significantly, spoke perfect English, as the Shikanais spoke none at all. The conversation was started by Wiesner, who expressed great interest in the work by Alexander Calder and told about both MIT's and his own personal experience with that great artist. The secretary listened to the story and then translated it from beginning to end, with Mr. Shikanai listening attentively.},
author = {Schiele, Bernt and Starner, Thad and Rhodes, Bradley and Clarkson, Brian and Pentland, Alex},
booktitle = {Augmented reality and wearable computers. Mahwah, NJ: Erlbaum},
file = {:home/abetan16/Dropbox/MendeleyV3/Schiele, Starner, Rhodes - 1999 - Situation Aware Computing with Wearable Computers.pdf:pdf},
isbn = {9780805829020},
pages = {1--20},
title = {{Situation aware computing with wearable computers}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.111.4887{\&}rep=rep1{\&}type=pdf},
year = {2000}
}
@inproceedings{Schlattman2007,
abstract = {In this paper we present a novel computer vision based handtracking method, which is capable of simultaneously tracking 6+4 degrees of freedom (DOFs) of each human hand in real-time (25 frames per second) with the help of 3 (or more) off-the-shelf consumer cameras. '6+4 DOF' means that the system can track the global pose (6 continuous parameters for translation and rotation) of 4 different gestures. Different studies discovered the need for two-handed interaction to enable an intuitive 3D Human-Computer-Interaction. Previously, using both hands as at least 6 DOF input devices involved the use of either datagloves or markers. Applying our two-hand-tracking we evaluated the use of both hands as input devices for two applications: fly-through exploration of a virtual world and a mesh editing application.},
address = {New York, NY, USA},
author = {Schlattman, Markus and Klein, Reinhard},
booktitle = {Proceedings of the 2007 ACM symposium on Virtual reality software and technology - VRST '07},
doi = {10.1145/1315184.1315188},
file = {:home/abetan16/Dropbox/Articles/Mendeley/Schlattman, Klein - 2007 - Simultaneous 4 Gestures 6 Dof Real-time Two-hand Tracking Without Any Markers.pdf:pdf},
isbn = {9781595938633},
keywords = {hand tracking,interaction techniques,virtual reality},
number = {212},
pages = {39},
publisher = {ACM Press},
title = {{Simultaneous 4 gestures 6 DOF real-time two-hand tracking without any markers}},
url = {http://portal.acm.org/citation.cfm?doid=1315184.1315188},
volume = {1},
year = {2007}
}
@article{Schlattmann2007,
abstract = {In this paper we present a novel computer vision based hand-tracking technique, which is capable of robustly tracking 6+4DOF of the human hand in real-time (at least 25 frames per second) with the help of 3 (or more) off-the-shelf consumer cameras. 6+4DOF means that the system can track the global pose (6 continuous parameters for translation and rotation) of 4 different gestures. A key feature of our system is its fully automatic real-time initialization procedure, which, along with a sound tracking-lost detector, makes the system fit for real-world applications. Because of this, our method acts as an enabling technology for uncumbersome hand-based 3D Human-Computer-Interaction (HCI). Previously, using the hand as an at least 6DOF input device involved the use of either datagloves or markers. Using our tracking we evaluated the use of the hand as an input device for two prevalent Virtual Reality applications: fly-through exploration of a virtual world and a simple digital assembly simulation.},
author = {Schlattmann, Markus and Kahlesz, Ferenc and Sarlette, Ralf and Klein, Reinhard},
doi = {10.1111/j.1467-8659.2007.01069.x},
file = {:home/abetan16/Dropbox/MendeleyV3/Schlattmann et al. - 2007 - Markerless 4 Gestures 6 Dof Real-time Visual Tracking of the Human Hand with Automatic Initialization.pdf:pdf},
issn = {01677055},
journal = {Computer Graphics Forum},
month = {sep},
number = {3},
pages = {467--476},
title = {{Markerless 4 gestures 6 DOF real-time visual tracking of the human hand with automatic initialization}},
url = {http://doi.wiley.com/10.1111/j.1467-8659.2007.01069.x},
volume = {26},
year = {2007}
}
@article{Schramm2015,
author = {Schramm, Rodrigo and Jung, Claudio Rosito and Miranda, Eduardo Reck},
doi = {10.1109/TMM.2014.2377553},
file = {:home/abetan16/Dropbox/MendeleyV3/Schramm, Jung, Miranda - 2015 - Dynamic Time Warping for Music Conducting Gestures Evaluation.pdf:pdf},
isbn = {1520-9210},
issn = {1520-9210},
journal = {IEEE Transactions on Multimedia},
month = {feb},
number = {2},
pages = {243--255},
title = {{Dynamic Time Warping for Music Conducting Gestures Evaluation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6975197},
volume = {17},
year = {2015}
}
@inproceedings{Pritch2007,
address = {Rio de Janeiro},
author = {Science, Computer},
booktitle = {Construction},
doi = {10.1109/ICCV.2007.4408934},
file = {:home/abetan16/Dropbox/MendeleyV3/Science - 2007 - Webcam Synopsis Peeking Around the World ∗.pdf:pdf},
isbn = {9781424416318},
pages = {1--8},
publisher = {Ieee},
title = {{Webcam Synopsis: Peeking Around the World ∗}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4408934},
year = {2007}
}
@inproceedings{Scollnik1996,
abstract = {This paper introduces the readers of the Proceed- ings to an important class of computer based simula- tion techniques known as Markov chain Monte Carlo (MCMC) methods. General properties characterizing these methods will be discussed, but the main empha- sis will be placed on one MCMC method known as the Gibbs sampler. The Gibbs sampler permits one to simu- late realizations from complicated stochastic models in high dimensions bymaking use of themodel's associated full conditional distributions, which will generally have a much simpler and more manageable form. In its most extreme version, the Gibbs sampler reduces the analy- sis of a complicated multivariate stochastic model to the consideration of that model's associated univariate full conditional distributions. In this paper, the Gibbs sampler will be illustrated with four examples. The first three of these examples serve as rather elementary yet instructive applications of the Gibbs sampler. The fourth example describes a reasonably sophisticated application of the Gibbs sam- pler in the important arena of credibility for classifica- tion ratemaking via hierarchical models, and involves the Bayesian prediction of frequency counts in workers compensation insurance.},
author = {Scollnik, Dpm},
booktitle = {Casualty Actuarial Society},
file = {:home/abetan16/Dropbox/MendeleyV3/Scollnik - 1996 - An Introduction to Markov Chain Monte Carlo Methods and Their Actuarial Applications.pdf:pdf},
pages = {114--165},
title = {{An introduction to Markov Chain Monte Carlo methods and their actuarial applications}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.111.4676{\&}rep=rep1{\&}type=pdf{\#}page=121},
year = {1996}
}
@inproceedings{Shahaf2010,
abstract = {The process of extracting useful knowledge from large datasets has become one of the most pressing problems in today's society. The problem spans entire sectors, from scientists to intelligence analysts and web users, all of whom are constantly struggling to keep up with the larger and larger amounts of content published every day. With this much data, it is often easy to miss the big picture. In this paper, we investigate methods for automatically connecting the dots -- providing a structured, easy way to navigate within a new topic and discover hidden connections. We focus on the news domain: given two news articles, our system automatically finds a coherent chain linking them together. For example, it can recover the chain of events starting with the decline of home prices (January 2007), and ending with the ongoing health-care debate. We formalize the characteristics of a good chain and provide an efficient algorithm (with theoretical guarantees) to connect two fixed endpoints. We incorporate user feedback into our framework, allowing the stories to be refined and personalized. Finally, we evaluate our algorithm over real news data. Our user studies demonstrate the algorithm's effectiveness in helping users understanding the news.},
address = {New York, New York, USA},
author = {Shahaf, Dafna and Guestrin, Carlos},
booktitle = {Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '10},
doi = {10.1145/1835804.1835884},
file = {:home/abetan16/Dropbox/MendeleyV3/Shahaf, Guestrin - 2010 - Connecting the Dots Between News Articles.pdf:pdf},
isbn = {9781450300551},
pages = {623},
publisher = {ACM Press},
title = {{Connecting the dots between news articles}},
url = {http://dl.acm.org/citation.cfm?doid=1835804.1835884},
year = {2010}
}
@inproceedings{CaifengShanYuchengWeiTieniuTan2004,
abstract = { Particle filter and mean shift are two successful approaches taken in the pursuit of robust tracking. Both of them have their respective strengths and weaknesses. In this paper, we proposed a new tracking algorithm, the mean shift embedded particle filter (MSEPF), to integrate advantages of the two methods. Compared with the conventional particle filter, the MSEPF leads to more efficient sampling by shifting samples to their neighboring modes, overcoming the degeneracy problem, and requires fewer particles to maintain multiple hypotheses, resulting in low computational cost. When applied to hand tracking, the MSEPF tracks hand in real time, saving much time for later gesture recognition, and it is robust to the hand's rapid movement and various kinds of distractors.},
author = {Shan, Caifeng Shan Caifeng and Wei, Yucheng Wei Yucheng and Tan, Tieniu Tan Tieniu and Ojardias, F.},
booktitle = {Sixth IEEE International Conference on Automatic Face and Gesture Recognition, 2004. Proceedings.},
doi = {10.1109/AFGR.2004.1301611},
file = {:home/abetan16/Dropbox/MendeleyV3/Shan et al. - 2004 - Real Time Hand Tracking by Combining Particle Filtering and Mean Shift.pdf:pdf},
isbn = {0-7695-2122-3},
pages = {1--6},
publisher = {Ieee},
title = {{Real time hand tracking by combining particle filtering and mean shift}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1301611},
year = {2004}
}
@article{Shan2007,
abstract = {Particle ﬁltering and mean shift (MS) are two successful approaches to visual tracking. Both have their respective strengths and weaknesses. In this paper, we propose to integrate advantages of the two approaches for improved tracking. By incorporating the MS optimization into particle ﬁltering to move particles to local peaks in the likelihood, the proposed mean shift embedded particle ﬁlter (MSEPF) improves the sampling efﬁciency considerably. Our work is conducted in the context of developing a hand control interface for a robotic wheelchair. We realize real-time hand tracking in dynamic environments of the wheelchair using MSEPF. Extensive experimental results demonstrate that MSEPF outperforms the MS tracker and the conventional particle ﬁlter in hand tracking. Our approach produces reliable tracking while effectively handling rapid motion and distraction with roughly 85{\%} fewer particles. We also present a simple method for dynamic gesture recognition. The hand control interface based on the proposed algorithms works well in dynamic environments of the wheelchair. 2007 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.},
author = {Shan, Caifeng and Tan, Tieniu and Wei, Yucheng},
doi = {10.1016/j.patcog.2006.12.012},
file = {:home/abetan16/Dropbox/MendeleyV3/Shan, Tan, Wei - 2007 - Real-time Hand Tracking Using a Mean Shift Embedded Particle Filter.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {computer interaction,hand gesture recognition,hand tracking,human,mean shift,particle filter},
month = {jul},
number = {7},
pages = {1958--1970},
title = {{Real-time hand tracking using a mean shift embedded particle filter}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0031320306005243},
volume = {40},
year = {2007}
}
@inproceedings{Shi2006,
abstract = {Graphical models are often used to represent and recognize activities. Purely unsupervised methods (such as HMMs) can be trained automatically but yield models whose internal structure - the nodes - are difficult to interpret semantically. Manually constructed networks typically have nodes corresponding to sub-events, but the programming and training of these networks is tedious and requires extensive domain expertise. In this paper, we propose a semi-supervised approach in which a manually structured, Propagation Network (a form of a DBN) is initialized from a small amount of fully annotated data, and then refined by an EM-based learning method in an unsupervised fashion. During node refinement (the M step) a boosting-based algorithm is employed to train the evidence detectors of individual nodes. Experiments on a variety of data types - vision and inertial measurements - in several tasks demonstrate the ability to learn from as little as one fully annotated example accompanied by a small number of positive but non-annotated training examples. The system is applied to both recognition and anomaly detection tasks.},
author = {Shi, Y and Bobick, A and Essa, I},
booktitle = {Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2006.174},
file = {:home/abetan16/Dropbox/MendeleyV3/Shi, Bobick, Essa - 2006 - Learning Temporal Sequence Model from Partially Labeled Data.pdf:pdf},
isbn = {0769525970},
pages = {1631 -- 1638},
publisher = {IEEE},
title = {{Learning Temporal Sequence Model from Partially Labeled Data}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1640951},
year = {2006}
}
@article{Siagian2007,
abstract = {We describe and validate a simple context-based scene recognition algorithm for mobile robotics applications. The system can differentiate outdoor scenes from various sites on a college campus using a multiscale set of early-visual features, which capture the "gist" of the scene into a low-dimensional signature vector. Distinct from previous approaches, the algorithm presents the advantage of being biologically plausible and of having low-computational complexity, sharing its low-level features with a model for visual attention that may operate concurrently on a robot. We compare classification accuracy using scenes filmed at three outdoor sites on campus (13,965 to 34,711 frames per site). Dividing each site into nine segments, we obtain segment classification rates between 84.21 percent and 88.62 percent. Combining scenes from all sites (75,073 frames in total) yields 86.45 percent correct classification, demonstrating the generalization and scalability of the approach.},
author = {Siagian, Christian and Itti, Laurent},
doi = {10.1109/TPAMI.2007.40},
file = {:home/abetan16/Dropbox/MendeleyV3/Siagian, Itti - 2007 - Rapid Biologically-inspired Scene Classification Using Features Shared with Visual Attention.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Algorithms,Artificial Intelligence,Attention,Automated,Automated: methods,Biomimetics,Biomimetics: methods,Computer Systems,Computer-Assisted,Computer-Assisted: methods,Image Enhancement,Image Enhancement: methods,Image Interpretation,Imaging,Information Storage and Retrieval,Information Storage and Retrieval: methods,Pattern Recognition,Robotics,Robotics: methods,Three-Dimensional,Three-Dimensional: methods,Visual Perception},
month = {feb},
number = {2},
pages = {300--312},
pmid = {17170482},
title = {{Rapid Biologically-Inspired Scene Classification Using Features Shared with Visual Attention}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4042704},
volume = {29},
year = {2007}
}
@article{Silvey2014,
author = {Silvey, B. a. and Major, M. L.},
doi = {10.1177/1321103X14523532},
file = {:home/abetan16/Dropbox/MendeleyV3/Silvey, Major - 2014 - Undergraduate music education majors' perceptions of their development as conductors Insights from a basic conduc.pdf:pdf},
issn = {1321-103X},
journal = {Research Studies in Music Education},
keywords = {and leadership are cited,as three of the,basic conducting,beginning conductors,effective teaching comprises a,gesture,leadership,most important characteristics of,nonverbal communication,of these skills,score study,teacher self-efficacy,wide array of skills},
month = {feb},
number = {1},
pages = {75--89},
title = {{Undergraduate music education majors' perceptions of their development as conductors: Insights from a basic conducting course}},
url = {http://rsm.sagepub.com/cgi/doi/10.1177/1321103X14523532},
volume = {36},
year = {2014}
}
@inproceedings{Serra2013,
abstract = {Portable devices for first-person camera views will play a central role in future interactive systems. One necessary step for feasible human-computer guided activities is gesture recognition, preceded by a reliable hand segmentation from egocentric vision. In this work we provide a novel hand segmentation algorithm based on Random Forest superpixel classification that integrates light, time and space consistency. We also propose a gesture recognition method based Exemplar SVMs since it requires a only small set of positive sampels, hence it is well suitable for the egocentric video applications. Furthermore, this method is enhanced by using segmented images instead of full frames during test phase. Experimental results show that our hand segmentation algorithm outperforms the state-of-the-art approaches and improves the gesture recognition accuracy on both the publicly available EDSH dataset and our dataset designed for cultural heritage applications.},
address = {New York, NY, USA},
author = {Singhai, Sonal and Satsangi, C.S},
booktitle = {Workshop on Interactive Multimedia on Mobile {\&} Portable Devices},
file = {:home/abetan16/Dropbox/MendeleyV3/Singhai, Satsangi - 2014 - Hand Segmentation for Hand Gesture Recognition.pdf:pdf},
isbn = {9781450323994},
keywords = {clutter,color model,hand segmentation,hci,human computer interaction,image processing},
number = {2},
pages = {48--52},
publisher = {ACM Press},
title = {{Hand Segmentation for Hand Gesture Recognition}},
url = {http://dl.acm.org/citation.cfm?id=2505490},
volume = {1},
year = {2014}
}
@inproceedings{Sivic2005,
abstract = { We seek to discover the object categories depicted in a set of unlabelled images. We achieve this using a model developed in the statistical text literature: probabilistic latent semantic analysis (pLSA). In text analysis, this is used to discover topics in a corpus using the bag-of-words document representation. Here we treat object categories as topics, so that an image containing instances of several categories is modeled as a mixture of topics. The model is applied to images by using a visual analogue of a word, formed by vector quantizing SIFT-like region descriptors. The topic discovery approach successfully translates to the visual domain: for a small set of objects, we show that both the object categories and their approximate spatial layout are found without supervision. Performance of this unsupervised method is compared to the supervised approach of Fergus et al. (2003) on a set of unseen images containing only one object per image. We also extend the bag-of-words vocabulary to include 'doublets' which encode spatially local co-occurring regions. It is demonstrated that this extended vocabulary gives a cleaner image segmentation. Finally, the classification and segmentation methods are applied to a set of images containing multiple objects per image. These results demonstrate that we can successfully build object class models from an unsupervised analysis of images.},
author = {Sivic, Josef and Russell, Bryan C. and Efros, Alexei a. and Zisserman, Andrew and Freeman, William T.},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2005.77},
file = {:home/abetan16/Dropbox/MendeleyV3/Sivic, Russell - 2005 - Discovering Objects and Their Location in Images.pdf:pdf},
isbn = {076952334X},
issn = {1550-5499},
number = {ii},
pages = {370--377},
title = {{Discovering objects and their location in images}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1541280},
volume = {I},
year = {2005}
}
@book{Smola2000,
address = {Cambridge, MA, USA},
author = {Smola, Alexander J. and Bartlett, Peter and Scholkopf, Bernhard and Schuurmans, Dale},
booktitle = {Advances},
file = {:home/abetan16/Dropbox/Articles/Mendeley/Smola - 2000 - Advances in Large Margin Classifiers.pdf:pdf},
isbn = {0262194481},
keywords = {Algorithms,Kernel functions,Machine learning},
publisher = {MIT Press},
title = {{Advances in Large Margin Classi ers}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=gOXI3fO3VUwC{\&}oi=fnd{\&}pg=PR6{\&}dq=Advances+in+Large+Margin+Classi+ers{\&}ots=ih7Hf9zQVG{\&}sig=nm6FJnXL5-M7ba-yPWxI9UV9pbw},
year = {1999}
}
@article{Song2015,
author = {Song, Sibo and Chandrasekhar, Vijay and Cheung, Ngai-man and Narayan, Sanath},
file = {:home/abetan16/Dropbox/MendeleyV3/Song et al. - 2014 - Activity Recognition in Egocentric Life-logging Videos.pdf:pdf},
journal = {Asian Conference on Computer Vision (ACCV)},
pages = {1--15},
title = {{Activity Recognition in Egocentric Life-logging Videos}},
url = {http://vijaychan.github.io/Publications/2014 - Egocentric Activity Recognition.pdf},
year = {2014}
}
@article{Spain2010,
abstract = {How important is a particular object in a photograph of a complex scene? We propose a definition of importance and present two methods for measuring object importance from human observers. Using this ground truth, we fit a function for predicting the importance of each object directly from a segmented image; our function combines a large number of object-related and image-related features. We validate our importance predictions on 2,841 objects and find that the most important objects may be identified automatically. We find that object position and size are particularly informative, while a popular measure of saliency is not.},
author = {Spain, Merrielle and Perona, Pietro},
doi = {10.1007/s11263-010-0376-0},
file = {:home/abetan16/Dropbox/MendeleyV3/Spain, Perona - 2011 - Measuring and Predicting Object Importance.pdf:pdf},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {Amazon Mechanical Turk,Importance,Keywording,Object recognition,Perception,Rank aggregation,Saliency,Visual recognition},
month = {aug},
number = {December 2009},
pages = {59--76},
title = {{Measuring and predicting object importance}},
url = {http://link.springer.com/10.1007/s11263-010-0376-0},
volume = {91},
year = {2011}
}
@article{Speth2013a,
abstract = {BACKGROUND: The Observational Skills Assessment Score (OSAS) measures amount and quality of use of the affected hand in children with unilateral Cerebral Palsy (CP) in bimanual activities and could therefore be a valuable addition to existing assessment tools. The OSAS consists of tasks that are age appropriate and require use of the affected hand.$\backslash$n$\backslash$nMETHODS: To measure the agreement and reliability of the OSAS a convenience sample of two groups of 16 children with unilateral spastic CP (2.5-6 and 12-16 years old), performed age specific bimanual tasks in 2 measurement sessions. Three experienced raters took part in testing and 8 in scoring. Intra class correlation (ICC) values for intra- and inter-rater reliability, and the mean and standard deviation of the differences between measurements were calculated. For test-retest reliability beside ICC scores, Smallest Detectable Differences (SDDs) were calculated in 16 older and 10 younger children.$\backslash$n$\backslash$nRESULTS: Generally, there seems to be good agreement between repeated measurements of the OSAS, as indicated by the small SDDs on most scales for quality of movement, compared to the range of their scales. This indicates potentially good sensitivity to change if used for patient evaluation purposes. The exceptions were the 'quality of reach' score for all tasks, and all quality scores for the stacking blocks task for the young children. As used in the present study, the OSAS has good discriminative capacity within patient populations as indicated by the high ICCs for most quality scores. Measuring the amount of use does not seem to be useful for either discrimination or evaluation.$\backslash$n$\backslash$nCONCLUSION: In general, the OSAS seems to be a reliable tool for assessing the quality of use of the affected hand in bimanual activities in younger and older children with unilateral CP. Some modifications may improve its usefulness and efficiency.},
author = {Speth, Lucianne and Janssen-Potten, Yvonne and Leffers, Pieter and Rameckers, Eugene and Defesche, Anke and Geers, Richard and Smeets, Rob and Vles, Hans},
doi = {10.1186/1471-2377-13-152},
file = {:home/abetan16/Dropbox/MendeleyV3/Speth et al. - 2013 - Observational skills assessment score reliability in measuring amount and quality of use of the affected hand i(2).pdf:pdf},
isbn = {1471237713152},
issn = {1471-2377},
journal = {BMC neurology},
keywords = {Adolescent,Cerebral Palsy,Cerebral Palsy: diagnosis,Cerebral Palsy: therapy,Child,Child, Preschool,Disability Evaluation,Double-Blind Method,Female,Hand,Hand: pathology,Humans,Male,Observer Variation,Psychomotor Performance,Psychomotor Performance: physiology,Reproducibility of Results},
pages = {152},
pmid = {24139170},
title = {{Observational skills assessment score: reliability in measuring amount and quality of use of the affected hand in unilateral cerebral palsy.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3853931{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {13},
year = {2013}
}
@article{Speth2013,
author = {Speth, Lucianne and Janssen-Potten, Yvonne and Leffers, Pieter and Rameckers, Eugene and Defesche, Anke and Geers, Richard and Smeets, Rob and Vles, Hans},
doi = {10.1186/1471-2377-13-152},
file = {:home/abetan16/Dropbox/MendeleyV3/Speth et al. - 2013 - Observational skills assessment score reliability in measuring amount and quality of use of the affected hand in u.pdf:pdf},
issn = {1471-2377},
journal = {BMC Neurology},
keywords = {Bimanual performance,Cerebral palsy,Outcome assessment,Reliability,Upper limb,bimanual performance,cerebral palsy,outcome assessment,reliability,upper limb},
number = {1},
pages = {152},
publisher = {BMC Neurology},
title = {{Observational skills assessment score: reliability in measuring amount and quality of use of the affected hand in unilateral cerebral palsy}},
url = {http://www.biomedcentral.com/1471-2377/13/152},
volume = {13},
year = {2013}
}
@inproceedings{Spriggs2009,
abstract = {Temporal segmentation of human motion into actions is central to the understanding and building of computational models of human motion and activity recognition. Several issues contribute to the challenge of temporal segmentation and classification of human motion. These include the large variability in the temporal scale and periodicity of human actions, the complexity of representing articulated motion, and the exponential nature of all possible movement combinations. We provide initial results from investigating two distinct problems -classification of the overall task being performed, and the more difficult problem of classifying individual frames over time into specific actions. We explore first-person sensing through a wearable camera and inertial measurement units (IMUs) for temporally segmenting human motion into actions and performing activity classification in the context of cooking and recipe preparation in a natural environment. We present baseline results for supervised and unsupervised temporal segmentation, and recipe recognition in the CMU-multimodal activity database (CMU-MMAC).},
author = {Spriggs, Ekaterina H. and {De La Torre}, Fernando and Hebert, Martial},
booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2009},
doi = {10.1109/CVPR.2009.5204354},
file = {:home/abetan16/Dropbox/MendeleyV3/Spriggs, De La Torre, Hebert - 2009 - Temporal Segmentation and Activity Classification from First-person Sensing.pdf:pdf},
isbn = {9781424439911},
issn = {2160-7508},
month = {jun},
pages = {17--24},
publisher = {IEEE},
title = {{Temporal segmentation and activity classification from first-person sensing}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5204354},
year = {2009}
}
@inproceedings{Spruyt2013,
abstract = {This paper proposes a complete tracking system that is capable of long-term, real-time hand tracking with unsupervised initialization and error recovery. Initialization is steered by a three-stage hand detector, combining spatial and temporal information. Hand hypotheses are generated by a random forest detector in the first stage, whereas a simple linear classifier eliminates false positive detections. Resulting detections are tracked by particle filters that gather temporal statistics in order to make a final decision. The detector is scale and rotation invariant, and can detect hands in any pose in unconstrained environments. The resulting discriminative confidence map is combined with a generative particle filter based observation model to enable robust, long-term hand tracking in real-time. The proposed solution is evaluated using several challenging, publicly available datasets, and is shown to clearly outperform other state of the art object tracking methods.},
address = {Melbourne, Australia},
author = {Spruyt, Vincent and Ledda, Alessandro and Philips, Wilfried},
booktitle = {Proceedings of the IEEE International Conference on Image Processing},
file = {:home/abetan16/Dropbox/MendeleyV3/Spruyt, Ledda, Philips - 2013 - Real-time, Long-term Hand Tracking with Unsupervised Initialization.pdf:pdf},
pages = {4},
publisher = {IEEE Comput. Soc},
title = {{Real-time, long-term hand tracking with unsupervised initialization}},
url = {https://biblio.ugent.be/publication/4158550},
year = {2013}
}
@inproceedings{Starner1998,
abstract = {Small, body-mounted video cameras enable a different style of wearable computing interface. As processing power increases, a wearable computer can spend more time observing its user to provide serendipitous information, manage interruptions and tasks, and predict future needs without being directly commanded by the user. This paper introduces an assistant for playing the real-space game Patrol. This assistant tracks the wearer's location and current task through computer vision techniques and without off-body infrastructure. In addition, this paper continues augmented reality research, started in 1995, for binding virtual data to physical locations},
author = {Starner, T and Schiele, B and Pentland, A},
booktitle = {Digest of Papers Second International Symposium on Wearable Computers Cat No98EX215},
doi = {10.1109/ISWC.1998.729529},
file = {:home/abetan16/Dropbox/MendeleyV3/Starner, Schiele, Pentland - 1998 - Visual contextual awareness in wearable computing.pdf:pdf},
isbn = {0-8186-9074-7},
pages = {50--57},
publisher = {IEEE Computer Society},
title = {{Visual contextual awareness in wearable computing}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=729529},
year = {1998}
}
@phdthesis{Starner1999,
abstract = {Computer hardware continues to shrink in size and increase in capability. This trend has allowed the prevailing concept of a computer to evolve from the mainframe to the minicomputer to the desktop. Just as the physical hardware changes, so does the use of the technology, tending towards more interactive and personal systems. Currently, another physical change is underway, placing computational power on the user's body. These wearable machines encourage new applications that were formerly infeasible and, correspondingly, will result in new usage patterns. This thesis suggests that the fundamental improvement offered by wearable computing is an increased sense of user context. I hypothesize that on-body systems can sense the user's context with little or no assistance from environmental infrastructure. These body-centered systems that "see" as the user sees and "hear" as the user hears, provide a unique "first-person" viewpoint of the user's environment. By exploiting models recovered ...},
author = {Starner, Thad},
file = {:home/abetan16/Dropbox/MendeleyV3/Starner - 1999 - Wearable Computing and Contextual Awareness.pdf:pdf},
number = {1991},
pages = {1--243},
school = {Massachusetts Institute of Technology},
title = {{Wearable Computing and Contextual Awareness}},
year = {1999}
}
@techreport{Starner1997,
abstract = {Wearable computing moves computation from the desktop to the user. We are forming a community of networked wearable computer users to explore, over a long period, the augmented realities that these systems can provide. By adapting its behavior to the user's changing environment, a body-worn computer can assist the user more intelligently, consistently, and continuously than a desktop system. A text-based augmented reality, the Remembrance Agent, is presented to illustrate this approach. Video cameras are used both to warp the visual input (mediated reality) and to sense the user's world for graphical overlay. With a camera, the computer tracks the user's finger, which acts as the system's mouse; performs face recognition; and detects passive objects to overlay 2.5D and 3D graphics onto the real world. Additional apparatus such as audio systems, infrared beacons for sensing location, and biosensors for learning about the wearer's affect are described. Using the input from these interfac...},
author = {Starner, Thad and Mann, Steve and Rhodes, Bradley and Levine, Je and Healey, Jennifer and Kirsch, Dana and Picard, R W and Pentland, Alex},
booktitle = {Presence Teleoperators and Virtual Environments},
doi = {10.1.1.26.2886},
file = {:home/abetan16/Dropbox/MendeleyV3/Starner et al. - 1997 - Augmented Reality Through Wearable Computing.pdf:pdf},
issn = {10547460},
number = {4},
pages = {386--398},
pmid = {17506695},
title = {{Augmented reality through wearable computing}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.26.2886{\&}rep=rep1{\&}type=pdf},
volume = {6},
year = {1997}
}
@article{Starner1998a,
abstract = {We present two real-time hidden Markov model-based systems for recognizing sentence-level continuous American Sign Language (ASL) using a single camera to track the user's unadorned hands. The first system observes the user from a desk mounted camera and achieves 92 percent word accuracy. The second system mounts the camera in a cap worn by the user and achieves 98 percent accuracy (97 percent with an unrestricted grammar). Both experiments use a 40-word lexicon.},
author = {Starner, Thad and Weaver, Joshua and Pentland, Alex},
doi = {10.1109/34.735811},
file = {:home/abetan16/Dropbox/MendeleyV3/Starner, Weaver, Pentland - 1998 - Real-Time American Sign Language Recognition Using Desk and Wearable Computer Based Video.pdf:pdf},
issn = {01628828},
journal = {Transactions on Pattern Analysis and Machine Intelligence},
number = {466},
pages = {1371--1375},
title = {{Real-Time American Sign Language Recognition Using Desk and Wearable Computer Based Video}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=735811},
volume = {20},
year = {1998}
}
@inproceedings{Stauffer1999,
abstract = {A common method for real-time segmentation of moving regions in image sequences involves “background subtraction“, or thresholding the error between an estimate of the image without moving objects and the current image. The numerous approaches to this problem differ in the type of background model used and the procedure used to update the model. This paper discusses modeling each pixel as a mixture of Gaussians and using an on-line approximation to update the model. The Gaussian, distributions of the adaptive mixture model are then evaluated to determine which are most likely to result from a background process. Each pixel is classified based on whether the Gaussian distribution which represents it most effectively is considered part of the background model. This results in a stable, real-time outdoor tracker which reliably deals with lighting changes, repetitive motions from clutter, and long-term scene changes. This system has been run almost continuously for 16 months, 24 hours a day, through rain and snow},
address = {Fort Collins, CO},
author = {Stauffer, C. and Grimson, W.E.L.},
booktitle = {Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149)},
doi = {10.1109/CVPR.1999.784637},
file = {:home/abetan16/Dropbox/MendeleyV3/Stauffer, Grimson - 1999 - Adaptive Background Mixture Models for Real-time Tracking.pdf:pdf},
isbn = {0-7695-0149-4},
pages = {246--252},
publisher = {IEEE Comput. Soc},
title = {{Adaptive background mixture models for real-time tracking}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=784637},
year = {1999}
}
@inproceedings{Stenger2001,
abstract = {This paper presents a novel method for hand tracking. It uses a 3D model built from quadrics which approximates the anatomy of a human hand. This approach allows for the use of results from projective geometry that yield an elegant technique to generate the projection of the model as a set of conics, as well as providing an efficient ray tracing algorithm to handle self-occlusion. Once the model is projected, an Unscented Kalman Filter is used to update its pose in order to minimise the geometric error between the model projection and a video sequence on the background. Results from experiments with real data show the accuracy of the technique. 1},
author = {Stenger, B and Mendon{\c{c}}a, PRS and Cipolla, R},
booktitle = {Bmvc},
file = {:home/abetan16/Dropbox/MendeleyV3/Stenger, Mendon{\c{c}}a, Cipolla - 2001 - Model-Based Hand Tracking Using an Unscented Kalman Filter.pdf:pdf},
pages = {63--72},
title = {{Model-Based Hand Tracking Using an Unscented Kalman Filter.}},
url = {http://pdf.aminer.org/000/067/511/model{\_}based{\_}hand{\_}tracking{\_}using{\_}an{\_}unscented{\_}kalman{\_}filter.pdf},
year = {2001}
}
@misc{Stern2015,
author = {Stern, Kevin},
booktitle = {https://github.com/KevinStern/software-and-algorithms},
title = {{Kevin Ster - Software and algorithms}},
year = {2015}
}
@inproceedings{Steunebrink2009,
abstract = {Although popular among computer scientists, theOCCmodel of emo- tions contains a number of ambiguities that stand in the way of a truthful formal- ization or implementation. This paper aims to identify and clarify several of these ambiguities. Furthermore, a new inheritance-based view of the logical structure of emotions of the OCC model is proposed and discussed.},
address = {Paderborn, Germany},
archivePrefix = {arXiv},
arxivId = {0810.3720v1},
author = {Steunebrink, Bas R. and Dastani, Mehdi and Meyer, John-Jules Ch.},
booktitle = {4th Workshop on Emotion and Computing},
eprint = {0810.3720v1},
file = {:home/abetan16/Dropbox/MendeleyV3/Steunebrink, Dastani, Meyer - 2009 - The OCC Model Revisited.pdf:pdf},
issn = {00084026},
pages = {1--8},
title = {{The OCC Model Revisited}},
url = {http://www.idsia.ch/{~}steunebrink/Publications/KI09{\_}OCC{\_}revisited.pdf},
year = {2009}
}
@article{Sucar2008,
abstract = {Each year millions of people in the world survive a stroke, in the U.S. alone the figure is over 600,000 people per year. Movement impairments after stroke are typically treated with intensive, hands-on physical and occupa- tional therapy for several weeks after the initial injury. However, due to eco- nomic pressures, stroke patients are receiving less therapy and going home sooner, so the potential benefit of the therapy is not completely realized. Thus, it is important to develop rehabilitation technology that allows individuals who had suffered a stroke to practice intensive movement training without the ex- pense of an always-present therapist. Current solutions are too expensive, as they require a robotic system for rehabilitation. We have developed a low-cost, computer vision system that allows individuals with stroke to practice arm movement exercises at home or at the clinic, with periodic interactions with a therapist. The system integrates a web based virtual environment for facilitating repetitive movement training, with state-of-the art computer vision algorithms that track the hand of a patient and obtain its 3-D coordinates, using two inex- pensive cameras and a conventional personal computer. An initial prototype of the system has been evaluated in a pilot clinical study with promising results.},
author = {Sucar, L. Enrique and Azc{\'{a}}rate, Gildardo and Leder, Ron S. and Reinkensmeyer, David and Hern{\'{a}}ndez, Jorge and Sanchez, Israel and Saucedo, Pedro},
doi = {10.1007/978-3-540-92219-3_40},
file = {:home/abetan16/Dropbox/MendeleyV3/Sucar et al. - 2008 - Gesture therapy A vision-based system for arm rehabilitation after stroke.pdf:pdf},
isbn = {3540922180},
issn = {18650929},
journal = {Communications in Computer and Information Science},
keywords = {Rehabilitation,computer vision,stroke,therapeutic technology},
number = {Asa 2004},
pages = {531--540},
title = {{Gesture therapy: A vision-based system for arm rehabilitation after stroke}},
volume = {25 CCIS},
year = {2008}
}
@article{Sucar2010,
abstract = {Stroke is the main cause of motor and cognitive disabilities requiring therapy in the world. Therefor it is important to develop rehabilitation technology that allows individuals who had suffered a stroke to practice intensive movement training without the expense of an always-present therapist. We have developed a low-cost vision-based system that allows stroke survivors to practice arm movement exercises at home or at the clinic, with periodic interactions with a therapist. The system integrates a virtual environment for facilitating repetitive movement training, with computer vision algorithms that track the hand of a patient, using an inexpensive camera and a personal computer. This system, called Gesture Therapy, includes a gripper with a pressure sensor to include hand and finger rehabilitation; and it tracks the head of the patient to detect and avoid trunk compensation. It has been evaluated in a controlled clinical trial at the National Institute for Neurology and Neurosurgery in Mexico City, comparing it with conventional occupational therapy. In this paper we describe the latest version of the Gesture Therapy System and summarize the results of the clinical trail.},
author = {Sucar, L. Enrique and Luis, Roger and Leder, Ron and Hern{\'{a}}ndez, Jorge and S{\'{a}}nchez, Israel},
doi = {10.1109/IEMBS.2010.5627458},
file = {:home/abetan16/Dropbox/MendeleyV3/Sucar et al. - 2010 - Gesture therapy A vision-based system for upper extremity stroke rehabilitation.pdf:pdf},
isbn = {9781424441235},
issn = {1557-170X},
journal = {2010 Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC'10},
number = {July 2015},
pages = {3690--3693},
pmid = {21096856},
title = {{Gesture therapy: A vision-based system for upper extremity stroke rehabilitation}},
year = {2010}
}
@inproceedings{Sun2009,
author = {Sun, L and Klank, U and Beetz, M},
booktitle = {Computer Vision and Pattern Recognition},
file = {:home/abetan16/Dropbox/MendeleyV3/Sun, Klank, Beetz - 2009 - Eyewatchme—3d Hand and Object Tracking for Inside out Activity Analysis.pdf:pdf},
isbn = {9781424439935},
pages = {9--16},
title = {{Eyewatchme—3d Hand and Object Tracking for Inside out Activity Analysis}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5204358},
year = {2009}
}
@article{Sundaram2009,
author = {Sundaram, S and Cuevas, W},
doi = {10.1109/CVPRW.2009.5204355},
file = {:home/abetan16/Dropbox/MendeleyV3/Sundaram, Cuevas - 2009 - High Level Activity Recognition Using Low Resolution Wearable Vision.pdf:pdf},
isbn = {978-1-4244-3994-2},
journal = {Computer Vision and Pattern Recognition Workshops},
month = {jun},
pages = {25--32},
publisher = {Ieee},
title = {{High Level Activity Recognition Using Low Resolution Wearable Vision}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5204355},
year = {2009}
}
@book{SurartJ.Russel2010,
author = {{Surart J. Russel}, Peter Norving},
doi = {a},
file = {:home/abetan16/Dropbox/MendeleyV3/Surart J. Russel - 2010 - Artificial Intelligence A Modern Approach.pdf:pdf},
isbn = {0-13-103805-2},
publisher = {Prentice-Hall Inc},
title = {{Artificial Intelligence: A Modern Approach}},
url = {http://www.ucllouvain.be/en-329933.html},
year = {2010}
}
@article{Swinnen2004,
abstract = {Bimanual coordination, a prototype of a complex motor skill, has recently become the subject of intensive investigation. Whereas past research focused mainly on the identification of the elementary coordination constraints that limit performance, the focus is now shifting towards overcoming these coordination constraints by means of task symbolization or perceptual transformation rules that promote the integration of the task components into a meaningful 'gestalt'. The study of these cognitive penetrations into action will narrow the brain-mind gap and will facilitate the development of a cognitive neuroscience perspective on bimanual movement control.},
author = {Swinnen, Stephan P. and Wenderoth, Nicole},
doi = {10.1016/j.tics.2003.10.017},
file = {:home/abetan16/Dropbox/MendeleyV3/Swinnen, Wenderoth - 2004 - Two hands, one brain Cognitive neuroscience of bimanual skill.pdf:pdf},
isbn = {1364-6613},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {1},
pages = {18--25},
pmid = {14697399},
title = {{Two hands, one brain: Cognitive neuroscience of bimanual skill}},
volume = {8},
year = {2004}
}
@inproceedings{Tan,
annote = {BLUR
Optical Flow - SIFT
Illumination
Least Square

      },
author = {Tan, Cheston and Goh, Hanlin and Chandrasekhar, Vijay},
booktitle = {Computer Vision and Pattern Recognition},
file = {:home/abetan16/Dropbox/MendeleyV3/Tan, Goh, Chandrasekhar - 2014 - Understanding the Nature of First-Person Videos Characterization and Classification using Low-Level Fea.pdf:pdf},
pages = {535--542},
title = {{Understanding the Nature of First-Person Videos: Characterization and Classification using Low-Level Features}},
url = {http://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}workshops{\_}2014/W16/html/Tan{\_}Understanding{\_}the{\_}Nature{\_}2014{\_}CVPR{\_}paper.html},
year = {2014}
}
@inproceedings{Tancharoen2005,
abstract = {This paper presents an experience recording system and proposes practical video retrieval techniques based on Life Log content and context analysis. We summarize our effective indexing methods including content based talking scene detection and context based key frame extraction based on GPS data. The voice annotation and detection is proposed for practical indexing method. Moreover, we apply an additional body sensor to record our life style and analyze human's physiological data for Life Log retrieval system. In the experiments, we demonstrated various video indexing results which provided their semantic key frames and Life Log interfaces to retrieve and index our life experiences effectively.},
address = {New York, New York, USA},
author = {Tancharoen, D and Yamasaki, T and Aizawa, K},
booktitle = {Workshop on Continuous Archival and Retrieval of Personal Experiences},
doi = {10.1145/1099083.1099092},
file = {:home/abetan16/Dropbox/MendeleyV3/Tancharoen, Yamasaki, Aizawa - 2005 - Practical experience recording and indexing of Life Log video.pdf:pdf},
isbn = {1595932461},
pages = {61},
publisher = {ACM Press},
title = {{Practical experience recording and indexing of Life Log video}},
url = {http://portal.acm.org/citation.cfm?doid=1099083.1099092},
year = {2005}
}
@article{Tao2007,
abstract = {The integration of visual and inertial sensors for human motion tracking has attracted significant attention recently, due to its robust performance and wide potential application. This paper introduces a real-time hybrid solution to articulated 3D arm motion tracking for home-based rehabilitation by combining visual and inertial sensors. Data fusion is a key issue in this hybrid system and two different data fusion methods are proposed. The first is a deterministic method based on arm structure and geometry information, which is suitable for simple rehabilitation motions. The second is a probabilistic method based on an Extended Kalman Filter (EKF) in which data from two sensors is fused in a predict-correct manner in order to deal with sensor noise and model inaccuracy. Experimental results are presented and compared with commercial marker-based systems, CODA and Qualysis. They show good performance for the proposed solution.},
author = {Tao, Y. and Hu, H. and Zhou, H.},
doi = {10.1177/0278364907079278},
file = {:home/abetan16/Dropbox/MendeleyV3/Tao, Hu, Zhou - 2007 - Integration of Vision and Inertial Sensors for 3D Arm Motion Tracking in Home-based Rehabilitation.pdf:pdf},
isbn = {0278364907079},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {extended kalman filter,home-based rehabilitation,human motion tracking,iner-,sensor fusion,tial sensor},
number = {6},
pages = {607--624},
title = {{Integration of Vision and Inertial Sensors for 3D Arm Motion Tracking in Home-based Rehabilitation}},
volume = {26},
year = {2007}
}
@inproceedings{Tapia2004,
abstract = {In this work, a system for recognizing activities in the home setting using a set of small and simple state-change sensors is introduced. The sensors are designed to be “tape on and forget” devices that can be quickly and ubiquitously installed in home environments. The proposed sensing system presents an alternative to sensors that are sometimes per- ceived as invasive, such as cameras and microphones. Unlike prior work, the system has been deployed in multiple residential environments with non-researcher occupants. Preliminary results on a small dataset show that it is possible to recognize activities of interest to medical profession- als such as toileting, bathing, and grooming with detection accuracies ranging from 25{\%} to 89{\%} depending on the evaluation criteria used},
author = {Tapia, E and Intille, S and Larson, K},
booktitle = {Pervasive Computing},
file = {:home/abetan16/Dropbox/MendeleyV3/Tapia, Intille, Larson - 2004 - Activity Recognition in the Home Using Simple and Ubiquitous Sensors.pdf:pdf},
pages = {158--175},
publisher = {CVPR},
title = {{Activity Recognition in the Home Using Simple and Ubiquitous Sensors}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-24646-6{\_}10},
year = {2004}
}
@article{Tasdemir2014,
author = {Tasdemir, Kadim},
doi = {10.1109/ICDMW.2014.42},
file = {:home/abetan16/Dropbox/MendeleyV3/Tasdemir - 2014 - Dimensionality Reduction Based Similarity Visualization for Neural Gas.pdf:pdf},
isbn = {978-1-4799-4274-9},
journal = {2014 IEEE International Conference on Data Mining Workshop},
keywords = {-data visualization,connectivity,dimensionality reduction,neural gas,similarity visualization,t-sne},
month = {dec},
pages = {668--675},
publisher = {Ieee},
title = {{Dimensionality Reduction Based Similarity Visualization for Neural Gas}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7022660},
year = {2014}
}
@article{Tatler2010,
abstract = {The impact of Yarbus's research on eye movements was enormous following the translation of his book Eye Movements and Vision into English in 1967. In stark contrast, the published material in English concerning his life is scant. We provide a brief biography of Yarbus and assess his impact on contemporary approaches to research on eye movements. While early interest in his work focused on his study of stabilised retinal images, more recently this has been replaced with interest in his work on the cognitive influences on scanning patterns. We extended his experiment on the effect of instructions on viewing a picture using a portrait of Yarbus rather than a painting. The results obtained broadly supported those found by Yarbus.},
author = {Tatler, B and Wade, N and Kwan, H and Findlay, J and Velichkovsky, B},
doi = {10.1068/i0382},
file = {:home/abetan16/Dropbox/MendeleyV3/Tatler et al. - 2010 - Yarbus, Eye Movements, and Vision.pdf:pdf},
issn = {2041-6695},
journal = {i-Perception},
keywords = {eye guidance,eye movement,face perception,saccade,scene perception,stabilised retinal,yarbus},
month = {jan},
number = {1},
pages = {7--27},
pmid = {23396904},
title = {{Yarbus, Eye Movements, and Vision.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3563050{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {1},
year = {2010}
}
@article{Teixeira2010,
abstract = {An increasingly common requirement of computer systems is to extract information regarding the people present in an environment. In this article, we provide a comprehensive, multi-disciplinary survey of the existing literature, focusing mainly on the extraction of five commonly needed spatio- temporal properties: namely presence, count, location, track and identity. We discuss a new taxonomy of observable human properties and physical traits, along with the sensing modalities that can be used to extract them. We compare active vs. passive sensors, and single-modality vs. sensor fusion approaches, in instrumented vs. uninstrumented settings, surveying sensors as diverse as cameras, motion sensors, pressure pads, radars, electric field sensors, and wearable inertial sensors, among others. The goal of this work is to summarize the existing solutions from various disciplines, to guide the creation of new systems and point toward future research directions.},
author = {Teixeira, T and Dublon, G and Savvides, A},
file = {:home/abetan16/Dropbox/MendeleyV3/Teixeira, Dublon, Savvides - 2010 - A Survey of Human-sensing Methods for Detecting Presence, Count, Location, Track, and Identity.pdf:pdf},
journal = {ACM Computing Surveys},
title = {{A Survey of Human-sensing: Methods for Detecting Presence, Count, Location, Track, and Identity}},
url = {http://thiagot.com/papers/teixeira{\_}techrep10{\_}survey{\_}of{\_}human{\_}sensing.pdf},
volume = {5},
year = {2010}
}
@inproceedings{Templeman2014,
abstract = {Cameras are now commonplace in our social and computing landscapes and embedded into consumer devices like smartphones and tablets. A new generation of wearable devices (such as Google Glass) will soon make ‘first-person' cameras nearly ubiquitous, capturing vast amounts of imagery without deliberate human action. ‘Lifelogging' devices and applications will record and share images from people's daily lives with their social networks. These devices that automatically capture images in the background raise serious privacy concerns, since they are likely to capture deeply private information. Users of these devices need ways to identify and prevent the sharing of sensitive images. As a first step, we introduce PlaceAvoider, a technique for owners of first-person cameras to ‘blacklist' sensitive spaces (like bathrooms and bedrooms). PlaceAvoider recognizes images captured in these spaces and flags them for review before the images are made available to applications. PlaceAvoider performs novel image analysis using both fine-grained image features (like specific objects) and coarse-grained, scene-level features (like colors and textures) to classify where a photo was taken. PlaceAvoider combines these features in a probabilistic framework that jointly labels streams of images in order to improve accuracy. We test the technique on five realistic first- person image datasets and show it is robust to blurriness, motion, and occlusion},
author = {Templeman, Robert and Korayem, Mohammed and Crandall, D.J. and Apu, Kadapia},
booktitle = {Network and Distributed System Security Symposium},
file = {:home/abetan16/Dropbox/MendeleyV3/Templeman et al. - 2014 - PlaceAvoider Steering first-person cameras away from sensitive spaces.pdf:pdf},
isbn = {1891562355},
number = {February},
pages = {23--26},
title = {{PlaceAvoider: Steering first-person cameras away from sensitive spaces}},
url = {https://www.cs.indiana.edu/{~}kapadia/papers/placeavoider-ndss14.pdf},
year = {2014}
}
@inproceedings{Tenmoku2005,
abstract = {By realizing augmented reality on wearable computers, it is possible to overlay annotations on the real world based on the user's current position and orientation. However, it is difficult for the user to understand links between anno- tations and real objects intuitively when the scene is com- plicated or many annotations are overlaid at the same time. This paper describes a viewmanagement methodwhich em- phasizes user-viewed real objects and their annotations us- ing 3Dmodels of the real scene. The proposedmethod high- lights the objects viewed by the user. In addition, when the viewed object is occluded by other real objects, the object is complemented by using an image, which is made from 3D models,},
author = {Tenmoku, R and Kanbara, M and Yokoya, N},
booktitle = {International Symposium on Mixed and Augmented Reality},
doi = {10.1109/ISMAR.2005.10},
file = {:home/abetan16/Dropbox/MendeleyV3/Tenmoku, Kanbara, Yokoya - 2005 - Annotating User-viewed Objects for Wearable Ar Systems.pdf:pdf},
isbn = {0-7695-2459-1},
pages = {192--193},
publisher = {Ieee},
title = {{Annotating User-viewed Objects for Wearable Ar Systems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1544689},
year = {2005}
}
@article{The,
author = {The, Cracking},
file = {:home/abetan16/Dropbox/MendeleyV3/The - Unknown - Cracking the Coding Interview, 4 Edition 150 Programming Interview Questions and Solutions.pdf:pdf},
title = {{Cracking the Coding Interview, 4 Edition: 150 Programming Interview Questions and Solutions}}
}
@inproceedings{Judd2009,
abstract = {For many applications in graphics, design, and human computer interaction, it is essential to understand where humans look in a scene. Where eye tracking devices are not a viable option, models of saliency can be used to predict fixation locations. Most saliency approaches are based on bottom-up computation that does not consider top-down image semantics and often does not match actual eye movements. To address this problem, we collected eye tracking data of 15 viewers on 1003 images and use this database as training and testing examples to learn a model of saliency based on low, middle and high-level image features. This large database of eye tracking data is publicly available with this paper.},
author = {Tilke, Juddk and Ehinger, Krista and Durand, Fr{\'{e}}do and Torralba, Antonio},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2009.5459462},
file = {:home/abetan16/Dropbox/MendeleyV3/Tilke et al. - 2009 - Learning to predict where humans look.pdf:pdf},
isbn = {9781424444205},
issn = {1550-5499},
number = {Iccv},
pages = {2106--2113},
pmid = {5459462},
title = {{Learning to predict where humans look}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5459462},
year = {2009}
}
@article{Torralba2003,
abstract = {There is general consensus that context can be a rich source of information about an object's identity, location and scale. In fact, the structure of many real-world scenes is governed by strong configurational rules akin to those that apply to a single object. Here we introduce a simple framework for modeling the relationship between context and object properties based on the correlation between the statistics of low-level features across the entire scene and the objects that it contains. The resulting scheme serves as an effective procedure for object priming, context driven focus of attention and automatic scale-selection on real-world scenes.},
author = {Torralba, A},
file = {:home/abetan16/Dropbox/MendeleyV3/Torralba - 2003 - Contextual Priming for Object Detection.pdf:pdf},
journal = {International Journal of Computer Vision},
keywords = {automatic scale selection,context,focus of attention,object priming,object recognition},
number = {2},
pages = {169--191},
title = {{Contextual Priming for Object Detection}},
url = {http://link.springer.com/article/10.1023/A:1023052124951},
volume = {53},
year = {2003}
}
@article{Torralba2006,
abstract = {Many experiments have shown that the human visual system makes extensive use of contextual information for facilitating object search in natural scenes. However, the question of how to formally model contextual influences is still open. On the basis of a Bayesian framework, the authors present an original approach of attentional guidance by global scene context. The model comprises 2 parallel pathways; one pathway computes local features (saliency) and the other computes global (scene-centered) features. The contextual guidance model of attention combines bottom-up saliency, scene context, and top-down mechanisms at an early stage of visual processing and predicts the image regions likely to be fixated by human observers performing natural search tasks in real-world scenes.},
author = {Torralba, A and Oliva, A and Castelhano, M and Henderson, J},
doi = {10.1037/0033-295X.113.4.766},
file = {:home/abetan16/Dropbox/MendeleyV3/Torralba et al. - 2006 - Contextual Guidance of Eye Movements and Attention in Real-world Scenes the Role of Global Features in Object S.pdf:pdf},
issn = {0033-295X},
journal = {Psychological Review},
keywords = {Attention,Bayes Theorem,Eye Movements,Humans,Models,Psychological,Social Environment,Visual Perception},
month = {oct},
number = {4},
pages = {766--86},
pmid = {17014302},
title = {{Contextual Guidance of Eye Movements and Attention in Real-world Scenes: the Role of Global Features in Object Search}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17014302},
volume = {113},
year = {2006}
}
@article{Turaga2008,
abstract = {The past decade has witnessed a rapid proliferation of video cameras in all walks of life and has resulted in a tremendous explosion of video content. Several applications such as content-based video annotation and retrieval, highlight extraction and video summarization require recognition of the activities occurring in the video. The analysis of human activities in videos is an area with increasingly important consequences from security and surveillance to entertainment and personal archiving. Several challenges at various levels of processing-robustness against errors in low-level processing, view and rate-invariant representations at midlevel processing and semantic representation of human activities at higher level processing-make this problem hard to solve. In this review paper, we present a comprehensive survey of efforts in the past couple of decades to address the problems of representation, recognition, and learning of human activities from video and related applications. We discuss the problem at two major levels of complexity: 1) "actions" and 2) "activities." "Actions" are characterized by simple motion patterns typically executed by a single human. "Activities" are more complex and involve coordinated actions among a small number of humans. We will discuss several approaches and classify them according to their ability to handle varying degrees of complexity as interpreted above. We begin with a discussion of approaches to model the simplest of action classes known as atomic or primitive actions that do not require sophisticated dynamical modeling. Then, methods to model actions with more complex dynamics are discussed. The discussion then leads naturally to methods for higher level representation of complex activities.},
author = {Turaga, P},
file = {:home/abetan16/Dropbox/MendeleyV3/Turaga - 2008 - Machine Recognition of Human Activities a Survey.pdf:pdf},
journal = {Circuits and Systems for Video Technology},
number = {11},
pages = {1473--1488},
title = {{Machine Recognition of Human Activities: a Survey}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4633644},
volume = {18},
year = {2008}
}
@article{Turolla2013,
abstract = {BACKGROUND: Recent evidence has demonstrated the efficacy of Virtual Reality (VR) for stroke rehabilitation nonetheless its benefits and limitations in large population of patients have not yet been studied.$\backslash$n$\backslash$nOBJECTIVES: To evaluate the effectiveness of non-immersive VR treatment for the restoration of the upper limb motor function and its impact on the activities of daily living capacities in post-stroke patients.$\backslash$n$\backslash$nMETHODS: A pragmatic clinical trial was conducted among post-stroke patients admitted to our rehabilitation hospital. We enrolled 376 subjects who had a motor arm subscore on the Italian version of the National Institutes of Health Stroke Scale (It-NIHSS) between 1 and 3 and without severe neuropsychological impairments interfering with recovery. Patients were allocated to two treatments groups, receiving combined VR and upper limb conventional (ULC) therapy or ULC therapy alone. The treatment programs consisted of 2 hours of daily therapy, delivered 5 days per week, for 4 weeks. The outcome measures were the Fugl-Meyer Upper Extremity (F-M UE) and Functional Independence Measure (FIM) scales.$\backslash$n$\backslash$nRESULTS: Both treatments significantly improved F-M UE and FIM scores, but the improvement obtained with VR rehabilitation was significantly greater than that achieved with ULC therapy alone. The estimated effect size of the minimal difference between groups in F-M UE and FIM scores was 2.5 ± 0.5 (P {\textless} 0.001) pts and 3.2 ± 1.2 (P = 0.007) pts, respectively.$\backslash$n$\backslash$nCONCLUSIONS: VR rehabilitation in post-stroke patients seems more effective than conventional interventions in restoring upper limb motor impairments and motor related functional abilities.$\backslash$n$\backslash$nTRIAL REGISTRATION: Italian Ministry of Health IRCCS Research Programme 2590412.},
author = {Turolla, Andrea and Dam, Mauro and Ventura, Laura and Tonin, Paolo and Agostini, Michela and Zucconi, Carla and Kiper, Pawel and Cagnin, Annachiara and Piron, Lamberto},
doi = {10.1186/1743-0003-10-85},
file = {:home/abetan16/Dropbox/MendeleyV3/Turolla et al. - 2013 - Virtual reality for the rehabilitation of the upper limb motor function after stroke a prospective controlled tr.pdf:pdf},
isbn = {1743-0003},
issn = {1743-0003},
journal = {Journal of neuroengineering and rehabilitation},
keywords = {Activities of Daily Living,Aged,Exercise Therapy,Female,Humans,Male,Middle Aged,Recovery of Function,Stroke,Stroke: physiopathology,Stroke: rehabilitation,Treatment Outcome,Upper Extremity,Upper Extremity: physiopathology,Virtual Reality Exposure Therapy,Virtual Reality Exposure Therapy: methods},
pages = {85},
pmid = {23914733},
title = {{Virtual reality for the rehabilitation of the upper limb motor function after stroke: a prospective controlled trial.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3734026{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {10},
year = {2013}
}
@article{Valentini2004,
author = {Valentini, Giorgio and Dietterich, TG},
file = {:home/abetan16/Dropbox/MendeleyV3/Valentini, Dietterich - 2004 - Bias-variance analysis of support vector machines for the development of SVM-based ensemble methods.pdf:pdf},
journal = {The Journal of Machine Learning Research},
keywords = {bias-variance analysis,ensemble methods,multi-classifier,support vector machines},
pages = {725--775},
title = {{Bias-variance Analysis of Support Vector Machines for the Development of Svm-based Ensemble Methods}},
url = {http://dl.acm.org/citation.cfm?id=1016783},
volume = {5},
year = {2004}
}
@inproceedings{Valstar2011,
abstract = {Automatic Facial Expression Recognition and Analysis, in particular FACS Action Unit (AU) detection and discrete emotion detection, has been an active topic in computer science for over two decades. Standardisation and comparability has come some way; for instance, there exist a number of commonly used facial expression databases. However, lack of a common evaluation protocol and lack of sufficient details to reproduce the reported individual results make it difficult to compare systems to each other. This in turn hinders the progress of the field. A periodical challenge in Facial Expression Recognition and Analysis would allow this comparison in a fair manner. It would clarify how far the field has come, and would allow us to identify new goals, challenges and targets. In this paper we present the first challenge in automatic recognition of facial expressions to be held during the IEEE conference on Face and Gesture Recognition 2011, in Santa Barbara, California. Two sub-challenges are defined: one on AU detection and another on discrete emotion detection. It outlines the evaluation protocol, the data used, and the results of a baseline method for the two sub-challenges.},
address = {Santa Barbara, CA},
author = {Valstar, M and Jiang, B and Mehu, M and Pantic, M and Scherer, K},
booktitle = {Automatic Face {\&} Gesture Recognition and Workshops},
doi = {10.1109/FG.2011.5771374},
file = {:home/abetan16/Dropbox/MendeleyV3/Valstar et al. - 2011 - The First Facial Expression Recognition and Analysis Challenge.pdf:pdf},
isbn = {978-1-4244-9140-7},
month = {mar},
pages = {921--926},
publisher = {IEEE},
title = {{The First Facial Expression Recognition and Analysis Challenge}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5771374},
year = {2011}
}
@article{VandenHoven2013,
abstract = {Understanding tangible interaction's foundational concepts can lead to systems with direct, integrated, and meaningful data control and representation.},
author = {van den Hoven, E and van de Garde-Perik, E and Offermans, S and van Boerdonk, K and Lenssen, K},
doi = {10.1109/MC.2012.360},
file = {:home/abetan16/Dropbox/Articles/Mendeley/van den Hoven et al. - 2013 - Moving Tangible Interaction Systems to the Next Level.pdf:pdf},
issn = {0018-9162},
journal = {Computer},
month = {aug},
number = {8},
pages = {70--76},
title = {{Moving Tangible Interaction Systems to the Next Level}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6336690},
volume = {46},
year = {2013}
}
@article{Starner2013,
address = {New York, New York, USA},
author = {van Laerhoven, Kristof and Roggen, Daniel and Gatica-Perez, Daniel and Fukumoto, Masaaki and Starner, Thad},
doi = {10.1145/2493988.2501097},
file = {:home/abetan16/Dropbox/MendeleyV3/van Laerhoven et al. - 2013 - Wearable computing.pdf:pdf},
isbn = {9781450321273},
journal = {the 17Th Annual International Symposium},
number = {2},
pages = {125},
publisher = {ACM Press},
title = {{Wearable computing}},
url = {http://dl.acm.org/citation.cfm?doid=2493988.2501097},
volume = {12},
year = {2013}
}
@article{Vanbellingen2011,
abstract = {Apraxia in patients with stroke may be overlooked, as clumsiness and deficient gestural communication are often attributed to frequently coexisting sensorimotor deficits and aphasia. Early and reliable detection of apraxia by a bedside test is relevant for functional outcome in patients with stroke. The present study was aimed at constructing a new bedside screening test for apraxia, called the Apraxia Screen of TULIA (AST), based on the comprehensive standardised Test for Upper-Limb Apraxia (TULIA).},
author = {Vanbellingen, T and Kersten, B and {Van de Winckel}, a and Bellion, M and Baronti, F and M{\"{u}}ri, R and Bohlhalter, S},
doi = {10.1136/jnnp.2010.213371},
file = {:home/abetan16/Dropbox/MendeleyV3/Vanbellingen et al. - 2011 - A new bedside test of gestures in stroke the apraxia screen of TULIA (AST).pdf:pdf},
isbn = {1468-330X (Electronic)$\backslash$n0022-3050 (Linking)},
issn = {0022-3050},
journal = {Journal of neurology, neurosurgery, and psychiatry},
number = {4},
pages = {389--392},
pmid = {20935324},
title = {{A new bedside test of gestures in stroke: the apraxia screen of TULIA (AST).}},
volume = {82},
year = {2011}
}
@article{Varini,
author = {Varini, Patrizia and Serra, Giuseppe and Cucchiara, Rita},
file = {:home/abetan16/Dropbox/MendeleyV3/Varini, Serra, Cucchiara - Unknown - Egocentric Video Personalization in Cultural Experiences Scenarios.pdf:pdf},
keywords = {cultural heritage,video analysis,video personalization},
title = {{Egocentric Video Personalization in Cultural Experiences Scenarios}}
}
@article{Verhulst1845,
author = {Verhulst, Pierre-Fran{\c{c}}ois},
file = {:home/abetan16/Dropbox/MendeleyV3/Verhulst - 1845 - Recherches math{\'{e}}matiques sur la loi d'accroissement de la population.pdf:pdf},
journal = {Nouveaux M{\'{e}}moires de l'Acad{\'{e}}mie Royale des Sciences et Belles-Lettres de Bruxelles},
pages = {1--42},
title = {{Recherches math{\'{e}}matiques sur la loi d'accroissement de la population.}},
volume = {18},
year = {1845}
}
@inproceedings{Vicente2000,
address = {North Falmouth, MA},
author = {Vicente, A and Pain, H},
booktitle = {AAAI Fall Symposium: Building Dialogue Systems for Tutorial Applications},
file = {:home/abetan16/Dropbox/Articles/Mendeley/Vicente, Pain - 2000 - A Computational Model of Affective Computational Dialogue.pdf:pdf},
publisher = {AAAI Press},
title = {{A Computational Model of Affective Computational Dialogue}},
url = {http://www.aaai.org/Papers/Symposia/Fall/2000/FS-00-01/FS00-01-016.pdf},
year = {2000}
}
@inproceedings{Vijayanarasimhan2008,
abstract = {Conventional supervised methods for image categorization rely on manually annotated (labeled) examples to learn good object models, which means their generality and scalability depends heavily on the amount of human effort available to help train them. We propose an unsupervised approach to construct discriminative models for categories specified simply by their names. We show that multiple-instance learning enables the recovery of robust category models from images returned by keyword-based search engines. By incorporating constraints that reflect the expected sparsity of true positive examples into a large-margin objective function, our approach remains accurate even when the available text annotations are imperfect and ambiguous. In addition, we show how to iteratively improve the learned classifier by automatically refining the representation of the ambiguously labeled examples. We demonstrate our method with benchmark datasets, and show that it performs well relative to both state-of-the-art unsupervised approaches and traditional fully supervised techniques.},
address = {Anchorage, AK},
author = {Vijayanarasimhan, S and Grauman, K},
booktitle = {Computer Vision and Pattern Recognition},
file = {:home/abetan16/Dropbox/MendeleyV3/Vijayanarasimhan, Grauman - 2008 - Keywords to Visual Categories Multiple-instance Learning Forweakly Supervised Object Categorization.pdf:pdf},
keywords = {department of computer sciences,grauman,multiple-instance learning for weakly,sudheendra vijayanarasimhan and kristen,supervised object categorization,to visual categories},
number = {June},
pages = {1 -- 8},
publisher = {IEEE},
title = {{Keywords to Visual Categories: Multiple-instance Learning Forweakly Supervised Object Categorization}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4587632},
year = {2008}
}
@article{Vincze2009,
abstract = {Interpretation of human activity is primarily known from surveillance and video analysis tasks and concerned with the persons alone. In this paper we present an integrated system that gives a natural language interpretation of activities where a person handles objects. The system integrates low-level image components such as hand and object tracking, detection and recognition, with high-level processes such as spatio-temporal object relationship generation, posture and gesture recognition, and activity reasoning. A task-oriented approach focuses processing to achieve near real-time and to react depending on the situation context. ?? 2008 Elsevier Inc. All rights reserved.},
author = {Vincze, Markus and Zillich, Michael and Ponweiser, Wolfgang and Hlavac, Vaclav and Matas, Jiri and Obdrzalek, Stepan and Buxton, Hilary and Howell, Jonathan and Sage, Kingsley and Argyros, Antonis and Eberst, Christoph and Umgeher, Gerald},
doi = {10.1016/j.cviu.2008.10.008},
file = {:home/abetan16/Dropbox/MendeleyV3/Vincze et al. - 2009 - Integrated vision system for the semantic interpretation of activities where a person handles objects.pdf:pdf},
isbn = {1077-3142},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {Activity interpretation,Cognitive vision,Semantic interpretation,System integration,Task orientation},
number = {6},
pages = {682--692},
publisher = {Elsevier Inc.},
title = {{Integrated vision system for the semantic interpretation of activities where a person handles objects}},
url = {http://dx.doi.org/10.1016/j.cviu.2008.10.008},
volume = {113},
year = {2009}
}
@inproceedings{Viola2001a,
abstract = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the "integral image" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a "cascade" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
author = {Viola, P and Jones, M},
booktitle = {Computer Vision and Pattern Recognition},
file = {:home/abetan16/Dropbox/MendeleyV3/Viola, Jones - 2001 - Rapid Object Detection Using a Boosted Cascade of Simple Features.pdf:pdf},
isbn = {0769512720},
pages = {511--518},
publisher = {IEEE},
title = {{Rapid Object Detection Using a Boosted Cascade of Simple Features}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=990517},
year = {2001}
}
@article{Vondrick2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.05461v1},
author = {Vondrick, Carl Carl Martin},
eprint = {arXiv:1502.05461v1},
file = {:home/abetan16/Dropbox/MendeleyV3/Vondrick - 2013 - Visualizing object detection features.pdf:pdf},
title = {{Visualizing object detection features}},
year = {2013}
}
@inproceedings{Vondrick2013a,
abstract = {We introduce algorithms to visualize feature spaces used by object detectors. The tools in this paper allow a human to put on ‘HOG goggles' and perceive the visual world as a HOG based object detector sees it. We found that these visualizations allow us to analyze object detection systems in new ways and gain new insight into the detector's fail- ures. For example, when we visualize the features for high scoring false alarms, we discovered that, although they are clearly wrong in image space, they do look deceptively sim- ilar to true positives in feature space. This result suggests that many of these false alarms are caused by our choice of feature space, and indicates that creating a better learning algorithm or building bigger datasets is unlikely to correct these errors. By visualizing feature spaces, we can gain a more intuitive understanding of our detection systems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1502.05461v1},
author = {Vondrick, Carl and Khosla, Aditya and Malisiewicz, Tomasz and Torralba, Antonio},
booktitle = {International Conference on Computer Vision},
doi = {10.1109/ICCV.2013.8},
eprint = {arXiv:1502.05461v1},
file = {:home/abetan16/Dropbox/MendeleyV3/Vondrick et al. - 2013 - HOGgles Visualizing object detection features.pdf:pdf},
isbn = {9781479928392},
issn = {1550-5499},
keywords = {hog,hoggles,object detection,visualization},
pages = {1--8},
title = {{HOGgles: Visualizing object detection features}},
year = {2013}
}
@inproceedings{Vyzas1999,
abstract = {We develop a method for offline and online recognition of the emotional state of a person deliberately expressing one of eight emotions. In terms of offline recognition, this paper presents recent improvements to a method previously developed in the MIT Media Lab, which involved recognition using physiological data collected from an actress over many weeks. The improvements involve (1) more robust handling of day-to-day variations in the data, (2) use of longer episodes of data, (3) use of...},
address = {Seattle; Wa.},
author = {Vyzas, E and Picard, R},
booktitle = {Workshop on Emotion-Based Agent Architectures, Third International Conference on Autonomous Agents},
file = {:home/abetan16/Dropbox/MendeleyV3/Vyzas, Picard - 1999 - Offline and Online Recognition of Emotion Expression from Physiological Data.pdf:pdf},
number = {488},
pages = {135--142},
title = {{Offline and Online Recognition of Emotion Expression from Physiological Data}},
year = {1999}
}
@inproceedings{Wake2015,
author = {Wake, Naoki and Sano, Yuko and Oya, Reishi and Sumitani, Masahiko and Kumagaya, Shin-ichiro},
booktitle = {Conference on Neural Engineering (NER)},
file = {:home/abetan16/Dropbox/MendeleyV3/Wake et al. - 2015 - Multimodal Virtual Reality Platform for the Rehabilitation of Phantom Limb Pain.pdf:pdf},
isbn = {9781467363891},
keywords = {Motor learning,Neural interfaces - Sensors and body interfaces,Neuromuscular systems - Neurorehabilitation,and neuromuscular,neural control},
pages = {22--24},
title = {{Multimodal Virtual Reality Platform for the Rehabilitation of Phantom Limb Pain *}},
year = {2015}
}
@inproceedings{Walk2010,
abstract = {Despite impressive progress in people detection the per- formance on challenging datasets like Caltech Pedestrians or TUD-Brussels is still unsatisfactory. In this work we show that motion features derived from optic flow yield sub- stantial improvements on image sequences, if implemented correctly—even in the case of low-quality video and conse- quently degraded flow fields. Furthermore, we introduce a new feature, self-similarity on color channels, which con- sistently improves detection performance both for static im- ages and for video sequences, across different datasets. In combination with HOG, these two features outperform the state-of-the-art by up to 20{\%}. Finally, we report two insights concerning detector evaluations, which apply to classifier-based object detection in general. First, we show that a commonly under-estimated detail of training, the number of bootstrapping rounds, has a drastic influence on the relative (and absolute) performance of different fea- ture/classifier combinations. Second, we discuss important intricacies of detector evaluation and show that current benchmarking protocols lack crucial details, which can dis- tort},
author = {Walk, S and Majer, N and Schindler, K and Schiele, B},
booktitle = {Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2010.5540102},
file = {:home/abetan16/Dropbox/MendeleyV3/Walk et al. - 2010 - New Features and Insights for Pedestrian Detection.pdf:pdf},
isbn = {978-1-4244-6984-0},
month = {jun},
pages = {1030--1037},
publisher = {Ieee},
title = {{New Features and Insights for Pedestrian Detection}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5540102},
year = {2010}
}
@article{Walther2006,
abstract = {Selective visual attention is believed to be responsible for serializing visual information for recognizing one object at a time in a complex scene. But how can we attend to objects before they are recognized? In coherence theory of visual cognition, so-called proto-objects form volatile units of visual information that can be accessed by selective attention and subsequently validated as actual objects. We propose a biologically plausible model of forming and attending to proto-objects in natural scenes. We demonstrate that the suggested model can enable a model of object recognition in cortex to expand from recognizing individual objects in isolation to sequentially recognizing all objects in a more complex scene.},
author = {Walther, D and Koch, C},
doi = {10.1016/j.neunet.2006.10.001},
file = {:home/abetan16/Dropbox/MendeleyV3/Walther, Koch - 2006 - Modeling Attention to Salient Proto-objects.pdf:pdf},
issn = {0893-6080},
journal = {Neural Networks : the Official Journal of the International Neural Network Society},
keywords = {Attention,Biological,Computer Simulation,Discrimination Learning,Discrimination Learning: physiology,Feedback,Humans,Models,Neural Networks (Computer),Pattern Recognition,Photic Stimulation,Photic Stimulation: methods,ROC Curve,Visual,Visual: physiology},
month = {nov},
number = {9},
pages = {1395--407},
pmid = {17098563},
title = {{Modeling Attention to Salient Proto-objects}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17098563},
volume = {19},
year = {2006}
}
@inproceedings{Wang2013,
abstract = {Wearable cameras and displays, such as the Google Glass, are around the corner. This paper explores techniques that jointly leverage camera-enabled glasses and smartphones to recognize individuals in the visual surrounding. While face recognition would be one approach to this problem, we believe that it may not be always possible to see a person's face. Our technique is complementary to face recognition, and exploits the intuition that colors of clothes, decorations, and even human motion patterns, can together make up a “fingerprint”. When leveraged systematically, it may be feasible to recognize individuals with reasonable consistency. This paper reports on our attempts, with early results from a prototype built on Android Galaxy phones and PivotHead's camera-enabled glasses. We call our system InSight.},
address = {New York, NY, USA},
author = {Wang, H and Bao, X},
booktitle = {Workshop on Mobile Computing Systems and Applications},
file = {:home/abetan16/Dropbox/MendeleyV3/Wang, Bao - 2013 - Insight Recognizing Humans Without Face Recognition.pdf:pdf},
isbn = {9781450314213},
pages = {2--7},
title = {{Insight: Recognizing Humans Without Face Recognition}},
url = {http://dl.acm.org/citation.cfm?id=2444786},
year = {2013}
}
@inproceedings{Wang2014,
annote = {Hand Detection and tracking
Viola Jones
adaboost

        

      },
author = {Wang, Jingtao and Yu, Chunxuan},
booktitle = {Chinese Control Conference},
doi = {10.1109/ChiCC.2014.6895774},
file = {:home/abetan16/Dropbox/MendeleyV3/Wang, Yu - 2014 - Finger-fist Detection in First-person View Based on Monocular Vision Using Haar-like Features.pdf:pdf},
isbn = {978-9-8815-6387-3},
keywords = {adaboosting,finger-fist,first-person view,haar-like features,hci},
month = {jul},
pages = {4920--4923},
publisher = {Ieee},
title = {{Finger-fist Detection in First-person View Based on Monocular Vision Using Haar-like Features}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6895774},
year = {2014}
}
@article{Wang2011,
abstract = {The advances of wearable sensors and wireless networks offer many opportunities to recognize human activities from sensor readings in pervasive computing. Existing work so far focuses mainly on recognizing activities of a single user in a home environment. However, there are typically multiple inhabitants in a real home and they often perform activities together. In this paper, we investigate the problem of recognizing multi-user activities using wearable sensors in a home setting. We develop a multi-modal, wearable sensor platform to collect sensor data for multiple users, and study two temporal probabilistic modelsCoupled Hidden Markov Model (CHMM) and Factorial Conditional Random Field (FCRF)to model interacting processes in a sensor-based, multi-user scenario. We conduct a real-world trace collection done by two subjects over two weeks, and evaluate these two models through our experimental studies. Our experimental results show that we achieve an accuracy of 96.41{\%} with CHMM and an accuracy of 87.93{\%} with FCRF, respectively, for recognizing multi-user activities. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
author = {Wang, Liang and Gu, Tao and Tao, Xianping and Chen, Hanhua and Lu, Jian},
doi = {10.1016/j.pmcj.2010.11.008},
file = {:home/abetan16/Dropbox/MendeleyV3/Wang et al. - 2011 - Recognizing multi-user activities using wearable sensors in a smart home.pdf:pdf},
isbn = {15741192},
issn = {15741192},
journal = {Pervasive and Mobile Computing},
keywords = {Multi-user activity recognition,Probabilistic model,Sensor-based human activity recognition,Wireless sensor network},
number = {3},
pages = {287--298},
publisher = {Elsevier B.V.},
title = {{Recognizing multi-user activities using wearable sensors in a smart home}},
url = {http://dx.doi.org/10.1016/j.pmcj.2010.11.008},
volume = {7},
year = {2011}
}
@article{Wang2009,
abstract = {Articulated hand-tracking systems have been widely used in virtual reality but are rarely deployed in consumer applications due to their price and complexity. In this paper, we propose an easy-to-use and inexpensive system that facilitates 3-D articulated user-input using the hands. Our approach uses a single camera to track a hand wearing an ordinary cloth glove that is imprinted with a custom pattern. The pattern is designed to simplify the pose estimation problem, allowing us to employ a nearest-neighbor approach to track hands at interactive rates. We describe several proof-of-concept applications enabled by our system that we hope will provide a foundation for new interactions in modeling, animation control and augmented reality.},
author = {Wang, R and Popovi{\'{c}}, J},
doi = {10.1145/1531326.1531369},
file = {:home/abetan16/Dropbox/MendeleyV3/Wang, Popovi{\'{c}} - 2009 - Real-time Hand-tracking with a Color Glove.pdf:pdf},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {augmented reality,hand tracking,motion capture,user},
month = {jul},
number = {3},
pages = {1},
title = {{Real-time Hand-tracking with a Color Glove}},
url = {http://portal.acm.org/citation.cfm?doid=1531326.1531369},
volume = {28},
year = {2009}
}
@incollection{Bisio2014,
abstract = {Human-machine interaction is performed by devices such as the keyboard, the touch-screen, or speech-to-text applications. For example, a speech-to-text application is software that allows the device to translate the spoken words into text. These tools translate explicit messages but ignore implicit messages, such as the emotional status of the speaker, filtering out a portion of information available in the interaction process. This chapter focuses on emotion detection. An emotion-aware device can also interact more personally with its owner and react appropriately according to the user's mood, making the user-machine interaction less stressful. The chapter gives the guidelines for building emotion-aware smartphone applications in an opportunistic way (i.e., without the user's collaboration). In general, smartphone applications might be employed in different contexts; therefore, the to-be-detected emotions might be different.},
address = {Hershey, PA},
author = {Wang, Zhu and Guo, Bin},
booktitle = {Creating Personal, Social and Urban Awarness through Pervasive Computing},
doi = {10.4018/978-1-4666-4695-7.ch001},
editor = {Guo, Bin and Riboni, Daniele and Hu, Peizhao},
isbn = {9781466646957},
pages = {158--160},
publisher = {IGI Global},
title = {{Creating Personal, Social, and Urban Awareness through Pervasive Computing}},
url = {http://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/978-1-4666-4695-7},
year = {2013}
}
@misc{Welch1995,
abstract = {In 1960, R.E. Kalman published his famous paper describing a recursive solution to the discrete-data linear filtering problem. Since that time, due in large part to ad- vances in digital computing, the Kalman filter has been the subject of extensive re- search and application, particularly in the area of autonomous or assisted navigation. The Kalman filter is a set of mathematical equations that provides an efficient com- putational (recursive) means to estimate the state of a process, in a way that mini- mizes the mean of the squared error. The filter is very powerful in several aspects: it supports estimations of past, present, and even future states, and it can do so even when the precise nature of the modeled system is unknown. The purpose of this paper is to provide a practical introduction to the discrete Kal- man filter. This introduction includes a description and some discussion of the basic discrete Kalman filter, a derivation, description and some discussion of the extend- ed Kalman filter, and a relatively simple (tangible) example with real numbers {\&} results.},
author = {Welch, Greg and Bishop, Gary},
file = {:home/abetan16/Dropbox/MendeleyV3/Welch, Bishop - 1995 - An introduction to the Kalman filter.pdf:pdf},
pages = {1--16},
title = {{An introduction to the Kalman filter}},
url = {http://clubs.ens-cachan.fr/krobot/old/data/positionnement/kalman.pdf},
year = {1995}
}
@article{Wolf2014,
author = {Wolf, Katrin and Schmidt, Albrecht and Bexheti, Agon and Langheinrich, Marc},
doi = {10.1109/MPRV.2014.53},
file = {:home/abetan16/Dropbox/MendeleyV3/Wolf et al. - 2014 - Lifelogging You're Wearing a Camera.pdf:pdf},
issn = {1536-1268},
journal = {IEEE Pervasive Computing},
number = {3},
pages = {8--12},
title = {{Lifelogging: You're Wearing a Camera?}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6850244},
volume = {13},
year = {2014}
}
@inproceedings{Wolf1996,
abstract = {This paper describes a new algorithm for identifying key frames in shots from video programs. We use optical flow computations to identify local minima of motion in a shot-stillness emphasizes the image for the viewer. This technique allows us to identify both gestures which are emphasized by momentary pauses and camera motion which links together several distinct images in a single shot. Results show that our algorithm can successfully select several key frames from a single complex shot which effectively summarize the shot},
address = {Atlanta, GA},
author = {Wolf, W},
booktitle = {Acoustics, Speech, and Signal Processing, 1996. {\ldots}},
file = {:home/abetan16/Dropbox/MendeleyV3/Wolf - 1996 - Key Frame Selection by Motion Analysis.pdf:pdf},
isbn = {0780331923},
pages = {1228--1231},
publisher = {IEEE},
title = {{Key Frame Selection by Motion Analysis}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=543588},
year = {1996}
}
@inproceedings{Wu2007,
abstract = {We propose an approach to activity recognition based on detecting and analyzing the sequence of objects that are being manipulated by the user. In domains such as cooking, where many activities involve similar actions, object-use information can be a valuable cue. In order for this approach to scale to many activities and objects, however, it is necessary to minimize the amount of human-labeled data that is required for modeling. We describe a method for automatically acquiring object models from video without any explicit human supervision. Our approach leverages sparse and noisy readings from RFID tagged objects, along with common-sense knowledge about which objects are likely to be used during a given activity, to bootstrap the learning process. We present a dynamic Bayesian network model which combines RFID and video data to jointly infer the most likely activity and object labels. We demonstrate that our approach can achieve activity recognition rates of more than 80{\%} on a real-world dataset consisting of 16 household activities involving 33 objects with significant background clutter. We show that the combination of visual object recognition with RFID data is significantly more effective than the RFID sensor alone. Our work demonstrates that it is possible to automatically learn object models from video of household activities and employ these models for activity recognition, without requiring any explicit human labeling.},
address = {Rio de Janeiro},
author = {Wu, J and Osuntogun, A},
booktitle = {Internantional Conference on Computer Vision},
doi = {10.1109/ICCV.2007.4408865},
file = {:home/abetan16/Dropbox/MendeleyV3/Wu, Osuntogun - 2007 - A Scalable Approach to Activity Recognition Based on Object Use.pdf:pdf},
isbn = {978-1-4244-1630-1},
pages = {1--8},
publisher = {IEEE},
title = {{A Scalable Approach to Activity Recognition Based on Object Use}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4408865 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4408865},
year = {2007}
}
@inproceedings{Wyatt2005,
abstract = {A fundamental difficulty in recognizing human activities is obtaining the labeled data needed to learn models of those activities. Given emerging sensor technology, however, it is possible to view activity data as a stream of natural language terms. Activity models are then mappings from such terms to activity names, and may be extracted from text corpora such as the web. We show that models so extracted are sufficient to automatically produce labeled segmentations of activity data with an accuracy of 42{\%} over 26 activities, well above the 3.89{\%} baseline. The segmentation so obtained is sufficient to bootstrap learning, with accuracy of learned models increasing to 52{\%}. To our knowledge, this is the first human activity inferencing system shown to learn from sensed activity data with no human intervention per activity learned, even for labeling.},
author = {Wyatt, D and Philipose, M and Choudhury, T},
booktitle = {National Conference on Artificial Intelligence},
file = {:home/abetan16/Dropbox/Articles/Mendeley/Wyatt, Philipose, Choudhury - 2005 - Unsupervised activity recognition using automatically mined common sense.pdf:pdf},
pages = {21--27},
title = {{Unsupervised Activity Recognition Using Automatically Mined Common Sense}},
url = {http://www.aaai.org/Papers/AAAI/2005/AAAI05-004.pdf},
year = {2005}
}
@inproceedings{Xiong2014,
abstract = {Wearable cameras capture a first-person view of the world, and offer a hands-free way to record daily experiences or special events. Yet, not every frame is worthy of being captured and stored.We propose to automatically predict “snap points” in unedited egocentric video— that is, those frames that look like they could have been intentionally taken photos. We develop a generative model for snap points that relies on aWeb photo prior together with domain-adapted features. Critically, our approach avoids strong assumptions about the particular content of snap points, focusing instead on their composition. Using 17 hours of egocentric video from both human and mobile robot camera wearers, we show that the approach accurately isolates those frames that human judges would believe to be intentionally snapped photos. In addition, we demonstrate the utility of snap point detection for improving object detection and keyframe selection in egocentric video.},
annote = {Video summarization and retrieval
Object detection
Web photo dataset
Blur
Directions Histogram orientations
GIST
HOG
SSIM (Self Similarity)
SIFT
PCA (90{\%})
GFK

        

        

        

        

        

        

        

      },
author = {Xiong, Bo and Grauman, Kristen},
booktitle = {Internantional Conference on Computer Vision},
file = {:home/abetan16/Dropbox/MendeleyV3/Xiong, Grauman - 2014 - Detecting Snap Points in Egocentric Video with a Web Photo Prior.pdf:pdf},
title = {{Detecting Snap Points in Egocentric Video with a Web Photo Prior}},
url = {http://link.springer.com/chapter/10.1007/978-3-319-10602-1{\_}19},
year = {2014}
}
@misc{Xu2012,
author = {Xu, Dan and Chen, Yen-Lun and Lin, Chuan and Kong, Xin and Wu, Xinyu},
booktitle = {2012 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
doi = {10.1109/ROBIO.2012.6491047},
file = {:home/abetan16/Dropbox/MendeleyV3/Xu et al. - 2012 - Real-time dynamic gesture recognition system based on depth perception for robot navigation.pdf:pdf},
isbn = {978-1-4673-2127-3},
month = {dec},
pages = {689--694},
publisher = {Ieee},
title = {{Real-time dynamic gesture recognition system based on depth perception for robot navigation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6491047},
year = {2012}
}
@inproceedings{Xuan2007,
abstract = {We show how to apply the efficient Bayesian changepoint detection techniques of Fearnhead in the multivariate setting. We model the joint density of vector-valued observations using undirected Gaussian graphical models, whose structure we estimate. We show how we can exactly compute the MAP segmentation, as well as how to draw perfect samples from the posterior over segmentations, simultaneously accounting for uncertainty about the number and location of changepoints, as well as uncertainty about the covariance structure. We illustrate the technique by applying it to financial data and to bee tracking data.},
author = {Xuan, Xiang and Murphy, Kevin},
booktitle = {Conference on Machine learning},
file = {:home/abetan16/Dropbox/MendeleyV3/Xuan, Murphy - 2007 - Modeling Changing Dependency Structure in Multivariate Time Series.pdf:pdf},
pages = {1055--1062},
title = {{Modeling Changing Dependency Structure in Multivariate Time Series}},
url = {http://dl.acm.org/citation.cfm?id=1273629},
year = {2007}
}
@inproceedings{Yamada2011a,
abstract = {The validity of using conventional saliency map models to predict human attention was investigated for video captured with an egocentric camera. Since conventional visual saliency models do not take into account visual motion caused by camera motion, high visual saliency may be erroneously assigned to regions that are not actually visually salient. To evaluate the validity of using saliency map models for ego- centric vision, an experiment was carried out to examine the correlation between visual saliency maps and measured gaze points for egocentric vision. The results show that conventional saliency map models can pre- dict visually salient regions better than chance for egocentric vision and that the accuracy decreases significantly with an increase in visual mo- tion induced by egomotion, which is presumably compensated for in the human visual system. This latter finding indicates that a visual saliency model is needed that can better predict human visual attention from egocentric videos. 1},
author = {Yamada, K and Sugano, Y and Okabe, T},
booktitle = {Internantional Conference on Computer Vision},
file = {:home/abetan16/Dropbox/MendeleyV3/Yamada, Sugano, Okabe - 2011 - Can Saliency Map Models Predict Human Egocentric Visual Attention.pdf:pdf},
pages = {1--10},
title = {{Can Saliency Map Models Predict Human Egocentric Visual Attention?}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-22822-3{\_}42},
year = {2011}
}
@inproceedings{Yamada2012,
abstract = {e propose a method of predicting human egocentric visual attention using bottom-up visual saliency and egomotion information. Computational mod- els of visual saliency are often employed to predict human attention; however, its mechanism and effectiveness have not been fully explored in egocentric vision. The purpose of our framework is to compute attention maps from an egocentric video that can be used to infer a person's visual attention. In addition to a stan- dard visual saliency model, two kinds of attention maps are computed based on a camera's rotation velocity and direction of movement. These rotation-based and translation-based attention maps are aggregated with a bottom-up saliency map to enhance the accuracy with which the person's gaze positions can be predicted. The efficiency of the proposed framework was examined in real environments by using a head-mounted gaze tracker, and we found that the egomotion-based attention maps contributed to accurately predicting human visual attention.},
author = {Yamada, K and Sugano, Y and Okabe, T and Sato, Y and Sugimoto, A and Hiraki, K},
booktitle = {Pacific Rim Conference on Advances in Image and Video Technology},
doi = {10.1007/978-3-642-25367-6_25},
file = {:home/abetan16/Dropbox/MendeleyV3//Yamada et al. - 2012 - Attention Prediction in Egocentric Video Using Motion and Visual Saliency.pdf:pdf},
keywords = {camera motion,first-person vision,visual attention,visual saliency},
pages = {277--288},
title = {{Attention Prediction in Egocentric Video Using Motion and Visual Saliency}},
url = {http://dx.doi.org/10.1007/978-3-642-25367-6{\_}25},
year = {2012}
}
@incollection{Yan2006,
abstract = {We cast the problem of motion segmentation of feature trajectories as linear manifold finding problems and propose a general framework for motion segmentation under affine projections which utilizes two properties of trajectory data: geometric constraint and locality. The geometric constraint states that the trajectories of the same motion lie in a low dimensional linear manifold and different motions result in different linear manifolds; locality, by which we mean in a transformed space a data and its neighbors tend to lie in the same linear manifold, provides a cue for efficient estimation of these manifolds. Our algorithm estimates a number of linear manifolds, whose dimensions are unknown beforehand, and segment the trajectories accordingly. It first transforms and normalizes the trajectories; secondly, for each trajectory it estimates a local linear manifold through local sampling; then it derives the affinity matrix based on principal subspace angles between these estimated linear manifolds; at last, spectral clustering is applied to the matrix and gives the segmentation result. Our algorithm is general without restriction on the number of linear manifolds and without prior knowledge of the dimensions of the linear manifolds. We demonstrate in our experiments that it can segment a wide range of motions including independent, articulated, rigid, non-rigid, degenerate, non-degenerate or any combination of them. In some highly challenging cases where other state-of-the-art motion segmentation algorithms may fail, our algorithm gives expected results.},
author = {Yan, J and Pollefeys, M},
booktitle = {Computer Vision – ECCV 2006},
editor = {Leonardis, Ales and Bischof, Horst and Pinz, Axel},
file = {:home/abetan16/Dropbox/MendeleyV3/Yan, Pollefeys - 2006 - A General Framework for Motion Segmentation Degenerate and Non-degenerate.pdf:pdf},
pages = {94--106},
publisher = {Springer Berlin Heidelberg},
title = {{A General Framework for Motion Segmentation : Degenerate and Non-degenerate}},
year = {2006}
}
@article{Yan2015,
author = {Yan, Yan and Ricci, Elisa and Liu, Gaowen and Sebe, Nicu and Member, Senior},
doi = {10.1109/TIP.2015.2438540},
file = {:home/abetan16/Dropbox/MendeleyV3/Yan et al. - 2015 - Egocentric Daily Activity Recognition via Multitask Clustering.pdf:pdf},
issn = {1057-7149},
journal = {Image Processing, IEEE Transactions on},
number = {10},
pages = {2984--2995},
title = {{Egocentric Daily Activity Recognition via Multitask Clustering}},
volume = {24},
year = {2015}
}
@inproceedings{Yang2010,
abstract = {Skin color detection is an important subject in computer vision research. Color segmentation takes a great attention because color is an effective and robust visual cue for characterizing an object from the others. To aim at existing skin color algorithms considering the luminance information not enough, a reliable color modeling approach was proposed. It is based on the fact that color distribution of a single-colored object is not invariant with respect to luminance variations even in the Cb-Cr plane and does not ignore the influence on luminance Y component in YCbCr color space. Firstly, according to statistics of skin color pixels, we take the luminance Y by ascending order, divide the total range of Y into finite number of intervals, collect pixels whose luminance belongs to the same luminance interval, calculate the covariance and the mean value of Cb and Cr with respect to Y, and use the above data to train the BP neural network, then we get the self-adaptive skin color model and design a Gaussian model classifier. The experimental results have indicated that this algorithm can effectively fulfill the skin-color detection for images captured under different environmental condition and the performance of the skin color segmentation is significantly improved.},
author = {Yang, G and Li, H and Zhang, L and Cao, Y},
booktitle = {International Conference on Communications and Intelligence Information Security},
doi = {10.1109/ICCIIS.2010.67},
file = {:home/abetan16/Dropbox/MendeleyV3/Yang et al. - 2010 - Research on a Skin Color Detection Algorithm Based on Self-adaptive Skin Color Model.pdf:pdf},
isbn = {978-1-4244-8649-6},
keywords = {-skin color detection,electrical engineering,gaussian,huan li,li zhang,model,school of mechanical,skin color model,skin color segmentation,yue cao},
month = {oct},
pages = {266--270},
publisher = {Ieee},
title = {{Research on a Skin Color Detection Algorithm Based on Self-adaptive Skin Color Model}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5629242},
year = {2010}
}
@article{Yang2011,
abstract = {The goal of this paper is to review the state-of-the-art progress on visual tracking methods, classify them into different categories, as well as identify future trends. Visual tracking is a fundamental task in many computer vision applications and has been well studied in the last decades. Although numerous approaches have been proposed, robust visual tracking remains a huge challenge. Difﬁculties in visual tracking can arise due to abrupt object motion, appearance pattern change, non-rigid object structures, occlusion and camera motion. In this paper, we ﬁrst analyze the state-of-the-art feature descriptors which are used to represent the appearance of tracked objects. Then, we categorize the tracking progresses into three groups, provide detailed descriptions of representative methods in each group, and examine their positive and negative aspects. At last, we outline the future trends for visual tracking research.},
author = {Yang, H and Shao, L and Zheng, F and Wang, L and Song, Z},
doi = {10.1016/j.neucom.2011.07.024},
file = {:home/abetan16/Dropbox/MendeleyV3/Yang et al. - 2011 - Recent Advances and Trends in Visual Tracking a Review.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
month = {nov},
number = {18},
pages = {3823--3831},
title = {{Recent Advances and Trends in Visual Tracking: a Review}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231211004668},
volume = {74},
year = {2011}
}
@book{Yarbus1967,
address = {New York, New York, USA},
author = {Yarbus, A},
file = {:home/abetan16/Dropbox/Articles/Mendeley/Yarbus, Haigh, Rigss - 1967 - Eye Movements and Vision.pdf:pdf},
publisher = {Plenum Press},
title = {{Eye Movements and Vision}},
url = {http://library.wur.nl/WebQuery/clc/340878},
year = {1967}
}
@article{Yi2009,
abstract = {Modeling human behavior is important for the design of robots as well as human-computer interfaces that use humanoid avatars. Constructive models have been built, but they have not captured all of the detailed structure of human behavior such as the moment-to-moment deployment and coordination of hand, head and eye gaze used in complex tasks. We show how this data from human subjects performing a task can be used to program a dynamic Bayes network (DBN) which in turn can be used to recognize new performance instances. As a specific demonstration we show that the steps in a complex activity such as sandwich making can be recognized by a DBN in real time.},
author = {Yi, W and Ballard, D},
doi = {10.1142/S0219843609001863.RECOGNIZING},
file = {:home/abetan16/Dropbox/MendeleyV3/Yi, Ballard - 2009 - Recognizing Behavior in Hand-eye Coordination Patterns.pdf:pdf},
journal = {International Journal of Humanoid Robotics},
keywords = {dynamic bayesian networks,humanoid avatars,markov models},
number = {3},
pages = {337--359},
title = {{Recognizing Behavior in Hand-eye Coordination Patterns}},
url = {http://www.worldscientific.com/doi/pdf/10.1142/S0219843609001863},
volume = {6},
year = {2009}
}
@article{Yin2009,
author = {Yin, G. and Wang, Le Yi and Kan, Shaobai},
doi = {10.1016/j.automatica.2008.11.025},
file = {:home/abetan16/Dropbox/MendeleyV3/Yin, Wang, Kan - 2009 - Tracking and Identification of Regime-switching Systems Using Binary Sensors.pdf:pdf},
issn = {00051098},
journal = {Automatica},
month = {apr},
number = {4},
pages = {944--955},
publisher = {Elsevier Ltd},
title = {{Tracking and Identification of Regime-switching Systems Using Binary Sensors}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0005109808005669},
volume = {45},
year = {2009}
}
@inproceedings{Ghosh2012,
abstract = {We present a video summarization approach for egocen- tric or “wearable” camera data. Given hours of video, the proposed method produces a compact storyboard sum- mary of the camera wearer's day. In contrast to traditional keyframe selection techniques, the resulting summary fo- cuses on the most important objects and people with which the camera wearer interacts. To accomplish this, we de- velop region cues indicative of high-level saliency in ego- centric video—such as the nearness to hands, gaze, and frequency of occurrence—and learn a regressor to predict the relative importance of any new region based on these cues. Using these predictions and a simple form of tempo- ral event detection, our method selects frames for the sto- ryboard that reflect the key object-driven happenings. Crit- ically, the approach is neither camera-wearer-specific nor object-specific; that means the learned importance metric need not be trained for a given user or context, and it can predict the importance of objects and people that have never been seen previously. Our results with 17 hours of ego- centric data show the method's promise relative to existing techniques for saliency and summarization},
author = {{Yong Jae Lee} and Ghosh, J. and Grauman, K.},
booktitle = {2012 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2012.6247820},
file = {:home/abetan16/Dropbox/MendeleyV3/Yong Jae Lee, Ghosh, Grauman - 2012 - Discovering important people and objects for egocentric video summarization.pdf:pdf},
isbn = {978-1-4673-1228-8},
month = {jun},
pages = {1346--1353},
publisher = {IEEE},
title = {{Discovering important people and objects for egocentric video summarization}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6247820},
year = {2012}
}
@incollection{Yoo2005,
abstract = {In order to build a human-computer interface that is sensitive to a user's expressed emotion, we propose a neural network based emotion estimation algorithm using heart rate variability (HRV) and galvanic skin response (GSR). In this study, a video clip method was used to elicit basic emotions from subjects while electrocardiogram (ECG) and GSR signals were measured. These signals reflect the influence of emotion on the autonomic nervous system (ANS). The extracted features that are emotion-specific characteristics from those signals are applied to an artificial neural network in order to recognize emotions from new signal collections. Results show that the proposed method is able to accurately distinguish a user's emotion.},
address = {Changsha, China},
annote = {Preguntas
- Es posible no predefinir las emociones a priori, tal vez dejarlas definirse, variar y crecer deacuerdo a las experiencias del usuario.
- Podria SOM ayudar este tipo de emociones, entrenar SOM con experiencias y SOM cells con acciones o reacciones?
- Based on the valence-arousal plane, this plane could be changed by a SOM? Would be that SOM clustered in a,b,c,d groups? Where they are located?
        
Concluciones
- El articulo es bastante bago en la descripcion de la red neuronal
- El articulo solo tira una tabla como resultado y un grafico
- Muestra un ajuste del 80{\%} pero de que? si no explican nada de la red neuronal.},
author = {Yoo, S and Lee, C and Park, Y and Kim, N},
booktitle = {Advances in Natural Computation},
edition = {Part 1},
editor = {Wang, Lipo and Chen, Ke and Ong, Yew Soon},
file = {:home/abetan16/Dropbox/MendeleyV3/Yoo et al. - 2005 - Neural Network Based Emotion Estimation Using Heart Rate Variability and Skin Resistance.pdf:pdf},
number = {1},
pages = {818--824},
publisher = {Springerlink},
title = {{Neural Network Based Emotion Estimation Using Heart Rate Variability and Skin Resistance}},
url = {http://link.springer.com/chapter/10.1007/11539087{\_}110},
volume = {3610},
year = {2005}
}
@inproceedings{Yu2002,
author = {Yu, C and Ballard, Dana},
booktitle = {Development and Learning},
pages = {28--33},
title = {{Learning To Recognize Human Action Sequences}},
year = {2002}
}
@article{Zariffa2013,
author = {Zariffa, J and Popovic, MR},
file = {:home/abetan16/Dropbox/MendeleyV3/Zariffa, Popovic - 2013 - Hand Contour Detection in Wearable Camera Video Using an Adaptive Histogram Region of Interest.pdf:pdf},
journal = {Journal of NeuroEngineering and Rehabilitation},
keywords = {colour histogram,hand contour,hand function,neuro rehabilitation,wearable system},
number = {114},
pages = {1--10},
title = {{Hand Contour Detection in Wearable Camera Video Using an Adaptive Histogram Region of Interest}},
url = {http://www.biomedcentral.com/content/pdf/1743-0003-10-114.pdf},
volume = {10},
year = {2013}
}
@inproceedings{Zelnik-Manor2001,
abstract = {Dynamic events can be regarded as long-term temporal objects, which are characterized by spatio-temporal features at multiple temporal scales. Based on this, we design a simple statistical distance measure between video sequences (possibly of different lengths) based on their behavioral content. This measure is non-parametric and can thus handle a wide range of dynamic events. We use this measure for isolating and clustering events within long continuous video sequences. This is done without prior knowledge of the types of events, their models, or their temporal extent. An outcome of such a clustering process is a temporal segmentation of long video sequences into event-consistent sub-sequences, and their grouping into event-consistent clusters. Our event representation and associated distance measure can also be used for event-based indexing into long video sequences, even when only one short example-clip is available. However, when multiple example-clips of the same event are available (either as a result of the clustering process, or given manually), these can be used to refine the event representation, the associated distance measure, and accordingly the quality of the detection and clustering process.},
author = {Zelnik-Manor, L and Irani, M},
booktitle = {Computer Vision and Pattern Recognition},
file = {:home/abetan16/Dropbox/MendeleyV3/Zelnik-Manor, Irani - 2001 - Event-based Analysis of Video.pdf:pdf},
publisher = {IEEE},
title = {{Event-based Analysis of Video}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=990935},
year = {2001}
}
@article{Zeng2009,
abstract = {Automated analysis of human affective behavior has attracted increasing attention from researchers in psychology, computer science, linguistics, neuroscience, and related disciplines. However, the existing methods typically handle only deliberately displayed and exaggerated expressions of prototypical emotions despite the fact that deliberate behaviour differs in visual appearance, audio profile, and timing from spontaneously occurring behaviour. To address this problem, efforts to develop algorithms that can process naturally occurring human affective behaviour have recently emerged. Moreover, an increasing number of efforts are reported toward multimodal fusion for human affect analysis including audiovisual fusion, linguistic and paralinguistic fusion, and multi-cue visual fusion based on facial expressions, head movements, and body gestures. This paper introduces and surveys these recent advances. We first discuss human emotion perception from a psychological perspective. Next we examine available approaches to solving the problem of machine understanding of human affective behavior, and discuss important issues like the collection and availability of training and test data. We finally outline some of the scientific and engineering challenges to advancing human affect sensing technology.},
author = {Zeng, Z and Pantic, M and Roisman, G and Huang, T},
doi = {10.1109/TPAMI.2008.52},
file = {:home/abetan16/Dropbox/MendeleyV3/Zeng et al. - 2009 - A Survey of Affect Recognition Methods Audio, Visual, and Spontaneous Expressions.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Affect,Affect: physiology,Algorithms,Artificial Intelligence,Automated,Automated: methods,Emotions,Emotions: physiology,Facial Expression,Monitoring,Pattern Recognition,Physiologic,Physiologic: methods,Sound Spectrography,Sound Spectrography: methods},
month = {jan},
number = {1},
pages = {39--58},
pmid = {19029545},
title = {{A Survey of Affect Recognition Methods: Audio, Visual, and Spontaneous Expressions}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19029545},
volume = {31},
year = {2009}
}
@article{Zhan2015,
abstract = {We propose a novel pervasive system to recognise human daily activities from a wearable device. The system is designed in a form of reading glasses, named 'Smart Glasses', integrating a 3-axis accelerometer and a first-person view camera. Our aim is to classify subject's activities of daily living (ADLs) based on their vision and head motion data. This ego-activity recognition system not only allows caretakers to track on a specific person (such as disabled patient or elderly people), but also has the potential to remind/warn people with cognitive impairments of hazardous situations. We present the following contributions: a feature extraction method from accelerometer and video; a classification algorithm integrating both locomotive (body motions) and stationary activities (without or with small motions); a novel multi-scale dynamic graphical model for structured classification over time. In this paper, we collect, train and validate our system on two large datasets: 20 h of elder ADLs datasets and 40 h of patient ADLs datasets, containing 12 and 14 different activities separately. The results show that our method efficiently improves the system performance (F-Measure) over conventional classification approaches by an average of 20{\%}-40{\%} up to 84.45{\%}, with an overall accuracy of 90.04{\%} for elders. Furthermore, we also validate our method on 30 patients with different disabilities, achieving an overall accuracy up to 77.07{\%}.},
author = {Zhan, Kai and Faux, Steven and Ramos, Fabio},
doi = {10.1016/j.pmcj.2014.11.004},
file = {:home/abetan16/Dropbox/MendeleyV3/Zhan, Faux, Ramos - 2015 - Multi-scale Conditional Random Fields for first-person activity recognition on elders and disabled patients.pdf:pdf},
issn = {15741192},
journal = {Pervasive and Mobile Computing},
keywords = {Activity recognition,Computer vision,Feature classification,Feature extraction,First-person,Graphical model},
number = {PB},
pages = {251--267},
publisher = {Elsevier B.V.},
title = {{Multi-scale Conditional Random Fields for first-person activity recognition on elders and disabled patients}},
url = {http://dx.doi.org/10.1016/j.pmcj.2014.11.004},
volume = {16},
year = {2015}
}
@inproceedings{Zhan2014,
abstract = {We propose a novel pervasive system to recognise human daily activities from a wearable device. The system is designed in a form of reading glasses, named ‘Smart Glasses', integrating a 3-axis accelerometer and a first-person view cam- era. Our aim is to classify user's activities of daily living (ADLs) based on both vision and head motion data. This ego-activity recognition system not only allows caretakers to track on a specific person (such as patient or elderly people), but also has the potential to remind/warn people with cognitive impairments of hazardous situations. We present the following contributions in this paper: a feature extraction method from accelerometer and video; a classification algorithm integrating both locomotive (body motions) and stationary activities (without or with small motions); a novel multi-scale dynamic graphical model structure for structured classification over time. We collect, train and validate our system on a large dataset containing 20 hours of ADLs data, including 12 daily activities under different environmental settings. Our method improves the classification performance (F-Score) of conventional approaches from 43.32{\%}(video features) and 66.02{\%}(acceleration features) by an average of 20-40{\%} to 84.45{\%}, with an overall accuracy of},
author = {Zhan, Kai and Faux, Steven and Ramos, Fabio},
booktitle = {International Conference on Pervasive Computing and Communications},
doi = {10.1109/PerCom.2014.6813944},
file = {:home/abetan16/Dropbox/MendeleyV3/Zhan, Faux, Ramos - 2014 - Multi-scale Conditional Random Fields for first-person activity recognition.pdf:pdf},
isbn = {978-1-4799-3445-4},
month = {mar},
pages = {51--59},
publisher = {Ieee},
title = {{Multi-scale Conditional Random Fields for first-person activity recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6813944},
year = {2014}
}
@article{Zhang1997,
abstract = {This paper presents an integrated system solution for computer assisted video parsing and content-based video retrieval and browsing. The effectiveness of this solution lies in its use of video content information derived from a parsing process, being driven by visual feature analysis. That is, parsing will temporally segment and abstract a video source, based on low-level image analyses; then retrieval and browsing of video will be based on key-frame, temporal and motion features of shots. These processes and a set of tools to facilitate content-based video retrieval and browsing using the feature data set are presented in detail as functions of an integrated system.},
author = {Zhang, H and Wu, J and Zhong, D and Smoliar, S},
file = {:home/abetan16/Dropbox/Articles/Mendeley/Zhang et al. - 1997 - An Integrated System for Content-based Video Retrieval and Browsing.pdf:pdf},
journal = {Pattern recognition},
number = {4},
pages = {643--658},
title = {{An Integrated System for Content-based Video Retrieval and Browsing}},
url = {http://www.sciencedirect.com/science/article/pii/S0031320396001094},
volume = {30},
year = {1997}
}
@article{Zhang2005,
abstract = {Recently, methods based on local image features have shown promise for texture and object recognition tasks. This paper presents a large-scale evaluation of an approach that represents images as distributions (signatures or histograms) of features extracted from a sparse set of keypoint locations and learns a Support Vector Machine classifier with kernels based on two effective measures for comparing distributions, the Earth Mover's Distance and the ÷2 distance. We first evaluate the performance of our approach with different keypoint detectors and descriptors, as well as different kernels and classifiers. We then conduct a comparative evaluation with several state-of-the-art recognition methods on 4 texture and 5 object databases. On most of these databases, our implementation exceeds the best reported results and achieves comparable performance on the rest. Finally, we investigate the influence of background correlations on recognition performance.},
author = {Zhang, J and Marsza{\l}ek, M and Lazebnik, S and Schmid, C},
doi = {10.1007/s11263-006-9794-4},
file = {:home/abetan16/Dropbox/Articles/Mendeley/Zhang et al. - 2005 - Local features and kernels for classification of texture and object categories An in-depth study.pdf:pdf},
isbn = {0769526462},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
month = {sep},
number = {2},
pages = {213--238},
publisher = {IEEE},
title = {{Local Features and Kernels for Classification of Texture and Object Categories: A Comprehensive Study}},
url = {http://hal.archives-ouvertes.fr/inria-00070281/ http://link.springer.com/10.1007/s11263-006-9794-4},
volume = {73},
year = {2006}
}
@article{Zhang2013,
abstract = {We present the prototype of an egocentric vision based assistive co-robot system. In this co-robot system, the user is wearing a pair of glasses with a forward looking camera, and is actively engaged in the control loop of the robot in navigational tasks. The egocentric vision glasses serve for two purposes. First, it serves as a source of visual input to request the robot to find a certain object in the environment. Second, the motion patterns computed from the egocentric video associated with a specific set of head movements are exploited to guide the robot to find the object. These are especially helpful for quadriplegic individuals who do not have needed hand functionality for interaction and control with other modalities (e.g., joystick). In our co-robot system, when the robot does not fulfill the object finding task in a pre-specified time window, it would actively solicit user controls for guidance. Then the users can use the egocentric vision based gesture interface to orient the robot towards the direction of the object. After that the robot will automatically navigate towards the object until it finds it. Our experiments validated the efficacy of the closed-loop design to engage the human in the loop.},
author = {Zhang, Jingzhe and Zhuang, Lishuo and Wang, Yang and Zhou, Yameng and Meng, Yan and Hua, Gang},
doi = {10.1109/ICORR.2013.6650473},
file = {:home/abetan16/Dropbox/MendeleyV3/Zhang et al. - 2013 - An Egocentric Vision Based Assistive co-robot.pdf:pdf},
isbn = {9781467360241},
issn = {1945-7901},
journal = {Conference on Rehabilitation Robotics},
keywords = {Algorithms,Computer-Assisted,Computer-Assisted: methods,Eyeglasses,Humans,Image Processing,Man-Machine Systems,Robotics,Robotics: instrumentation,Self-Help Devices,Visual Perception},
month = {jun},
number = {Jun},
pages = {1--7},
pmid = {24187290},
title = {{An Egocentric Vision Based Assistive co-robot.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24187290},
volume = {2013},
year = {2013}
}
@inproceedings{Zheng,
abstract = {This paper is focused on developing a new approach for video-based action detection where a set of temporally synchronized videos are taken by multiple wearable cameras from different and varying views and our goal is to accurately localize the starting and ending time of each instance of the actions of interest in such videos. Compared with traditional approaches based on fixed-camera videos, this new approach incorporates the visual attention of the camera wearers and allows for the action detection in a larger area, although it brings in new challenges such as unconstrained motion of cameras. In this approach, we leverage the multi-view information and the temporal synchronization of the in- put videos for more reliable action detection. Specifically, we detect and track the focal character in each video and conduct action recognition only for the focal character in each temporal sliding window. To more accurately localize the starting and ending time of actions, we develop a strategy that may merge temporally adjacent sliding windows when detecting durative actions, and non-maximally suppress temporally ad- jacent sliding windows when detecting momentary actions. Finally we propose a voting scheme to integrate the detection results from multi- ple videos for more accurate action detection. For the experiments, we collect a new dataset of multiple wearable-camera videos that reflect the complex scenarios in practice.},
author = {Zheng, K and Lin, Y and Zhou, Y and Salvi, D and Fan, X and Guo, D and Meng, Zibo and Wang, Song},
booktitle = {Workshop on ChaLearn Looking at People},
file = {:home/abetan16/Dropbox/MendeleyV3/Zheng et al. - 2014 - Video-based Action Detection using Multiple Wearable Cameras.pdf:pdf},
keywords = {action detection,focal character,multi-view videos,wear-},
title = {{Video-based Action Detection using Multiple Wearable Cameras}},
year = {2014}
}
@article{Zhou2014,
abstract = {Scene recognition is one of the hallmark tasks of computer vision, allowing definition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet and the rise of Convolutional Neural Networks (CNNs) for learning high-level features, performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competitive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers' responses allows us to show differences in the internal representations of object-centric and scene-centric networks.},
author = {Zhou, Bolei and Lapedriza, Agata and Xiao, Jianxiong and Torralba, Antonio and Oliva, Aude},
file = {:home/abetan16/Dropbox/MendeleyV3/Zhou et al. - 2014 - Learning Deep Features for Scene Recognition using Places Database.pdf:pdf},
journal = {Advances in Neural Information Processing Systems 27},
pages = {487--495},
title = {{Learning Deep Features for Scene Recognition using Places Database}},
url = {http://papers.nips.cc/paper/5349-learning-deep-features-for-scene-recognition-using-places-database.pdf},
year = {2014}
}
@article{Zhou2008,
abstract = {Human motion tracking for rehabilitation has been an active research topic since the 1980s. It has been motivated by the increased number of patients who have suffered a stroke, or some other motor function disability. Rehabilitation is a dynamic process which allows patients to restore their functional capability to normal. To reach this target, a patients' activities need to be continuously monitored, and subsequently corrected. This paper reviews recent progress in human movement detection/tracking systems in general, and existing or potential application for stroke rehabilitation in particular. Major achievements in these systems are summarised, and their merits and limitations individually presented. In addition, bottleneck problems in these tracking systems that remain open are highlighted, along with possible solutions. ?? 2007 Elsevier Ltd. All rights reserved.},
author = {Zhou, Huiyu and Hu, Huosheng},
doi = {10.1016/j.bspc.2007.09.001},
file = {:home/abetan16/Dropbox/MendeleyV3/Zhou, Hu - 2008 - Human motion tracking for rehabilitation-A survey.pdf:pdf},
isbn = {1746-8094},
issn = {17468094},
journal = {Biomedical Signal Processing and Control},
keywords = {Biomedical signal processing,Control,Motion tracking,Sensor technology,Stroke rehabilitation},
number = {1},
pages = {1--18},
title = {{Human motion tracking for rehabilitation-A survey}},
volume = {3},
year = {2008}
}
@article{Zhou2004,
author = {Zhou, Huiyu and Zhou, Huiyu and Hu, Huosheng and Hu, Huosheng},
file = {:home/abetan16/Dropbox/MendeleyV3/Zhou et al. - 2004 - A Survey - Human Movement Tracking and Stroke Rehabilitation.pdf:pdf},
journal = {System},
number = {December},
title = {{A Survey - Human Movement Tracking and Stroke Rehabilitation}},
year = {2004}
}
@article{Zhu2011,
abstract = {In this paper, we proposed an approach to indoor human daily activity recognition which combines motion data and location information. One inertial sensor is worn on the right thigh of a human subject to provide motion data, while an optical motion capture system is used to provide the human location information. Such a combination has the advantage of significantly reducing the obtrusiveness to the human subject at a moderate cost of vision processing, while maintaining a high accuracy of recognition. First, a two-step algorithm is proposed to recognize the activity based on motion data only. In the coarse-grained classification, two neural networks are used to classify the basic activities. In the fine-grained classification, the sequence of activities is modeled by an HMM to consider the sequential constraints. The modified short-time Viterbi algorithm is used for real-time daily activity recognition. Second, to fuse the motion data with the location information, Bayes' theorem is used to update the activities recognized from the motion data. We conducted experiments in a mock apartment and the obtained results proved the effectiveness and accuracy of our algorithms. ?? 2011 Elsevier B.V. All rights reserved.},
author = {Zhu, Chun and Sheng, Weihua},
doi = {10.1016/j.pmcj.2010.11.004},
file = {:home/abetan16/Dropbox/MendeleyV3/Zhu, Sheng - 2011 - Motion- and location-based online human daily activity recognition.pdf:pdf},
isbn = {9781466636828},
issn = {15741192},
journal = {Pervasive and Mobile Computing},
keywords = {Activity recognition,Sensor fusion,Wearable computing},
number = {2},
pages = {256--269},
publisher = {Elsevier B.V.},
title = {{Motion- and location-based online human daily activity recognition}},
url = {http://dx.doi.org/10.1016/j.pmcj.2010.11.004},
volume = {7},
year = {2011}
}
@inproceedings{Zhu2010,
abstract = {The growing self-organizing map (GSOM) is a variation of the popular self-organizing map (SOM). It was developed to address the issue of identifying a suitable size of the SOM, which is usually concerned with vectorial items. To deal with algoritms implemented as programs, which are hardly represented by vectors, a new version of GSOM for clustering non-vectorial items (GSOM/NV) is proposed here. By syntax analysis, source codes of programs are converted into syntax trees, on a basis of which similarities between these codes are computed, so that the normal GSOM could be applied to clustering the algorithms that are implemented as the programs. An experiment shows that those whose implemented algorithms are the same, but coded differently each other, are gathered together on the visualization map generated by the proposed method. {\textcopyright} 2010 IEEE.},
author = {Zhu, G and Zhu, X},
booktitle = {International Conference on Artificial Intelligence and Computational Intelligence, AICI 2010},
doi = {10.1109/AICI.2010.276},
file = {:home/abetan16/Dropbox/MendeleyV3/Zhu, Zhu - 2010 - The growing self-organizing map for clustering algorithms in programming codes.pdf:pdf},
isbn = {9780769542256 (ISBN)},
keywords = {Algorithm clustering,Algoritms,Artificial intelligence,Clustering algorithms,Conformal mapping,Growing self-organizing map,Non-vectorial,Programming codes,Source codes,Spread factor,Spread factors,Syntactics,Syntax analysis,Syntax tree,Trees (mathematics),Visualization},
pages = {178--182},
title = {{The growing self-organizing map for clustering algorithms in programming codes}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-78651438004{\&}partnerID=40{\&}md5=8e76332a0d43c2777fb1268279188ec7},
volume = {3},
year = {2010}
}
@article{Zhu2015,
abstract = {Hand detection has many important applications in Human-Computer Interactions, yet it is a challenging problem because the appearance of hands can vary greatly in images. In this paper, we present a new approach that exploits the inherent contextual information from structured hand labelling for pixel-level hand detection and hand part labelling. By using a random forest framework, our method can predict hand mask and hand part labels in an efficient and robust manner. Through experiments, we demonstrate that our method can outperform other state-of-the-art pixel-level detection methods in ego-centric videos, and further be able to parse hand parts in details.},
author = {Zhu, Xiaolong and Jia, Xuhui and Wong, Kwan Yee K},
doi = {10.1016/j.cviu.2015.07.008},
file = {:home/abetan16/Dropbox/MendeleyV3/Zhu, Jia, Wong - 2015 - Structured forests for pixel-level hand detection and hand part labelling.pdf:pdf},
isbn = {9783319168173},
issn = {1090235X},
journal = {Computer Vision and Image Understanding},
keywords = {Hand detection,Egocentric vision,Random forests,Ha,Hand part labelling},
pages = {95--107},
publisher = {Elsevier Ltd.},
title = {{Structured forests for pixel-level hand detection and hand part labelling}},
url = {http://dx.doi.org/10.1016/j.cviu.2015.07.008},
volume = {141},
year = {2015}
}
@inproceedings{Zhu2014,
abstract = {Hand detection has many important applications in HCI, yet it is a challenging problem because the appearance of hands can vary greatly in images. In this paper, we propose a novel method for effi- cient pixel-level hand detection. Unlike previous method which assigns a binary label to every pixel independently, our method estimates a proba- bility shape mask for a pixel using structured forests. This approach can better exploit hand shape information in the training data, and enforce shape constraints in the estimation. Aggregation of multiple predictions generated from neighboring pixels further improves the robustness of our method.We evaluate our method on both ego-centric videos and un- constrained still images. Experiment results show that our method can detect hands efficiently and outperform other state-of-the-art methods. 1},
address = {Singapore},
author = {Zhu, Xiaolong and Jia, Xuhui and Wong, Kwan-yee K},
booktitle = {Asian Conference on Computer Vision},
file = {:home/abetan16/Dropbox/MendeleyV3/Zhu, Jia, Wong - 2014 - Pixel-Level Hand Detection with Shape-aware Structured Forests.pdf:pdf},
pages = {1--15},
title = {{Pixel-Level Hand Detection with Shape-aware Structured Forests}},
year = {2014}
}
@inproceedings{Zhu2002,
abstract = {The paper presents a sequential classification approach to improve the efficiency in visual object (face) detection. To reduce the computation while maintaining detection accuracy, a two-level hierarchy of sequential classification is proposed. At the top level, the overall detector is built on a cascade of classifiers at multiple resolution scales produced by a wavelet transform. Classifiers at low-resolution scales quickly rule out the regions likely to be background. Only object-like candidates are passed to subsequent high-resolution scales for more expensive tests. At the bottom level of the hierarchy, each classifier is implemented as a sequential Bayesian test using the features within the scale. The features are ranked adaptively according to their discrimination ability, which also leads to a quick decision. We demonstrate the scheme by an example of frontal view face detection.},
author = {Zhu, Y and Schwartz, S},
booktitle = {Image Processing},
file = {:home/abetan16/Dropbox/MendeleyV3/Zhu, Schwartz - 2002 - Efficient Face Detection with Multiscale Sequential Classification.pdf:pdf},
isbn = {0780376226},
pages = {121--124},
publisher = {IEEE},
title = {{Efficient Face Detection with Multiscale Sequential Classification}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1039902},
year = {2002}
}
@article{Zhu2013,
abstract = {In this paper, we propose a mathematical framework to jointly model related activities with both motion and context information for activity recognition and anomaly detection. This is motivated from observations that activities related in space and time rarely occur independently and can serve as context for each other. The spatial and temporal distribution of different activities provides useful cues for the understanding of these activities. We denote the activities occurring with high frequencies in the database as normal activities. Given training data which contains labeled normal activities, our model aims to automatically capture frequent motion and context patterns for each activity class, as well as each pair of classes, from sets of predefined patterns during the learning process. Then, the learned model is used to generate globally optimum labels for activities in the testing videos. We show how to learn the model parameters via an unconstrained convex optimization problem and how to predict the correct labels for a testing instance consisting of multiple activities. The learned model and generated labels are used to detect anomalies whose motion and context patterns deviate from the learned patterns. We show promising results on the VIRAT Ground Dataset that demonstrates the benefit of joint modeling and recognition of activities in a wide-area scene and the effectiveness of the proposed method in anomaly detection.},
author = {Zhu, Yingying and Nayak, N M and Roy-Chowdhury, a K},
doi = {10.1109/JSTSP.2012.2234722},
file = {:home/abetan16/Dropbox/MendeleyV3/Zhu, Nayak, Roy-Chowdhury - 2013 - Context-Aware Activity Recognition and Anomaly Detection in Video.pdf:pdf},
issn = {1932-4553},
journal = {Selected Topics in Signal Processing},
number = {1},
pages = {91--101},
title = {{Context-Aware Activity Recognition and Anomaly Detection in Video}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6384655$\backslash$npapers3://publication/doi/10.1109/JSTSP.2012.2234722},
volume = {7},
year = {2013}
}
@article{Zuniga2010,
author = {Z{\'{u}}{\~{n}}iga, VP and Azofeifa, IV},
file = {:home/abetan16/Dropbox/MendeleyV3/Z{\'{u}}{\~{n}}iga, Azofeifa - 2010 - Hydrogeochemistry in the coastal aquifer of the Uraba region.pdf:pdf},
journal = {Revista Ingenier{\'{i}}as Universidad de Medell{\'{i}}n},
keywords = {acu{\'{i}}fero,aspirante a mag{\'{i}}ster en,co,correo electr{\'{o}}nico,costa rica,edu,hidrogeolog{\'{i}}a y manejo de,hidrogeoqu{\'{i}}mica,ingeniera civil,intrusi{\'{o}}n,paredes,recursos h{\'{i}}dricos,salinizaci{\'{o}}n,san jos{\'{e}},udea,universidad de costa rica,vanessa},
number = {17},
pages = {51--62},
title = {{Hydrogeochemistry in the coastal aquifer of the Uraba region}},
url = {http://www.scielo.org.co/pdf/rium/v9n17/v9n17a05.pdf},
volume = {9},
year = {2010}
}
